\documentclass{article}
\usepackage{nips07submit_e,times}
%\documentstyle[nips07submit_09,times]{article}

\usepackage{amsmath,amsthm,amssymb}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary} {Corollary}
\newtheorem{definition} {Definition}

\newcommand{\OpSym}{\mathrm{O\!p}}
\newcommand{\opsym}{{\scriptstyle \mathrm{o\!p}}}
% \newcommand{\OpSym}{\bigoplus\nolimits}
% \newcommand{\opsym}{\oplus}
\newcommand{\OpCurry}[3][]{\mathop{\OpSym^{#1}_{#2}{#3}}}
\newcommand{\opcurry}[3][]{\mathop{\opsym^{#1}_{#2}{#3}}}
\newcommand{\Op}[2][]{\OpCurry[#1]{#2}{}}
\newcommand{\op}[2][]{\opcurry[#1]{#2}{}}
\newcommand{\VecOp}[2][]{\mathop{\overrightarrow{\OpSym^{#1}_{#2}}}}
\newcommand{\vecop}[2][]{\mathop{\overrightarrow{\opsym^{#1}_{#2}}}}

\newcommand{\comp}{\mathop{\circ}\nolimits}
\newcommand{\GNP}{\psi_{\Theta}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\map}{map}


\title{Some Awesome Title}

\author{
Ryan N.~Riegel \\
College of Computiong \\
Georgia Institute of Technology \\
Atlanta, GA 30332 \\
\texttt{rriegel@cc.gatech.edu} \\
\And
Alexander G.~Gray \\
College of Computiong \\
Georgia Institute of Technology \\
Atlanta, GA 30332 \\
\texttt{agray@cc.gatech.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\begin{document}

\makeanontitle

\begin{abstract}
We present mathematical foundations for a highly successful
algorithmic strategy that has resulted in the fastest algorithms for
many machine learning methods and is broadly applicable to scaling
many future methods up to large datasets.  We formalize for the first
time a class of computational problems which are very common in
machine learning, called {\em generalized $N$-body problems}, and
subsequently present a template {\em generalized $N$-body algorithm},
which can be specialized to produce efficient problem-dependent
algorithms.  We demonstrate the use of this mathematical framework for
deriving algorithms for new machine learning methods, including the
recent affinity propagation method.
\end{abstract}

\section{Introduction}

The ability to apply machine learning methods to large datasets is
increasingly important for many applications.  Unfortunately,
generally speaking, the more accurate the method, the greater its
computational cost; most nonparametric methods are $O(N^2)$ or
$O(N^3)$, where there are $O(N)$ test (or query) points and training
(or reference) points.

The authors of [[nips2000paper]] presented a new algorithmic strategy
based on the simultaneous traversal of multiple space-partitioning
trees, which applies to a number of machine learning methods.  The set
of problems to which this strategy is applicable was informally named
{\em generalized $N$-body problems}, in analogy to breakthrough
methods for computational physics problems [[Appel's algorithm,
Barnes-Hut, Fast Multipole Method]], all of which have the same
characteristic form.  This algorithmic strategy generalizes other
successful algorithms for specific problems in addition to these
physics problems, such as computing the well-separated pair
decomposition in theoretical computer science [[wspd]], computing the
spatial join in databases [[spatial join]], and computing set
intersections [[baeza-yates]].  In each of these problems no superior
algorithmic strategy is known.
% also a kind of dual-tree method in graphics for object collision
% detection, but we need the reference

This multi-tree algorithmic strategy has been applied to a succession
of well-known statistical learning methods, each representing certain
unique challenges for the strategy, including
all-$k$-nearest-neighbors (a generalization of $k$-nearest-neighbors)
[[nips2000paper]], kernel density estimation [[nips2000paper,
kde-siamdm, kde-aistats, kde-nips-dong, kde-uai-dong]],
$k$-nearest-neighbor classification [[ting-liu]], kernel discriminant
analysis (or nonparametric Bayes classification) [[nbc-phystat,
nbc-compstat]], and $n$-point correlation functions [[nips2000paper,
moore-npt, npt-2004]].  These algorithms have been demonstrated on
large scientific datasets, producing numerous high-profile results,
e.g.~[[science 2003, nature 2005]].  For each of these problems, no
overall faster algorithms are known.  In addition, other authors have
directed the strategy toward other machine learning problems,
including dimensionality reduction methods [[hochreiter00beyond]],
nonparametric belief propagation [[alex ihler]], multiple tracking
[[jeremy kubica]], linear algebraic machine learning methods [[nando,
below]], particle filters [[klaas papers, below]].  Aside from speed,
a unique advantage of this algorithmic approach is the ability to
specify and achieve rigorous relative error tolerances when
approximation is necessary, contrary to virtually all other
approximate speedup approaches.
%
%Nando de Freitas, Yang Wang, Maryam Mahdaviani and Dustin Lang. Fast
%Krylov Methods for N-Body Learning . NIPS 2005.
%
%Mike Klaas, Dustin Lang and Nando de Freitas. Fast Maximum a
%Posteriori Inference in Monte Carlo State Spaces . AISTATS 2005.
%
%Mike Klaas, Nando de Freitas and Arnaud Doucet. Toward Practical N^2
%Monte Carlo: The Marginal Particle Filter . UAI 2005
%
%Mike Klaas, Mark Briers, Nando de Freitas, Arnaud Doucet, Simon
%Maskell and Dustin Lang. Fast Particle Smoothing: If I Had a Million
%Particles. ICML 2006. 
%
%inproceedings{ hochreiter00beyond,
%    author = "Sepp Hochreiter and Michael Mozer",
%    title = "Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models",
%    booktitle = "{NIPS}",
%    pages = "535-541",
%    year = "2000",
%    url = "citeseer.ist.psu.edu/541475.html" }

Some fundamental problems to which the multi-tree strategy is
applicable are shown in Figure~\ref{tab:probs}, including some canonical
problems in theoretical computer science [[preparata-shamos]].  These
problems commonly occur in many guises in many fields, including many
machine learning methods, both classical and modern, such as recent
manifold learning [[lle, isomap]] methods.

\begin{table}[t]
\caption{Various generalized $N$-body problems.}
\label{tab:probs}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lr}
\hline
Problem & Expression \\
\hline
All-nn & $\displaystyle \map_{q \in Q} \argmin_{r \in R} d(q,r)$ \\
$n$-point cor. & $\displaystyle \sum_{x_1 \in X} \cdots \sum_{x_n \in X} I(\forall x_i, \forall x_j, d(x_i,x_j) \leq r)$ \\
KDE & $\displaystyle \map_{q \in Q} \sum_{r \in R} K(d(q,r))$ \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

In this paper we begin to investigate the scope of this successful
algorithmic approach.  For the first time, we mathematically formalize
the class of {\em generalized $N$-body problems}.  We also formalize
the algorithmic approach as a single generic algorithm, {\em the
generalized $N$-body algorithm}, which applies to any problem in the
class.  We elucidate some of the mathematical properties of such
problems that can be exploited to achieve computational efficiency.
These mathematical foundations facilitate the application of the ideas
we have developed over many problem examples by researchers wishing to
design fast algorithms for new problems in the class.  These
foundations also open up the possibility of automatic derivation of
algorithms for new problems [[autobayes]], as did the specification of
the generic EM algorithm by [[dempster et al.]].

In the next section, to make the ideas concrete, we present one of the
more easily understood problems in our class and an efficient
multi-tree method for it.  We then define generalized $N$-body
problems, and show that they have a decomposability property which
ultimately admits a general recursive formulation, the generalized
$N$-body algorithm.  Making the generalized $N$-body algorithm
efficient requires a problem-dependent pruning step.  The following
two sections discuss problem-dependent properties which admit
efficient pruning steps in the cases of exact and approximate
computation, respectively.  We conclude with examples of designing
fast algorithms for machine learning problems, starting with the
problem of nonparametric density thresholding, a common folk method
for anomaly detection.  Finally, as a demonstration of the practical
relevance of this methodology to state-of-the-art machine learning
methods, we considered the most recent method we could find, affinity
propagation [[affinity]], and designed and implemented in one day a
fast algorithm for it allowing it to scale to large datasets despite
its straightforward $O(N^2)$ runtime.

\section{Examples}

There exist many problems that initially appear to be quadratic or
worse in nature, involving consideration of all pairs or larger
groupings of inputs, but are later seen to have efficient algorithms.
These algorithms exploit a high-order formulation of
divide-and-conqure, often representing inputs in spatial trees and
using distance bounds between regions to guide the order of
computation.  Some examples include all-nearest-neighbors,
nonparametric Bayes classification, and the $n$-point correlation.

\subsection{All-nearest-neighbors}

The all-nearest-neighbors problem (All-NN) aims to find for each
point in some set $Q$ of queries the nearest point from a set $R$ of
references, possibly identical to $Q$.  The problem is represented
mathematically as
\[
\map_{1 \leq i \leq |Q|}\argmin_{1 \leq j \leq |R|} d(q_i,r_j),
\]
where metric $d(q_i,r_i) \equiv \infty$ if $Q = R$.

[[Examples of uses.]]

All-NN is solved efficiently by Algorithm~\ref{alg:all-nn}, which
works on trees formed for both queries and references.  It maintains
for each query node an upper bound on the distance to any of its
points' nearest neighbors; in short, the maximum distance to any
candidate nearest neighbor seen so far.  When a pair of query and
reference nodes have a lower-bound distance greater than the query
node's stored upper bound, it is impossible for the reference node to
contribute any of the contained queries' nearest neighbors and all
work between the nodes may be pruned.

\subsection{Kernel Density Estimation}

Kernel Density Estimation (KDE) wraps a small probability density
function around each point in some data set in order to estimate that
set's distribution.  It is then of interest to determine each point's
density in order to detect outliers.  Alternately, we may find
densities for a set of queries not from the original data set.
\[
\map_{q \in Q} \sum_{r \in R} K_h(q,r).
\]
[[Perhaps mention fitting bandwidth with LOO and L2E, etc.]]

\subsection{Nonparametric Bayes Classification}

Nonparametric Bayes classification (NBC) applies Bayes' Rule to the
results of kernel density estimation in order to predict the class of
each of some set of queries $Q$ given sets of references $R_k$ for
classes $C_k$, $1 \leq k \leq M$.  The problem is given by
\[
\map_{1 \leq i \leq |Q|} \argmax_{1 \leq k \leq M} \sum_{1 \leq j \leq |R_k|} K_{h_k}(q_i,r_j),
\]
where $K(q_i,r_i) \equiv 0$ if $Q = R$, permitting computation for
leave-one-out cross-validation.

[[Examples of uses.]]

Algorithm~\ref{alg:nbc} demonstrate an efficient means of computing
NBC in the two-class case.  It forms trees for all involved sets and
maintains at query nodes both upper and lower bounds on density
contributions from the various classes.  When one class's lower-bound
joint probability (found by multiplying the class's lower-bound
density and prior) is greater than all other's upper-bounds, then we
may safely conclude that all queries within the node should be
attributed to that class.

\subsection{Multi-radius $n$-point Correlation}

The $n$-point correlation is found by counting all unique $n$-tuples
of points within some radius $r$ of one another.  It is found with
\[
\sum_{x_{i_1} \in X} \cdots \sum_{x_{i_n} \in X} I(d(x_{i_j},x_{i_k}) < r \forall j,k),
\]
where $I(d(x_{i_j},x_{i_k}) < r \forall j,k) \equiv 0$ unless $i_1 <
\cdots < i_n$.

[[Examples of uses.]]

[[Description of algorithm.]]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized $N$-body Problems}

The class of Generalized $N$-body Problems (GNPs), or problems
solvable by means similar to those shown above, is a subset of the
problems representable as nested applications of high-order function
reduce.  Reduce is a long-standing and highly versatile feature of
functional programming languages, traditionally operating over a list
from beginning to end or vice versa.  We pose reduce as a function on
unordered input multisets\footnote{Unless otherwise noted, data sets
are understood to be finite multisets, thereby permitting entries with
identical values; alternately, data sets may store a distinct index
for each entry, rendering all entries unique.} rather than lists, and
allow for pre- and postprocessing functions.
\begin{definition}
  A {\em first-order reduce problem} is a tuple $\theta =
  (\mathcal{X},\otimes,f,g)$ of set of possible inputs $\mathcal{X}$,
  commmutative, associative operator $\otimes \colon \mathcal{A}
  \times \mathcal{A} \to \mathcal{A}$, and functions $f \colon
  \mathcal{X} \to \mathcal{A}$ and $g \colon \mathcal{A} \to
  \mathcal{B}$.  Its components form $\Psi_{\theta} = g \comp
  \psi_{\theta}$, where $\psi_{\theta}(X) = \bigotimes_{x \in X} f(x)$
  for nonempty $X \subset \mathcal{X}$.
\end{definition}
\noindent The operators involved in reduce problems form abelian
semigroups; frequently, they also have identities and inverses in
$\mathcal{A}$, thereby forming abelian groups.  An example of a
first-order reduce problem is finding the expected value of some
function under a sampled distribution: $\frac{1}{N} \sum_{x \in X}
f(x)$.

The inner functions of reduce problems may themselves be reduce
problems, for example with $f_1(x) = \Psi_{\theta_2}(X_2)$.  While the
methods presented below can potentially extend to arbitrary nesting
depth, this paper focuses on second-order problems, or problems of two
operators.
\begin{definition}
  A {\em second-order reduce problem} is a tuple $\Theta =
  (\mathcal{X},\mathcal{Y},\otimes,\odot,f,g,h)$ of sets of possible
  inputs $\mathcal{X}$ and $\mathcal{Y}$, commutative, associative
  operators $\otimes \colon \mathcal{A} \times \mathcal{A} \to
  \mathcal{A}$ and $\odot \colon \mathcal{B} \times \mathcal{B} \to
  \mathcal{B}$, and functions $f \colon \mathcal{X} \times \mathcal{Y}
  \to \mathcal{A}$, $g \colon \mathcal{Y} \times \mathcal{A} \to
  \mathcal{B}$, and $h \colon \mathcal{B} \to \mathcal{C}$.  Its
  components form $\Psi_{\Theta} = h \comp \psi_{\Theta}$, where
  $\psi_{\Theta}(Y,X) = \bigodot_{y \in Y} g \left( y,\bigotimes_{x
  \in X} f(x,y) \right)$ for nonempty $X \subset \mathcal{X}$ and $Y
  \subset \mathcal{Y}$.
\end{definition}
\noindent The results of second-order reduce problems may be found
na\"{\i}vely through nested iteration.  If involved functions are
constant time and $X$ and $Y$ are $O(N)$, this method of computation
is $O(N^2)$, which is infeasible for large $N$.  An example of a
second-order reduce problem is the log-likelihood of a mixture of
Gaussians: $\sum_{x \in X} \log \sum_{k \in C} \omega_k \phi(x |
\mu_k, \Sigma_k)$.

Certain constraints are required by the algorithmic technique
presented in the next section, though we will later introduce a
transform to help work around them.
\begin{definition}
  A second-order reduce problem $\Theta$ is {\em regular} if $g(y,a) =
  a$ for all $y \in \mathcal{Y}$ and $a \in \mathcal{A}$.  Such a
  problem is given by $\Psi_{\Theta} = h \comp \psi_{\Theta}$, where
  $\psi_{\Theta}(Y,X) = \bigodot_{y \in Y} \bigotimes_{x \in X}
  f(x,y)$.
\end{definition}
\begin{definition}
  A regular second-order reduce problem $\Theta$ is {\em block
  decomposable} if, for all nonempty partitions $X^L \cup X^R = X
  \subset \mathcal{X}$ and nonempty $Y \in \mathcal{Y}$, $\GNP(Y,X) =
  \GNP(Y,X^L) \otimes \GNP(Y,X^R)$.  Such a problem\footnote{Observe
  that commutativity and associativity ensure that $\GNP(Y,X) =
  \GNP(Y^L,X) \odot \GNP(Y^R,X)$.} is known as a {\em second-order
  generalized $N$-body problem}.
\end{definition}

% \subsection{The Map Operator}

{\bf The Map Operator.}  High-order function map is suitable for all
problems that compute separate results for some set of queries and may
help with some problems that do not.  Reduce problems subsume the
functionality of map with $\map_{x \in X} f(x) \equiv \bigcup_{x \in
X} \{(x,f(x))\} = \{(x,f(x)) | x \in X\}$.  We extend this key-value
pair interpretation of map to regular reduce problems via
vectorization.
\begin{lemma}
  Second-order reduce problem $\Theta$ with $\odot = \map$ is
  equivalent to $\overrightarrow{\Theta}$ with
  \[ \begin{array}{rclrcl}
    \overrightarrow{f}(x,y) & = & \{(y, f(x,y))\}, & \overrightarrow{g}(A) & = & \{(y, g(y,v)) | (y,v) \in A\}, \\
    A \mathop{\overrightarrow{\otimes}}\nolimits B & = & \{(y, u \otimes v) | (y,u) \in A, (y,v) \in B\}, & A \odot B & = & A \cup B.
  \end{array} \]
\end{lemma}
\begin{proof}
  During na\"{\i}ve computation, arguments presented to
  $\mathop{\overrightarrow{\otimes}}\nolimits$ and
  $\overrightarrow{g}$ are singleton sets for some $y \in Y$.  Vector
  operations then trivially match the original version of the
  algorithm.
\end{proof}
\noindent Vectorized operations become more interesting after the
application of block decomposition.

% \subsection{Block Decomposability}

{\bf Block Decomposability.} The primary advantage of GNPs is the
significant ability to rearrange their order of computation.  The
na\"{\i}ve computation of a second-order GNP forms a grid of
evaluations of $f$ connected with $\otimes$ and $\odot$ as shown in
the left half of Figure~\ref{fig:grid}.  A single application of block
decomposition might result in the right half of Figure~\ref{fig:grid}.
Further applications can form any hierarchy of nested rectangular
regions, possibly with permuted orderings of rows or columns.

\begin{figure}
  \begin{eqnarray*}
    \begin{array}{ccccccccc}
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_1) \!\!\!&\scriptstyle\!\!\! ) \\
      &\scriptstyle \odot \\
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_2) \!\!\!&\scriptstyle\!\!\! ) \\
      &\scriptstyle \odot \\
      &\scriptstyle \vdots \\
      &\scriptstyle \odot \\
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_M) \!\!\!&\scriptstyle\!\!\! )
    \end{array}
    & = &
    \begin{array}{ccc}
      \left( \begin{array}{c}
	\scriptstyle \!\!\!f(x_1,y_1)\!\!\! \\
	\scriptstyle \odot \\
	\scriptstyle \!\!\!f(x_1,y_2)\!\!\! \\
	\scriptstyle \odot \\
	\scriptstyle \vdots \\
	\scriptstyle \odot \\
	\scriptstyle \!\!\!f(x_1,y_M)\!\!\!
      \end{array} \right)
      \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\!\!\!
      \left( \begin{array}{ccccccc}
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_1) \!\!\!&\scriptstyle\!\!\! ) \\
	&\scriptstyle \odot \\
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_2) \!\!\!&\scriptstyle\!\!\! ) \\
	&\scriptstyle \odot \\
	&\scriptstyle \vdots \\
	&\scriptstyle \odot \\
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_M) \!\!\!&\scriptstyle\!\!\! )
      \end{array} \right)
    \end{array}
  \end{eqnarray*}
  \caption{\label{fig:grid}{\em (Left)} Na\"{i}ve computation of a
  second-order GNP.  {\em (Right)} An application of block
  decomposition restructures computation.}
\end{figure}

While $\mathcal{X}$, $\mathcal{Y}$, and $f$ may have some effect on
whether a problem is block decomposable, the most significant players
are $\otimes$ and $\odot$.  There are two important classes of
operator pairs that gaurantee block decomposability.
\begin{lemma}
  A regular second-order reduce problem is block decomposable if
  $\odot = \otimes$.
\end{lemma}
\begin{proof}
  By commutativity and associativity, we may rearrange
  \[ \begin{array}{ll}
    \multicolumn{2}{l}{\displaystyle \GNP(Y,X) = \bigotimes_{y \in Y} \bigotimes_{x \in X} f(x,y) = \bigotimes_{x \in X} \bigotimes_{y \in Y} f(x,y) = \bigotimes_{x \in X^L} \bigotimes_{y \in Y} f(x,y) \otimes \bigotimes_{x \in X^R} \bigotimes_{y \in Y} f(x,y)} \\
    & \displaystyle = \bigotimes_{y \in Y} \bigotimes_{x \in X^L} f(x,y) \otimes \bigotimes_{y \in Y} \bigotimes_{x \in X^R} f(x,y) = \GNP(Y,X^L) \otimes \GNP(Y,X^R).
  \end{array} \]
\end{proof}
\begin{lemma}\label{lem:map}
  A regular second-order reduce problem is block decomposable if
  $\odot = \map$.
\end{lemma}
\begin{proof}
  For $\psi_{\Theta}(Y,X) = \map_{y \in Y} \bigotimes_{x \in X} f(x,y)
  \equiv \bigcup_{y \in Y} \mathop{\overrightarrow{\bigotimes}}_{x \in
  X} \{(y,f(x,y))\} = \psi_{\overrightarrow{\Theta}}(Y,X)$ and by
  commutativity, associativity, and the definition of map, we have
  \[ \begin{array}{ll}
    \multicolumn{2}{l}{\displaystyle \psi_{\overrightarrow{\Theta}}(Y,X) \equiv \Big\{ \!\Big( y,\bigotimes_{x \in X} f(x,y) \Big)\! \Big| y \in Y \Big\} = \Big\{ \!\Big( y,\bigotimes_{x \in X^L} f(x,y) \otimes \bigotimes_{x \in X^R} f(x,y) \Big)\! \Big| y \in Y \Big\}} \\
    & \displaystyle = \Big\{ \!\Big( y,\bigotimes_{x \in X^L} f(x,y) \Big)\! \Big| y \in Y \Big\} \mathop{\overrightarrow{\otimes}}\nolimits \Big\{ \!\Big( y,\bigotimes_{x \in X^R} f(x,y) \Big)\! \Big| y \in Y \Big\} \equiv \psi_{\overrightarrow{\Theta}}(Y,X^L) \mathop{\overrightarrow{\otimes}}\nolimits \psi_{\overrightarrow{\Theta}}(Y,X^R).
  \end{array} \]
\end{proof}

% \subsection{Transforming Problems into GNPs}

{\bf Transforming Problems into GNPs.}  All second-order reduce
problems that are not GNPs may be transformed into GNPs.  Simple
algebra can often distribute or factor non-identity $g$ into less
obstructive positions, but failing that (or failing block
decomposability if $g$ is not a problem), we may form a GNP by
injecting map.
\begin{lemma}
  Second-order reduce problem $\Theta$ is equivalent to
  GNP $\breve{\Theta}$ with
  \[
  \Psi_{\Theta}(Y,X) = h \Big( \bigodot_{y \in Y} g \Big( \bigotimes_{x \in X} f(x,y) \Big) \Big) \equiv h \comp {\textstyle \bigodot} \comp \overrightarrow{g} \Big( \map_{y \in Y} \bigotimes_{x \in X} f(x,y) \Big) = \Psi_{\breve{\Theta}}(Y,X),
  \]
  where unsubscripted $\bigodot$ is understood to reduce the values in
  a set of key-value pairs.
\end{lemma}
\begin{proof}
  Observe that $\bigodot_{x \in X} f(x) = \bigodot{} \map_{x \in X}
  f(x)$ for arbitrary $f$ and $\bigodot$.  Further, observe that
  $\map_{x \in X} g(x,f(x)) = \overrightarrow{g} \left( \map_{x \in X}
  f(x) \right)$ for arbitrary $f$ and $g$.  Applying these in
  succession to the left-hand side of the above demonstrates its
  equality to the right.  By Lemma~\ref{lem:map}, $\breve{\Theta}$ is
  a GNP.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Generalized $N$-body Algorithm}

The block decomposable nature of GNPs suggests a recursive alternative
to the na\"{\i}ve computation,
\[
\GNP(Y,X) = \left\{ \begin{array}{lrr}
  f(x,y) & \multicolumn{2}{r}{\mbox{if } X = \{x\} \mbox{ and } Y = \{y\},} \\
  \multicolumn{2}{l}{\GNP(Y^L,X) \odot \GNP(Y^R,X)} & \mbox{if } |Y| > |X|, \\
  \multicolumn{2}{l}{\GNP(Y,X^L) \otimes \GNP(Y,X^R)} & \mbox{otherwise}.
\end{array} \right.
\]
Recursion forms a binary tree with one leaf per element of $X \times
Y$.  Exhuastive computation thus requires time $O(N^2)$, as with
na\"{\i}ve.  It may be possible, however, to obtain results for some
components of the recursive block decomposition without computing them
exhaustively.  Exploiting this, we hope to drive the expected running
time down.

\subsection{Abstraction}

We denote abstract results with the set $\GNP^A(X'_1,\ldots,X'_n)$ of
all possible results of $\GNP(X'_1,\ldots,X'_n)$ given some selection
of descriptive statistics on input $X'_i$, $1 \leq i \leq n$.  This
set must contain the exact result; accordingly, it cannot be empty.
Operations on abstract results yield the set of all possible outcomes
\[
\begin{array}{rcl}
  \lefteqn{\GNP^A(X'_1,\ldots,X^L_i,\ldots,X'_n) \op{i} \GNP^A(X'_1,\ldots,X^R_i,\ldots,X'_n)} \\
  & \equiv & \{a \op{i} b | a \in \GNP^A(X'_1,\ldots,X^L_i,\ldots,X'_n), b \in \GNP^A(X'_1,\ldots,X^R_i,\ldots,X'_n)\} \\
  & & \cap \GNP^A(X'_1,\ldots,X^L_i \cup X^R_i,\ldots,X'_n).
\end{array}
\]
[[Intersecting with abstract results for the composed region is
optional, as the composed abstract results for the subregions is
almost always tighter.  Perhaps also indicate that exact results
composed with abstract results work how you'd expect.]]

\subsection{The Algorithm}

[[Initialize frontier $\GNP^F$ to $\GNP^A(X_1,\ldots,X_n)$.  Then
perform the following update procedure until all abstract results have
been elminated:]]

\begin{itemize}
\item Select some $\GNP^A(X'_1,\ldots,X'_n)$ from $\GNP^F$
\item If prune possible, replace with $a \in \GNP^A(X'_1,\ldots,X'_n)$
\item If leaf, replace with $f(x_1,\ldots,x_n)$
\item Otherwise, replace with $\GNP^A(X'_1,\ldots,X^L_i,\ldots,X'_n) \op{i} \GNP^A(X'_1,\ldots,X^R_i,\ldots,X'_n)$
\end{itemize}

\[
\begin{array}{rcl}
  \lefteqn{\GNP^A(X'_1,\ldots,X'_n)} \\
  & \leftarrow & \left\{
  \begin{array}{lr}
    a \in \GNP^A(X'_1,\ldots,X'_n) & \mbox{ if prune} \\
    f(x'_1,\ldots,x'_n) & \mbox{ if leaf} \\
    \GNP^A(X'_1,\ldots,X^L_i,\ldots,X'_n) \op{i} \GNP^A(X'_1,\ldots,X^R_i,\ldots,X'_n) & \mbox{ otherwise}
  \end{array}
  \right.
\end{array}
\]

\section{Exact Pruning}

\subsection{Intrinsic}

\subsection{Extrinsic}

\subsection{Global}

\section{Approximate Pruning}

\subsection{Global}

\subsection{Intrinsic}

\subsection{Extrinsic}

\section{Practical Considertaions}

[[Abstract results must be represented somehow.  When intermediate
results form a lattice, upper and lower bounds may be used.  Further
conditions on operators allow for easy maintenance of bounds.]]

[[Too expensive to decide how to decompose blocks on an individual
basis, so reused precomputed tree splits when breaking up work.  Trees
also handy to find bottom-up statistics and store intermediate
results.]]

[[We can't afford to recompute the global bounds on $\GNP^F$ after
each update from scratch, but instead need some means of updating a
running total, so to speak.  For invertible operators, it is
sufficient to apply and undo changes directly to some globally
accessible value.  Some noninvertible operators (such as min) don't
need to be undone, but others can pose a challenge (resolved below).]]

[[Any expansion pattern may be used for choosing the next abstract
result to refine in $\GNP^F$.  Which to use is a trade-off between
overhead and improved pruning information.  Also, some expansion
patterns (e.g.~depth-first) can eliminate the ``undo'' problem by
allowing partial results to be stored at low cost.]]

\appendix

% \section{Full Permutability}
% 
% \begin{definition}
%   A regular reduce problem is {\em fully permutable} if, for all
%   permutations $p_1,\ldots,p_n$ of the numbers $1,\ldots,n$,
%   \[
%   \Op{1}_{x_1 \in X'_1}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) = \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}f(x_1,\ldots,x_n).
%   \]
% \end{definition}
% 
% \begin{theorem}
%   Full permutability is logically equivalent to exchangeability (and
%   thus block decomposability).
% \end{theorem}
% 
% \begin{proof}
%   ($\Rightarrow$) Given selected split $i$ and partitions $X^L_i \cup
%   X^R_i = X_i$, we have
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(X'_1,\ldots,X'_i,\ldots,X'_n)} \\
%     & = & \displaystyle \Op{1}_{x_1 \in X'_1}\cdots\Op{i}_{x_i \in X'_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{i}_{x_i \in X'_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{i}_{x_i \in X^L_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & & \displaystyle \mbox{} \op{i} \Op{i}_{x_i \in X^R_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{1}_{x_1 \in X'_i}\cdots\Op{i}_{x_i \in X^L_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & & \displaystyle \mbox{} \op{i} \Op{1}_{x_1 \in X'_i}\cdots\Op{i}_{x_i \in X^R_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \GNP(X'_1,\ldots,X^L_i,\ldots,X'_n) \op{i} \GNP(X'_1,\ldots,X^R_i,\ldots,X'_n).
%   \end{array}
%   \]
%   ($\Leftarrow$) Given permutation $p_1,\ldots,p_n$ of the numbers
%   $1,\ldots,n$, with $x^t_{p_i}$ denoting successive elements of
%   $X'_{p_1}$ and $X^t_{p_i} = \{x^1_{p_i},\ldots,x^t_{p_i}\}$, $1 \leq
%   t \leq |X'_{p_i}|$, and the understanding that permuted inputs are
%   mapped to the appropriate arguments of $\GNP$, we have
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X^t_{p_i},\ldots,X'_n)} \\
%     & = & \GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X^{t-1}_{p_i},\ldots,X'_n) \\
%     & & \mbox{} \op{p_i} \GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,\{x^t_{p_i}\},\ldots,X'_n).
%   \end{array}
%   \]
%   Induction over $t$ from $|X_{p_i}|$ down to $2$ yields
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X'_{p_i},\ldots,X'_n)} \\
%     & = & \displaystyle \Op{p_i}_{x_{p_i} \in X'_{p_i}}\GNP(\{x_{p_1}\},\ldots,\{x_{p_i}\},X'_1,\ldots,X'_{p_{i+1}},\ldots,X'_n).
%   \end{array}
%   \]
%   Further induction over $i$ from $1$ to $n$ yields
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\Op{1}_{x_1 \in X'_1}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n)} \\
%     & = & \GNP(X'_1,\ldots,X'_n) \\
%     & = & \displaystyle \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}\GNP(\{x_{p_1}\},\ldots,\{x_{p_n}\}) \\
%     & = & \displaystyle \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}f(x_1,\ldots,x_n).
%   \end{array}
%   \]
% \end{proof}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper.

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references.

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
