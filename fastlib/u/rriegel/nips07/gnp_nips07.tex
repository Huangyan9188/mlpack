\documentclass{article}
\usepackage{nips07submit_e,times}
%\documentstyle[nips07submit_09,times]{article}

\usepackage{amsmath,amsthm,amssymb}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary} {Corollary}
\newtheorem{definition} {Definition}


\newcommand{\GNP}[1][\psi]{{#1}_{\Theta}}
\newcommand{\GNPvec}[1][\psi]{{#1}_{\overrightarrow{\Theta}}}

\newcommand{\otimesvec}{\mathbin{\overrightarrow{\otimes}}}
\newcommand{\bigotimesvec}{\mathop{\overrightarrow{\bigotimes}}}
\newcommand{\otimeshat}{\mathbin{\widehat{\otimes}}}
\newcommand{\odothat}{\mathbin{\widehat{\odot}}}
\newcommand{\otimestilde}{\mathbin{\widetilde{\otimes}}}
\newcommand{\odottilde}{\mathbin{\widetilde{\odot}}}
\newcommand{\bigotimestilde}{\mathop{\widetilde{\bigotimes}}}
\newcommand{\bigodottilde}{\mathop{\widetilde{\bigodot}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\map}{map}

\newcommand{\comp}{\mathbin{\circ}}
\newcommand{\st}{{\rm~s.t.~}}


\title{Some Awesome Title}

\author{
Ryan N.~Riegel \\
College of Computiong \\
Georgia Institute of Technology \\
Atlanta, GA 30332 \\
\texttt{rriegel@cc.gatech.edu} \\
\And
Alexander G.~Gray \\
College of Computiong \\
Georgia Institute of Technology \\
Atlanta, GA 30332 \\
\texttt{agray@cc.gatech.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\begin{document}

\makeanontitle

\begin{abstract}
We present mathematical foundations for a highly successful
algorithmic strategy that has resulted in the fastest algorithms for
many machine learning methods and is broadly applicable to scaling
many future methods up to large datasets.  We formalize for the first
time a class of computational problems which are very common in
machine learning, called {\em generalized $N$-body problems}, and
subsequently present a template {\em generalized $N$-body algorithm},
which can be specialized to produce efficient problem-dependent
algorithms.  We demonstrate the use of this mathematical framework for
deriving algorithms for new machine learning methods, including the
recent affinity propagation method.
\end{abstract}

\section{Introduction}

The ability to apply machine learning methods to large datasets is
increasingly important for many applications.  Unfortunately,
generally speaking, the more accurate the method, the greater its
computational cost; most nonparametric methods are $O(N^2)$ or
$O(N^3)$, where there are $O(N)$ test (or query) points and training
(or reference) points.

The authors of [[nips2000paper]] presented a new algorithmic strategy
based on the simultaneous traversal of multiple space-partitioning
trees, which applies to a number of machine learning methods.  The set
of problems to which this strategy is applicable was informally named
{\em generalized $N$-body problems}, in analogy to breakthrough
methods for computational physics problems [[Appel's algorithm,
Barnes-Hut, Fast Multipole Method]], all of which have the same
characteristic form.  This algorithmic strategy generalizes other
successful algorithms for specific problems in addition to these
physics problems, such as computing the well-separated pair
decomposition in theoretical computer science [[wspd]], computing the
spatial join in databases [[spatial join]], and computing set
intersections [[baeza-yates]].  In each of these problems no superior
algorithmic strategy is known.
% also a kind of dual-tree method in graphics for object collision
% detection, but we need the reference

This multi-tree algorithmic strategy has been applied to a succession
of well-known statistical learning methods, each representing certain
unique challenges for the strategy, including
all-$k$-nearest-neighbors (a generalization of $k$-nearest-neighbors)
[[nips2000paper]], kernel density estimation [[nips2000paper,
kde-siamdm, kde-aistats, kde-nips-dong, kde-uai-dong]],
$k$-nearest-neighbor classification [[ting-liu]], kernel discriminant
analysis (or nonparametric Bayes classification) [[nbc-phystat,
nbc-compstat]], and $n$-point correlation functions [[nips2000paper,
moore-npt, npt-2004]].  These algorithms have been demonstrated on
large scientific datasets, producing numerous high-profile results,
e.g.~[[science 2003, nature 2005]].  For each of these problems, no
overall faster algorithms are known.  In addition, other authors have
directed the strategy toward other machine learning problems,
including dimensionality reduction methods [[hochreiter00beyond]],
nonparametric belief propagation [[alex ihler]], multiple tracking
[[jeremy kubica]], linear algebraic machine learning methods [[nando,
below]], particle filters [[klaas papers, below]].  Aside from speed,
a unique advantage of this algorithmic approach is the ability to
specify and achieve rigorous relative error tolerances when
approximation is necessary, contrary to virtually all other
approximate speedup approaches.
%
%Nando de Freitas, Yang Wang, Maryam Mahdaviani and Dustin Lang. Fast
%Krylov Methods for N-Body Learning . NIPS 2005.
%
%Mike Klaas, Dustin Lang and Nando de Freitas. Fast Maximum a
%Posteriori Inference in Monte Carlo State Spaces . AISTATS 2005.
%
%Mike Klaas, Nando de Freitas and Arnaud Doucet. Toward Practical N^2
%Monte Carlo: The Marginal Particle Filter . UAI 2005
%
%Mike Klaas, Mark Briers, Nando de Freitas, Arnaud Doucet, Simon
%Maskell and Dustin Lang. Fast Particle Smoothing: If I Had a Million
%Particles. ICML 2006. 
%
%inproceedings{ hochreiter00beyond,
%    author = "Sepp Hochreiter and Michael Mozer",
%    title = "Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models",
%    booktitle = "{NIPS}",
%    pages = "535-541",
%    year = "2000",
%    url = "citeseer.ist.psu.edu/541475.html" }

Some fundamental problems to which the multi-tree strategy is
applicable are shown in Figure~\ref{tab:probs}, including some canonical
problems in theoretical computer science [[preparata-shamos]].  These
problems commonly occur in many guises in many fields, including many
machine learning methods, both classical and modern, such as recent
manifold learning [[lle, isomap]] methods.

\begin{table}[t]
\caption{Various generalized $N$-body problems.}
\label{tab:probs}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lr}
\hline
Problem & Expression \\
\hline
All-nn & $\displaystyle \map_{q \in Q} \argmin_{r \in R} d(q,r)$ \\
$n$-point cor. & $\displaystyle \sum_{x_1 \in X} \cdots \sum_{x_n \in X} I(\forall x_i, \forall x_j, d(x_i,x_j) \leq r)$ \\
KDE & $\displaystyle \map_{q \in Q} \sum_{r \in R} K(d(q,r))$ \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

In this paper we begin to investigate the scope of this successful
algorithmic approach.  For the first time, we mathematically formalize
the class of {\em generalized $N$-body problems}.  We also formalize
the algorithmic approach as a single generic algorithm, {\em the
generalized $N$-body algorithm}, which applies to any problem in the
class.  We elucidate some of the mathematical properties of such
problems that can be exploited to achieve computational efficiency.
These mathematical foundations facilitate the application of the ideas
we have developed over many problem examples by researchers wishing to
design fast algorithms for new problems in the class.  These
foundations also open up the possibility of automatic derivation of
algorithms for new problems [[autobayes]], as did the specification of
the generic EM algorithm by [[dempster et al.]].

In the next section, to make the ideas concrete, we present one of the
more easily understood problems in our class and an efficient
multi-tree method for it.  We then define generalized $N$-body
problems, and show that they have a decomposability property which
ultimately admits a general recursive formulation, the generalized
$N$-body algorithm.  Making the generalized $N$-body algorithm
efficient requires a problem-dependent pruning step.  The following
two sections discuss problem-dependent properties which admit
efficient pruning steps in the cases of exact and approximate
computation, respectively.  We conclude with examples of designing
fast algorithms for machine learning problems, starting with the
problem of nonparametric density thresholding, a common folk method
for anomaly detection.  Finally, as a demonstration of the practical
relevance of this methodology to state-of-the-art machine learning
methods, we considered the most recent method we could find, affinity
propagation [[affinity]], and designed and implemented in one day a
fast algorithm for it allowing it to scale to large datasets despite
its straightforward $O(N^2)$ runtime.

\section{Examples}

There exist many problems that initially appear to be quadratic or
worse in nature, involving consideration of all pairs or larger
groupings of inputs, but are later seen to have efficient algorithms.
These algorithms exploit a high-order formulation of
divide-and-conqure, often representing inputs in spatial trees and
using distance bounds between regions to guide the order of
computation.  Some examples include all-nearest-neighbors,
nonparametric Bayes classification, and the $n$-point correlation.

\subsection{All-nearest-neighbors}

The all-nearest-neighbors problem (All-NN) aims to find for each
point in some set $Q$ of queries the nearest point from a set $R$ of
references, possibly identical to $Q$.  The problem is represented
mathematically as
\[
\map_{1 \leq i \leq |Q|}\argmin_{1 \leq j \leq |R|} d(q_i,r_j),
\]
where metric $d(q_i,r_i) \equiv \infty$ if $Q = R$.

[[Examples of uses.]]

All-NN is solved efficiently by Algorithm~\ref{alg:all-nn}, which
works on trees formed for both queries and references.  It maintains
for each query node an upper bound on the distance to any of its
points' nearest neighbors; in short, the maximum distance to any
candidate nearest neighbor seen so far.  When a pair of query and
reference nodes have a lower-bound distance greater than the query
node's stored upper bound, it is impossible for the reference node to
contribute any of the contained queries' nearest neighbors and all
work between the nodes may be pruned.

\subsection{Kernel Density Estimation}

Kernel Density Estimation (KDE) wraps a small probability density
function around each point in some data set in order to estimate that
set's distribution.  It is then of interest to determine each point's
density in order to detect outliers.  Alternately, we may find
densities for a set of queries not from the original data set.
\[
\map_{q \in Q} \sum_{r \in R} K_h(q,r).
\]
[[Perhaps mention fitting bandwidth with LOO and L2E, etc.]]

\subsection{Nonparametric Bayes Classification}

Nonparametric Bayes classification (NBC) applies Bayes' Rule to the
results of kernel density estimation in order to predict the class of
each of some set of queries $Q$ given sets of references $R_k$ for
classes $C_k$, $1 \leq k \leq M$.  The problem is given by
\[
\map_{1 \leq i \leq |Q|} \argmax_{1 \leq k \leq M} \sum_{1 \leq j \leq |R_k|} K_{h_k}(q_i,r_j),
\]
where $K(q_i,r_i) \equiv 0$ if $Q = R$, permitting computation for
leave-one-out cross-validation.

[[Examples of uses.]]

Algorithm~\ref{alg:nbc} demonstrate an efficient means of computing
NBC in the two-class case.  It forms trees for all involved sets and
maintains at query nodes both upper and lower bounds on density
contributions from the various classes.  When one class's lower-bound
joint probability (found by multiplying the class's lower-bound
density and prior) is greater than all other's upper-bounds, then we
may safely conclude that all queries within the node should be
attributed to that class.

\subsection{Multi-radius $n$-point Correlation}

The $n$-point correlation is found by counting all unique $n$-tuples
of points within some radius $r$ of one another.  It is found with
\[
\sum_{x_{i_1} \in X} \cdots \sum_{x_{i_n} \in X} I(d(x_{i_j},x_{i_k}) < r \forall j,k),
\]
where $I(d(x_{i_j},x_{i_k}) < r \forall j,k) \equiv 0$ unless $i_1 <
\cdots < i_n$.

[[Examples of uses.]]

[[Description of algorithm.]]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized $N$-body Problems}

The class of Generalized $N$-body Problems (GNPs), or problems
solvable by means similar to those shown above, is a subset of the
problems representable as nested applications of high-order function
reduce.  Reduce is a long-standing and highly versatile feature of
functional programming languages, traditionally operating over a list
from beginning to end or vice versa.  We pose reduce as a function on
unordered input multisets\footnote{Unless otherwise noted, data sets
are understood to be finite multisets, thereby permitting entries with
identical values; alternately, data sets may store a distinct index
for each entry, rendering all entries unique.} rather than lists, and
allow for pre- and postprocessing functions.
\begin{definition}
  A {\em first-order reduce problem} is a tuple $\theta =
  (\mathcal{X},\otimes,f,g)$ of set of possible inputs $\mathcal{X}$,
  commmutative, associative operator $\otimes \colon \mathcal{A}
  \times \mathcal{A} \to \mathcal{A}$, and functions $f \colon
  \mathcal{X} \to \mathcal{A}$ and $g \colon \mathcal{A} \to
  \mathcal{B}$.  Its components form $\Psi_{\theta} = g \comp
  \psi_{\theta}$, where $\psi_{\theta}(X) = \bigotimes_{x \in X} f(x)$
  for nonempty $X \subset \mathcal{X}$.
\end{definition}
\noindent The operators involved in reduce problems form abelian
semigroups; frequently, they also have identities and inverses in
$\mathcal{A}$, thereby forming abelian groups.  An example of a
first-order reduce problem is finding the expected value of some
function under a sampled distribution: $\frac{1}{N} \sum_{x \in X}
f(x)$.

The inner functions of reduce problems may themselves be reduce
problems, for example with $f_1(x) = \Psi_{\theta_2}(X_2)$.  While the
methods presented below can potentially extend to arbitrary nesting
depth, this paper focuses on second-order problems, or problems of two
operators.
\begin{definition}
  A {\em second-order reduce problem} is a tuple $\Theta =
  (\mathcal{X},\mathcal{Y},\otimes,\odot,f,g,h)$ of sets of possible
  inputs $\mathcal{X}$ and $\mathcal{Y}$, commutative, associative
  operators $\otimes \colon \mathcal{A} \times \mathcal{A} \to
  \mathcal{A}$ and $\odot \colon \mathcal{B} \times \mathcal{B} \to
  \mathcal{B}$, and functions $f \colon \mathcal{X} \times \mathcal{Y}
  \to \mathcal{A}$, $g \colon \mathcal{Y} \times \mathcal{A} \to
  \mathcal{B}$, and $h \colon \mathcal{B} \to \mathcal{C}$.  Its
  components form $\Psi_{\Theta} = h \comp \psi_{\Theta}$, where
  $\psi_{\Theta}(Y,X) = \bigodot_{y \in Y} g \left( y,\bigotimes_{x
  \in X} f(x,y) \right)$ for nonempty $X \subset \mathcal{X}$ and $Y
  \subset \mathcal{Y}$.
\end{definition}
\noindent The results of second-order reduce problems may be found
na\"{\i}vely through nested iteration.  If involved functions are
constant time and $X$ and $Y$ are $O(N)$, this method of computation
is $O(N^2)$, which is infeasible for large $N$.  An example of a
second-order reduce problem is the log-likelihood of a mixture of
Gaussians: $\sum_{x \in X} \log \sum_{k \in C} \omega_k \phi(x |
\mu_k, \Sigma_k)$.

Certain constraints are required by the algorithmic technique
presented in the next section, though we will later introduce a
transform to help work around them.
\begin{definition}
  A second-order reduce problem $\Theta$ is {\em regular} if $g(y,a) =
  a$ for all $y \in \mathcal{Y}$ and $a \in \mathcal{A}$, and is thus
  given by $\Psi_{\Theta} = h \comp \psi_{\Theta}$, where
  $\psi_{\Theta}(Y,X) = \bigodot_{y \in Y} \bigotimes_{x \in X}
  f(x,y)$.  Note that $\mathcal{B} = \mathcal{A}$.
\end{definition}
\begin{definition}
  A regular second-order reduce problem $\Theta$ is {\em block
  decomposable} if, for all nonempty partitions $X^{\!L} \cup X^{\!R} = X
  \subset \mathcal{X}$ and nonempty $Y \in \mathcal{Y}$, $\GNP(Y,X) =
  \GNP(Y,X^{\!L}) \otimes \GNP(Y,X^{\!R})$.  Such a problem\footnote{Observe
  that commutativity and associativity ensure that $\GNP(Y,X) =
  \GNP(Y^{\!L},X) \odot \GNP(Y^{\!R},X)$.} is known as a {\em second-order
  generalized $N$-body problem}.
\end{definition}

% \subsection{The Map Operator}

{\bf The Map Operator.}  High-order function map is suitable for all
problems that compute separate results for some set of queries and may
help with some problems that do not.  Reduce problems subsume the
functionality of map with $\map_{x \in X} f(x) \equiv \bigcup_{x \in
X} \{(x,f(x))\} = \{(x,f(x)) | x \in X\}$.  We extend this key-value
pair interpretation of map to regular reduce problems via
vectorization.
\begin{lemma}
  Second-order reduce problem $\Theta$ with $\odot = \map$ is
  equivalent to $\overrightarrow{\Theta}$ with
  \[ \begin{array}{rclrcl}
    \overrightarrow{f}(x,y) & = & \{(y, f(x,y))\}, & \overrightarrow{g}(A) & = & \{(y, g(y,v)) | (y,v) \in A\}, \\
    A \otimesvec B & = & \{(y, u \otimes v) | (y,u) \in A, (y,v) \in B\}, & A \odot B & = & A \cup B.
  \end{array} \]
\end{lemma}
\begin{proof}
  During na\"{\i}ve computation, arguments presented to $\otimesvec$
  and $\overrightarrow{g}$ are singleton sets for some $y \in Y$.
  Vector operations then trivially match the original version of the
  algorithm.
\end{proof}
\noindent Vectorized operations become more interesting after the
application of block decomposition.

% \subsection{Block Decomposability}

{\bf Block Decomposability.} The primary advantage of GNPs is the
significant ability to rearrange their order of computation.  The
na\"{\i}ve computation of a second-order GNP forms a grid of
evaluations of $f$ connected with $\otimes$ and $\odot$ as shown in
the left half of Figure~\ref{fig:grid}.  A single application of block
decomposition might result in the right half of Figure~\ref{fig:grid}.
Further applications can form any hierarchy of nested rectangular
regions, possibly with permuted orderings of rows or columns.

\begin{figure}
  \begin{eqnarray*}
    \begin{array}{ccccccccc}
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_1) \!\!\!&\scriptstyle\!\!\! ) \\
      \multicolumn{9}{c}{\scriptstyle \odot} \\
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_2) \!\!\!&\scriptstyle\!\!\! ) \\
      \multicolumn{9}{c}{\scriptstyle \odot} \\
      \multicolumn{9}{c}{\scriptstyle \vdots} \\
      \multicolumn{9}{c}{\scriptstyle \odot} \\
      \scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_1,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_2,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_M) \!\!\!&\scriptstyle\!\!\! )
    \end{array}
    & = &
    \begin{array}{ccc}
      \left( \begin{array}{c}
	\scriptstyle \!\!\!f(x_1,y_1)\!\!\! \\
	\scriptstyle \odot \\
	\scriptstyle \!\!\!f(x_1,y_2)\!\!\! \\
	\scriptstyle \odot \\
	\scriptstyle \vdots \\
	\scriptstyle \odot \\
	\scriptstyle \!\!\!f(x_1,y_M)\!\!\!
      \end{array} \right)
      \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\!\!\!
      \left( \begin{array}{ccccccc}
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_1) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_1) \!\!\!&\scriptstyle\!\!\! ) \\
	\multicolumn{7}{c}{\scriptstyle \odot} \\
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_2) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_2) \!\!\!&\scriptstyle\!\!\! ) \\
	\multicolumn{7}{c}{\scriptstyle \odot} \\
	\multicolumn{7}{c}{\scriptstyle \vdots} \\
	\multicolumn{7}{c}{\scriptstyle \odot} \\
	\scriptstyle ( \!\!\!&\scriptstyle\!\!\! f(x_2,y_M) \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! \cdots \!\!\!&\scriptstyle\!\!\! \otimes \!\!\!&\scriptstyle\!\!\! f(x_N,y_M) \!\!\!&\scriptstyle\!\!\! )
      \end{array} \right)
    \end{array}
  \end{eqnarray*}
  \caption{\label{fig:grid}{\em (Left)} Na\"{i}ve computation of a
  second-order GNP.  {\em (Right)} An application of block
  decomposition restructures computation.}
\end{figure}

While $\mathcal{X}$, $\mathcal{Y}$, and $f$ may have some effect on
whether a problem is block decomposable, the most significant players
are $\otimes$ and $\odot$.  There are two important classes of
operator pairs that gaurantee block decomposability.
\begin{lemma}
  A regular second-order reduce problem is block decomposable if
  $\odot = \otimes$.
\end{lemma}
\begin{proof}
  By commutativity and associativity, we may rearrange
  \[ \begin{array}{ll}
    \multicolumn{2}{l}{\displaystyle \GNP(Y,X) = \bigotimes_{y \in Y} \bigotimes_{x \in X} f(x,y) = \bigotimes_{x \in X} \bigotimes_{y \in Y} f(x,y) = \bigotimes_{x \in X^{\!L}} \bigotimes_{y \in Y} f(x,y) \otimes \bigotimes_{x \in X^{\!R}} \bigotimes_{y \in Y} f(x,y)} \\
    & \displaystyle = \bigotimes_{y \in Y} \bigotimes_{x \in X^{\!L}} f(x,y) \otimes \bigotimes_{y \in Y} \bigotimes_{x \in X^{\!R}} f(x,y) = \GNP(Y,X^{\!L}) \otimes \GNP(Y,X^{\!R}).
  \end{array} \]
\end{proof}
\begin{lemma}\label{lem:map}
  A regular second-order reduce problem is block decomposable if
  $\odot = \map$.
\end{lemma}
\begin{proof}
  For $\GNP(Y,X) = \map_{y \in Y} \bigotimes_{x \in X} f(x,y) \equiv
  \bigcup_{y \in Y} \bigotimesvec_{x \in X} \{(y,f(x,y))\} =
  \GNPvec(Y,X)$ and by commutativity, associativity, and the
  definition of map, we have
  \[ \begin{array}{ll}
    \multicolumn{2}{l}{\displaystyle \GNPvec(Y,X) \equiv \Big\{ \!\Big( y,\bigotimes_{x \in X} f(x,y) \Big)\! \Big| y \in Y \Big\} = \Big\{ \!\Big( y,\bigotimes_{x \in X^{\!L}} f(x,y) \otimes \bigotimes_{x \in X^{\!R}} f(x,y) \Big)\! \Big| y \in Y \Big\}} \\
    & \displaystyle = \Big\{ \!\Big( y,\bigotimes_{x \in X^{\!L}} f(x,y) \Big)\! \Big| y \in Y \Big\} \otimesvec \Big\{ \!\Big( y,\bigotimes_{x \in X^{\!R}} f(x,y) \Big)\! \Big| y \in Y \Big\} \equiv \GNPvec(Y,X^{\!L}) \otimesvec \GNPvec(Y,X^{\!R}).
  \end{array} \]
\end{proof}

% \subsection{Transforming Problems into GNPs}

{\bf Transforming Problems into GNPs.}  All second-order reduce
problems that are not GNPs may be transformed into GNPs.  Simple
algebra can often distribute or factor non-identity $g$ into less
obstructive positions, but failing that (or failing block
decomposability if $g$ is not a problem), we may form a GNP by
injecting map.
\begin{lemma}
  Second-order reduce problem $\Theta$ is equivalent to
  GNP $\breve{\Theta}$ with
  \[
  \Psi_{\Theta}(Y,X) = h \Big( \bigodot_{y \in Y} g \Big( \bigotimes_{x \in X} f(x,y) \Big) \Big) \equiv h \comp {\textstyle \bigodot} \comp \overrightarrow{g} \Big( \map_{y \in Y} \bigotimes_{x \in X} f(x,y) \Big) = \Psi_{\breve{\Theta}}(Y,X),
  \]
  where unsubscripted $\bigodot$ is understood to reduce the values in
  a set of key-value pairs.
\end{lemma}
\begin{proof}
  Observe that $\bigodot_{x \in X} f(x) = \bigodot{} \map_{x \in X}
  f(x)$ for arbitrary $f$ and $\bigodot$.  Further, observe that
  $\map_{x \in X} g(x,f(x)) = \overrightarrow{g} \left( \map_{x \in X}
  f(x) \right)$ for arbitrary $f$ and $g$.  Applying these in
  succession to the left-hand side of the above demonstrates its
  equality to the right.  By Lemma~\ref{lem:map}, $\breve{\Theta}$ is
  a GNP.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Generalized $N$-body Algorithm}

The block decomposable nature of GNPs suggests a recursive alternative
to the na\"{\i}ve computation,
\[
\GNP(Y,X) = \left\{ \begin{array}{lrr}
  f(x,y) & \multicolumn{2}{r}{\mbox{if } X = \{x\} \mbox{ and } Y = \{y\},} \\
  \multicolumn{2}{l}{\GNP(Y^{\!L},X) \odot \GNP(Y^{\!R},X)} & \mbox{if } Y \succ X, \\
  \multicolumn{2}{l}{\GNP(Y,X^{\!L}) \otimes \GNP(Y,X^{\!R})} & \mbox{otherwise},
\end{array} \right.
\]
where $Y \succ X$ is some means of deciding what to split first, such
as $|Y| \geq |X|$.  Recursion forms a binary tree with one leaf per
element of $X \times Y$.  Exhuastive computation thus requires time
$O(N^2)$, the same as the na\"{\i}ve algorithm.  It may be possible,
however, to obtain results for some components of the recursive block
decomposition without computing them exhaustively (i.e.~to {\em prune}
them).  Exploiting this, we hope to drive the expected running time
down.

% \subsection{Summaries and Statistics}

{\bf Summaries and Statistics.}  In order to accelerate computation,
we need some quick means of summarizing the possible results of
$\GNP(Y,X)$.  This typically involves consideration of $X$ and $Y$ at
the abstract level formed by a concise (and ideally precomputed) set
of statistics on the two.
\begin{definition}
  Given statistics functions $\sigma_x \colon 2^{\mathcal{X}} \to
  \mathcal{S}_x$ and $\sigma_y \colon 2^{\mathcal{Y}} \to
  \mathcal{S}_y$, let summary function $\GNP[\sigma] \colon
  \mathcal{S}_y \times \mathcal{S}_x \to 2^\mathcal{A}$ be such that
  \[
  \GNP[\sigma](\sigma_y(Y),\sigma_x(X)) \supseteq \{\GNP(Y^*,X^*) | X^* \st \sigma(X^*) = \sigma(X), Y^* \st \sigma(Y^*) = \sigma(Y)\}.
  \]
\end{definition}
\noindent Intuitively, $\GNP[\sigma](\sigma_y(Y),\sigma_x(X))$
represents all possible results of $\GNP(Y,X)$ given what we know
about $X$ and $Y$.  It is permitted to be a superset of such results
because the exact set may be costly or impossible to represent.  Note
that functions $\sigma_x$, $\sigma_y$, and $\GNP[\sigma]$ are not
unique for a given GNP; indeed, chosing the right statistics can
significantly impact running time.  A common example of statistics is
finding bounding boxes of data in Euclidean space.  Summaries computed
from these may be represented with upper and lower bounds on distances
between points in $X$ and $Y$.

% \subsection{Intrinsic Pruning}

{\bf Intrinsic Pruning.}  Summarization leads directly to our first
form of pruning.
\begin{lemma}
  We may perform {\em intrinsic pruning} when summary results form a
  singleton set.
\end{lemma}
\begin{proof}
  Setting $X^* = X$ and $Y^* = Y$ meets the requirements for inclusion
  in $\GNP[\sigma](\sigma_y(Y),\sigma_x(X))$.  Thus, summary results
  contain the exact result.  Thus, singleton
  $\GNP[\sigma](\sigma_y(Y),\sigma_x(X)) = \{\GNP(Y,X)\}$ and we are
  free to shortcut all further computation on $X$ and $Y$ with this
  value.
\end{proof}

\begin{corollary}
  We may perform intrinsic pruning for the 2-point correlation.
\end{corollary}
\begin{proof}
  Let $\sigma_x$ and $\sigma_y$ find bounding boxes for points in $X$
  and $Y$ and let $D^{\!U}$ and $D^{\!L}$ find upper and lower bounds
  between bounding boxes.  For $x \in X$ and $y \in Y$, the bounds on
  $I(d(x,y) \leq r)$ are then $\left[
  I(D^{\!U}(\sigma_x(X),\sigma_y(Y)) \leq r),
  I(D^{\!L}(\sigma_x(X),\sigma_y(Y)) \leq r) \right]$.  We may thus
  define
  \[
  \GNP[\sigma](Y,X) = \left[ |Y| \cdot |X| \cdot I(D^{\!U}(\sigma_x(X),\sigma_y(Y)) \leq r), |Y| \cdot |X| \cdot I(D^{\!L}(\sigma_x(X),\sigma_y(Y)) \leq r) \right].
  \]
  This set is $\{|Y| \cdot |X|\}$ when
  $D^{\!U}(\sigma_x(X),\sigma_y(Y)) \leq r$ and $\{0\}$ when
  $D^{\!L}(\sigma_x(X),\sigma_y(Y)) > r$.
\end{proof}

% \subsection{Iterative Refinement.}

{\bf Iterative Refinement.}  Further pruning is possible in some
problems by considering results gathered from other parts of
computation.  This is assisted by the ability to compose summary
results.
\begin{definition}
  Given summary results $A, B \subset \mathcal{A}$, let operators
  $\otimeshat, \odothat \colon 2^{\mathcal{A}} \times 2^{\mathcal{A}}
  \to 2^{\mathcal{A}}$ be such that
  \[ \begin{array}{rcl}
    A \otimeshat B \supseteq \{a \otimes b | a \in A, b \in B\} & \mbox{ and, likewise, } & A \odothat B \supseteq \{a \odot b | a \in A, b \in B\}.
  \end{array} \]
\end{definition}
\noindent We develope a notion of itertive refinement by means of
replacing summary result sets for the various components of
computation with composed summary results for their left and right
subcomponents.
\begin{definition}
  To perform {\em iterative refinement}, first initialize
  $\GNP[\Sigma](Y,X) \leftarrow
  \GNP[\sigma](\sigma_y(Y),\sigma_x(X))$, and then repeatedly select
  some $\GNP[\sigma](\sigma_y(Y'),\sigma_x(X'))$ present in
  $\GNP[\Sigma](Y,X)$ and replace it with
  \[
  \GNP[\sigma](\sigma_y(Y'),\sigma_x(X')) \leftarrow \left\{ \begin{array}{lrr}
    \{f(x,y)\} & \multicolumn{2}{r}{\mbox{if } X' = \{x\} \mbox{ and } Y' = \{y\},} \\
    \multicolumn{2}{l}{\GNP[\sigma](\sigma_y(Y^{\!L}),\sigma_x(X')) \odothat \GNP[\sigma](\sigma_y(Y^{\!R}),\sigma_x(X'))} & \mbox{if } Y \succ X, \\
    \multicolumn{2}{l}{\GNP[\sigma](\sigma_y(Y'),\sigma_x(X^{\!L})) \otimeshat \GNP[\sigma](\sigma_y(Y'),\sigma_x(X^{\!R}))} & \mbox{otherwise}.
  \end{array} \right.
  \]
  Futher, define $\GNP[\Sigma](Y',X')$ to refer to any component
  $\GNP[\sigma](\sigma_y(Y'),\sigma_x(X'))$ having been introduced or
  updated during refinement.  The values of all $\GNP[\Sigma](Y',X')$
  are understood to reflect any changes made to their subcomponents.
\end{definition}
\noindent Like the recursive formulation, iterative refinement
constructs a binary tree that grows by one node per replacement, and
thus must terminate after $O(N^2)$ steps.  It is useful to speak of
this tree directly, with nodes $\GNP[\Sigma](Y',X')$ and functions
$left$, $right$, $op$, $parent$, and $sibling$ defined intuitively.

Refinement need not be performed in any particular pattern.
Depth-first is often a good choice due to its low overhead, though
pruning in some problems strongly favors other expansion patterns.

% \subsection{Extrinsic Pruning}

{\bf Extrinsic Pruning.}  Iterative refinement allows us to prune
components when more precise knowledge of their results cannot affect
the global result.
\begin{lemma}
  For node $\GNP[\Sigma](Y',X')$ at depth $D$ of tree
  $\GNP[\Sigma](Y,X)$ and path $A_0,\ldots,A_D$ given by $A_D =
  \GNP[\Sigma](Y',X')$ and $A_{d-1} = parent(A_{d})$ for $1 \leq d
  \leq D$, construct sets of summary result sets
  \[ \begin{array}{rcl}
    R_D = \{\GNP[\Sigma](Y',X')\} & \mbox{ and } & R_{d-1} = \{\{a\} \mathbin{op(A_{d-1})} B | a \in sibling(A_d), B \in R_d\}
  \end{array} \]
  for $1 \leq d \leq D$.  We may then perform {\em extrinsic pruning}
  if $B$ is singleton for all $B \in R_0$.
\end{lemma}
\begin{proof}
  As constructed, $R_0$ is the set of all possible summary result sets
  for $\GNP[\Sigma](Y,X)$ given summary results of
  $\GNP[\Sigma](Y',X')$ and one possible result of all other
  components.  Singleton $B \in R_0$ then implies that all possible
  results of $\GNP[\Sigma](Y',X')$ lead to the same global result
  under some valuation of the other components.  If all such $B$ are
  singleton, then any $p \in \GNP[\Sigma](Y',X')$ must lead to the
  same global result as any other regardless of the valuation of the
  other components and we are free to shortcut all further compuation
  on $X'$ and $Y'$ with this value.
\end{proof}

\begin{corellary}
  We may perform extrinsic pruning for all-nearest-neighbors.
\end{corellary}
\begin{proof}
  
\end{proof}

% \subsection{Thresholded Pruning}

{\bf Thresholded Pruning.}  Postprocessing function $h$ introduces a
third form of pruning.
\begin{lemma}
  We may perform {\em thresholded pruning} if $\{h(b) | b \in B\}$ is
  singleton for all $B \in R_0$.
\end{lemma}
\begin{proof}
  Similar to the above, with all $p \in \GNP[\Sigma](Y',X')$ leading
  to the same postprocesed result.
\end{proof}

% \subsection{Approximation Pruning}

% {\bf Approximation Pruning.}  Some problems do not lend themselves to
% any of the above forms of pruning.  For these, we may still be able to
% find approximate results with bounded error more quickly than
% exhaustive computation.  We must first establish a notion of error.
% \begin{definition}
%   Let $div \colon 2^\mathcal{A} \to \mathcal{A} \times \mathcal{A} \to
%   \mathbb{R}$ be some measure of divergence between results given the
%   summary results of the full computation, written $div(a,b |
%   \GNP[\Sigma](Y,X))$.  Define $err \colon 2^\mathcal{A} \to
%   2^\mathcal{A} \to \mathbb{R}$ to be $err(\GNP[\Sigma](Y',X') |
%   \GNP[\Sigma](Y,X)) = \min_{\widehat{a} \in \mathcal{A}} \max_{b \in
%   \GNP[\Sigma](Y',X')} div(\widehat{a},b | \GNP[\Sigma](Y,X))$.
% \end{definition}
% \noindent For example, we might have $div(a,b | \GNP[\Sigma](Y,X)) =
% |a - b| / \min_{c \in \GNP[\Sigma](Y,X)} |c|$, or relative error.
% 
% Given a desired $\epsilon$, iterative refinement may terminate once
% $err(\GNP[\Sigma](Y,X) | \GNP[\Sigma](Y,Z)) < \epsilon$, returning the
% minimizing $\widehat{a}$ found for $err$.  A simple algorithm might
% then check the error of $\GNP[\Sigma](Y,X)$ after each step, refining
% components in the order of descending error in attempt to make the
% most of its work.  This approach must use a priority queue to manage
% expansion, incuring significant overhead.
% 
% An alternate approach distributes error to the various components of
% computation.
% \begin{lemma}
%   Given summary results $A, B \subset \mathcal{A}$, let operators
%   $\otimestilde, \odottilde \colon \mathbb{R} \times \mathbb{R}
%   \to \mathbb{R}$ be such that
%   \begin{eqnarray*}
%     err(A | \GNP[\Sigma](Y,X)) \otimestilde err(B | \GNP[\Sigma](Y,X)) & = & err(A \otimeshat B | \GNP[\Sigma](Y,X)) \\
%     err(A | \GNP[\Sigma](Y,X)) \odottilde err(B | \GNP[\Sigma](Y,X)) & = & err(A \odothat B | \GNP[\Sigma](Y,X)).
%   \end{eqnarray*}
%   Given some desired $\epsilon$, let $\epsilon^*$ be such that
%   $\bigodottilde_{y \in Y} \bigotimestilde_{x \in X} \epsilon^* =
%   \epsilon$ and let $\epsilon' = \bigodottilde_{y \in Y'}
%   \bigotimestilde_{x \in X'} \epsilon^*$.  We may perform {\em
%   approximation pruning} when $err(\GNP[\Sigma](Y',X') |
%   \GNP[\Sigma](Y,X)) < \epsilon'$.
% \end{lemma}

% \subsection{Practical Considerations: Trees, Bounds}

\section{Practical Considertaions}

[[Too expensive to decide how to decompose blocks on an individual
basis, so reused precomputed tree splits when breaking up work.  Trees
also handy to find bottom-up statistics and store intermediate
results.]]

\appendix

% \section{Full Permutability}
% 
% \begin{definition}
%   A regular reduce problem is {\em fully permutable} if, for all
%   permutations $p_1,\ldots,p_n$ of the numbers $1,\ldots,n$,
%   \[
%   \Op{1}_{x_1 \in X'_1}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) = \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}f(x_1,\ldots,x_n).
%   \]
% \end{definition}
% 
% \begin{theorem}
%   Full permutability is logically equivalent to exchangeability (and
%   thus block decomposability).
% \end{theorem}
% 
% \begin{proof}
%   ($\Rightarrow$) Given selected split $i$ and partitions $X^{\!L}_i \cup
%   X^{\!R}_i = X_i$, we have
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(X'_1,\ldots,X'_i,\ldots,X'_n)} \\
%     & = & \displaystyle \Op{1}_{x_1 \in X'_1}\cdots\Op{i}_{x_i \in X'_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{i}_{x_i \in X'_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{i}_{x_i \in X^{\!L}_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & & \displaystyle \mbox{} \op{i} \Op{i}_{x_i \in X^{\!R}_i}\Op{1}_{x_1 \in X'_i}\cdots\Op{i-1}_{x_{i-1} \in X'_{i-1}}\Op{i+1}_{x_{i+1} \in X'_{i+1}}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \Op{1}_{x_1 \in X'_i}\cdots\Op{i}_{x_i \in X^{\!L}_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & & \displaystyle \mbox{} \op{i} \Op{1}_{x_1 \in X'_i}\cdots\Op{i}_{x_i \in X^{\!R}_i}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n) \\
%     & = & \displaystyle \GNP(X'_1,\ldots,X^{\!L}_i,\ldots,X'_n) \op{i} \GNP(X'_1,\ldots,X^{\!R}_i,\ldots,X'_n).
%   \end{array}
%   \]
%   ($\Leftarrow$) Given permutation $p_1,\ldots,p_n$ of the numbers
%   $1,\ldots,n$, with $x^t_{p_i}$ denoting successive elements of
%   $X'_{p_1}$ and $X^t_{p_i} = \{x^1_{p_i},\ldots,x^t_{p_i}\}$, $1 \leq
%   t \leq |X'_{p_i}|$, and the understanding that permuted inputs are
%   mapped to the appropriate arguments of $\GNP$, we have
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X^t_{p_i},\ldots,X'_n)} \\
%     & = & \GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X^{t-1}_{p_i},\ldots,X'_n) \\
%     & & \mbox{} \op{p_i} \GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,\{x^t_{p_i}\},\ldots,X'_n).
%   \end{array}
%   \]
%   Induction over $t$ from $|X_{p_i}|$ down to $2$ yields
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\GNP(\{x_{p_1}\},\ldots,\{x_{p_{i-1}}\},X'_1,\ldots,X'_{p_i},\ldots,X'_n)} \\
%     & = & \displaystyle \Op{p_i}_{x_{p_i} \in X'_{p_i}}\GNP(\{x_{p_1}\},\ldots,\{x_{p_i}\},X'_1,\ldots,X'_{p_{i+1}},\ldots,X'_n).
%   \end{array}
%   \]
%   Further induction over $i$ from $1$ to $n$ yields
%   \[
%   \begin{array}{rcl}
%     \lefteqn{\Op{1}_{x_1 \in X'_1}\cdots\Op{n}_{x_n \in X'_n}f(x_1,\ldots,x_n)} \\
%     & = & \GNP(X'_1,\ldots,X'_n) \\
%     & = & \displaystyle \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}\GNP(\{x_{p_1}\},\ldots,\{x_{p_n}\}) \\
%     & = & \displaystyle \Op{p_1}_{x_{p_1} \in X'_{p_1}}\cdots\Op{p_n}_{x_{p_n} \in X'_{p_n}}f(x_1,\ldots,x_n).
%   \end{array}
%   \]
% \end{proof}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper.

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references.

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
