\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0 << /S /r >> }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\MakeUppercase  {List of Tables}}{iii}{section*.4}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\MakeUppercase  {List of Figures}}{iv}{section*.6}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\HyPL@Entry{5 << /S /D >> }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Section} 1}\MakeUppercase  {Introduction}}{1}{section.8}}
\newlabel{intro}{{1}{1}{Introduction\relax }{section.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  1.1}{\ignorespaces Kernel Density of 2 dimensional points. Over every point a kernel is set. In areas with with high concentration of points there is more overlap on the kernels which shows higher probablility}}{2}{figure.11}}
\newlabel{2d_kernel_density_estimation}{{1.1}{2}{Kernel Density of 2 dimensional points. Over every point a kernel is set. In areas with with high concentration of points there is more overlap on the kernels which shows higher probablility\relax }{figure.11}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Section} 2}\MakeUppercase  {Origin and history of the problem}}{2}{section.12}}
\newlabel{origin}{{2}{2}{Origin and history of the problem\relax }{section.12}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Section} 3}\MakeUppercase  {Preliminary Research}}{4}{section.13}}
\newlabel{prelim}{{3}{4}{Preliminary Research\relax }{section.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Nearest Neighbor Problem}{4}{subsection.14}}
\newlabel{The_Nearest_Neighbor_Problem}{{3.1}{4}{The Nearest Neighbor Problem\relax }{subsection.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Multidimensional Trees}{4}{subsubsection.15}}
\newlabel{Multidimensional_Trees}{{3.1.1}{4}{Multidimensional Trees\relax }{subsubsection.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Kd trees}{5}{subsubsection.16}}
\newlabel{Kd_trees}{{3.1.2}{5}{Kd trees\relax }{subsubsection.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Data structures for Kd trees}{5}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.1}{\ignorespaces The parent node of a 2 dimensional kd-tree}}{6}{figure.18}}
\newlabel{parent_kd_node}{{3.1}{6}{The parent node of a 2 dimensional kd-tree\relax }{figure.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Ball trees}{6}{subsubsection.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.2}{\ignorespaces A two node 2 dimensional kd-tree}}{7}{figure.19}}
\newlabel{2_kd_node}{{3.2}{7}{A two node 2 dimensional kd-tree\relax }{figure.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Nearest Neighbor Algorithm}{7}{subsubsection.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.3}{\ignorespaces Nearest neighbor with pruning}}{8}{figure.20}}
\newlabel{nearest_neighbor_searchon_kd}{{3.3}{8}{Nearest neighbor with pruning\relax }{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.4}{\ignorespaces True nearest neighbor is out of the leaf}}{9}{figure.21}}
\newlabel{nearest_neighbor_searchon_kd_true_neighbor}{{3.4}{9}{True nearest neighbor is out of the leaf\relax }{figure.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.5}{\ignorespaces  Kd tree with no possible pruning}}{10}{figure.22}}
\newlabel{nearest_neighbor_searchon_kd_no_prune}{{3.5}{10}{ Kd tree with no possible pruning\relax }{figure.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.6}{\ignorespaces A two dimensional pathological kd-tree}}{11}{figure.23}}
\newlabel{kd_pathological_points}{{3.6}{11}{A two dimensional pathological kd-tree\relax }{figure.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}All nearest Neighbors}{11}{subsubsection.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.7}{\ignorespaces In this case the partition is very bad and almost for every point pruning is not feasible}}{12}{figure.24}}
\newlabel{pathological_kd_tree}{{3.7}{12}{In this case the partition is very bad and almost for every point pruning is not feasible\relax }{figure.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Large scale trees, for out of core memory}{13}{subsection.30}}
\newlabel{Large_scale_trees}{{3.2}{13}{Large scale trees, for out of core memory\relax }{subsection.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Memory layout of Kd-trees}{14}{subsubsection.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}System architecture}{15}{subsubsection.32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Memory Manager Architecture}{15}{subsubsection.33}}
\@writefile{toc}{\contentsline {paragraph}{Memory Mapped Files}{16}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{User defined cache, with TPIE}{16}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Kernel methods}{17}{subsection.36}}
\newlabel{Kernel_methods}{{3.3}{17}{Kernel methods\relax }{subsection.36}{}}
\citation{Belkin}
\citation{Lafon}
\citation{jenssen17lpd}
\citation{Silverman}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Kernel principal component analysis}{18}{subsubsection.37}}
\newlabel{Kernel_principal_component_analysis}{{3.3.1}{18}{Kernel principal component analysis\relax }{subsubsection.37}{}}
\citation{girolami2002osd}
\citation{Lafon}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Geometric diffusion on a manifold}{19}{subsubsection.41}}
\newlabel{kernel}{{3.5}{20}{Geometric diffusion on a manifold\relax }{equation.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Kernel PCA on a manifold}{20}{subsubsection.43}}
\newlabel{Kernel_PCA_on_a_manifold}{{3.3.3}{20}{Kernel PCA on a manifold\relax }{subsubsection.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Parameter estimation for manifold learning, through density estimation}{21}{subsection.44}}
\citation{Silverman}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Non parametric density estimation}{22}{subsubsection.45}}
\newlabel{powerdensity}{{3.6}{22}{Non parametric density estimation\relax }{equation.46}{}}
\citation{Cvetkovic}
\citation{Silverman}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Eigenvalue analysis of the diffusion graph}{24}{subsubsection.48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Kernel choice}{24}{subsubsection.49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Examples}{24}{subsubsection.51}}
\citation{jenssen17lpd}
\citation{gray2001nbp}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.5}Discussion}{26}{subsubsection.52}}
\citation{quatieri2002dts}
\citation{bentley1975bst}
\citation{moore2000ahu}
\citation{gray2000nbp}
\citation{ravindran:inr}
\citation{jansen2006ifa}
\citation{scholkopf:nca}
\citation{coifman2006dm}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Speech recognition with nearest neighbor search}{27}{subsection.59}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Introduction}{27}{subsubsection.60}}
\citation{shamma_main}
\citation{moore-tutorial}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Speech features}{28}{subsubsection.61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Fast N-body methods for feature comparison}{29}{subsubsection.62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Experimental Results}{29}{subsubsection.66}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.1}{\ignorespaces Pseudo-code for the dual-tree all nearest neighbor algorithm}}{30}{table.63}}
\newlabel{dualtree_algorithm}{{3.1}{30}{Pseudo-code for the dual-tree all nearest neighbor algorithm\relax }{table.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.5}All nearest neighbor performance}{30}{subsubsection.67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.6}Comparison of NRAF and MFCC}{30}{subsubsection.68}}
\citation{ravindran:inr}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.7}Discussion}{32}{subsubsection.70}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.2}{\ignorespaces CPU time (in hours) for evaluating all 20 nearest neighbors on TIMIT database}}{32}{table.73}}
\newlabel{timing}{{3.5.7}{32}{Discussion\relax }{table.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.3}{\ignorespaces Nearest k-neighbor classfier for TIMIT database}}{32}{table.74}}
\newlabel{kneighborTIMIT}{{3.5.7}{32}{Discussion\relax }{table.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.4}{\ignorespaces Nearest k-neighbor classfier for NTIMIT database}}{32}{table.75}}
\newlabel{kneighborNTIMIT}{{3.5.7}{32}{Discussion\relax }{table.75}{}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Section} 4}\MakeUppercase  {Proposed Research}}{32}{section.78}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.5}{\ignorespaces Nearest k-neighbor classfier for Leave One Speaker out Cross Validation in TIMIT database}}{33}{table.76}}
\newlabel{kneighborLOOCVTIMIT}{{3.5.7}{33}{Discussion\relax }{table.76}{}}
\@writefile{lot}{\contentsline {table}{\numberline {Table\nobreakspace  3.6}{\ignorespaces Nearest k-neighbor classfier for Leave One Speaker Out Cross validation in NTIMIT database}}{33}{table.77}}
\newlabel{kneighborLOOCVNTIMIT}{{3.5.7}{33}{Discussion\relax }{table.77}{}}
\newlabel{proposed}{{4}{33}{Proposed Research\relax }{section.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Customized Kernels}{33}{subsection.79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Fast kernel summation with trees}{35}{subsection.87}}
\newlabel{kernel_sum}{{4.1}{35}{Fast kernel summation with trees\relax }{equation.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Computing Gaussian Kernel Matrix with trees}{36}{subsection.92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Speech recognition with nearest neighbor search}{36}{subsection.93}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Acoustic Environment adaptation}{36}{subsection.94}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Section} 5}\MakeUppercase  {Work remaining to be done}}{36}{section.95}}
\newlabel{remains}{{5}{36}{Work remaining to be done\relax }{section.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.8}{\ignorespaces The query node in red after top down recursion ends up in a leaf of the reference tree. Then every point in the query node (red) finds with the naive method its candidate nearest neighbor. Then for all of them we compare all the candidate nearest distances and find the maximum $r_{max}$. Now we know that if there is any node in distance greater than $r_max$ there is no point in checking for candidate nearest neighbors. As we see in the right figure the dashed box doesn't intersect with the bounding box of the leaf. This means that we can prune the yellow boxes.}}{37}{figure.29}}
\newlabel{dual_tree}{{3.8}{37}{The query node in red after top down recursion ends up in a leaf of the reference tree. Then every point in the query node (red) finds with the naive method its candidate nearest neighbor. Then for all of them we compare all the candidate nearest distances and find the maximum $r_{max}$. Now we know that if there is any node in distance greater than $r_max$ there is no point in checking for candidate nearest neighbors. As we see in the right figure the dashed box doesn't intersect with the bounding box of the leaf. This means that we can prune the yellow boxes}{figure.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.9}{\ignorespaces The same data with fig.\nobreakspace  {}\ref  {fig1}a analyzed with a larger global bandwidth. (a) The estimated density, (b) The first non-trivial eigenvector}}{38}{figure.53}}
\newlabel{fig1.1}{{3.9}{38}{The same data with fig.~\ref {fig1}a analyzed with a larger global bandwidth. (a) The estimated density, (b) The first non-trivial eigenvector\relax }{figure.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.10}{\ignorespaces (a) Two clusters of uniformly distributed 2-D points, (b) The estimated density, (c) The first non-trivial eigenvector}}{38}{figure.54}}
\newlabel{fig1}{{3.10}{38}{(a) Two clusters of uniformly distributed 2-D points, (b) The estimated density, (c) The first non-trivial eigenvector\relax }{figure.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.11}{\ignorespaces (a) Two clusters of non-uniformly distributed 2-D points, (b)The estimated density, (c) The first non-trivial eigenvector}}{39}{figure.55}}
\newlabel{fig2}{{3.11}{39}{(a) Two clusters of non-uniformly distributed 2-D points, (b)The estimated density, (c) The first non-trivial eigenvector\relax }{figure.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.12}{\ignorespaces The same data set with fig.\nobreakspace  {}\ref  {fig2} processed with a bandwidth one order of magnitude larger. Although the first eigenvector still separates the two classes, the classes are closer and not compact (a) Two clusters of non-uniformly distributes 2-D points, (b)The estimated density, (c) The first non trivial eigenvector}}{39}{figure.56}}
\newlabel{fig3}{{3.12}{39}{The same data set with fig.~\ref {fig2} processed with a bandwidth one order of magnitude larger. Although the first eigenvector still separates the two classes, the classes are closer and not compact (a) Two clusters of non-uniformly distributes 2-D points, (b)The estimated density, (c) The first non trivial eigenvector\relax }{figure.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.13}{\ignorespaces (a) The 3 phoneme classes after the dimensionality reduction with the optimal bandwidth, plotted in two dimensions, (b) The 3 phoneme classes with a larger bandwidth,(c) The 3 phoneme classes with a smaller bandwidth}}{39}{figure.57}}
\newlabel{fig4}{{3.13}{39}{(a) The 3 phoneme classes after the dimensionality reduction with the optimal bandwidth, plotted in two dimensions, (b) The 3 phoneme classes with a larger bandwidth,(c) The 3 phoneme classes with a smaller bandwidth\relax }{figure.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.14}{\ignorespaces The diffusion graph for three different local bandwidths (a)Optimal, (b)An order of magnitude larger, (c)An order of magnitude smaller}}{40}{figure.58}}
\newlabel{fig5}{{3.14}{40}{The diffusion graph for three different local bandwidths (a)Optimal, (b)An order of magnitude larger, (c)An order of magnitude smaller\relax }{figure.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.15}{\ignorespaces A two dimensional kd-tree}}{40}{figure.64}}
\newlabel{kdtree}{{3.15}{40}{A two dimensional kd-tree\relax }{figure.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.16}{\ignorespaces Simulation of the dual tree algorithm}}{40}{figure.65}}
\newlabel{dualkdtree}{{3.16}{40}{Simulation of the dual tree algorithm\relax }{figure.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.17}{\ignorespaces Principal Component Analysis of TIMIT (top) and NTIMIT (bottom) for MFCC (solid line) and for NRAF (dashed line).}}{41}{figure.71}}
\newlabel{pca}{{3.17}{41}{Principal Component Analysis of TIMIT (top) and NTIMIT (bottom) for MFCC (solid line) and for NRAF (dashed line)}{figure.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {Figure\nobreakspace  3.18}{\ignorespaces Kernel PCA spectrum for TIMIT MFCC features}}{41}{figure.72}}
\newlabel{kernel_pca}{{3.18}{41}{Kernel PCA spectrum for TIMIT MFCC features\relax }{figure.72}{}}
\bibstyle{ieeetr}
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {\MakeUppercase  {Appendix} A}\MakeUppercase  {This is my first appendix}}{42}{section.96}}
\bibdata{template}
\@writefile{toc}{\setcounter{tocdepth}{3}}
\@writefile{toc}{\contentsline {section}{\MakeUppercase  {References}}{43}{section*.97}}
\@writefile{toc}{\setcounter {tocdepth}{2}}
