\documentclass[twoside,leqno, 12pt]{article}
\usepackage{ltexpprt}

%\documentstyle[nips07submit_09,times]{article}

\usepackage{amsmath,amssymb}
\usepackage{graphicx, subfigure,wrapfig}
\usepackage[left=1in,right=1in,top=1in,bottom=1in,nohead,nofoot]{geometry}
\usepackage[font=footnotesize]{caption}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newcommand{\spcA}{\hspace*{0in}}
\newcommand{\spcB}{\hspace*{.1in}}
\newcommand{\spcC}{\hspace*{.2in}}
\newcommand{\spcD}{\hspace*{.3in}}
\newcommand{\spcE}{\hspace*{.4in}}
\newcommand{\spcF}{\hspace*{.5in}}
\newcommand{\spcG}{\hspace*{.6in}}


%\title{NSF Research Proposal}
%\author{Bill March}
\date{}                                       

\begin{document}
%\maketitle
%\begin{center}
%\Large{Previous Research}
%\end{center}
%\vspace{-0.25in}

%%%%%  Instructions %%%%%%%%%
%Describe any scientific research activities in which you have participated, such as experience in undergraduate research programs, or research experience gained through summer or part-time employment or in work-study programs, or other research activities, either academic or job-related. Explain the purpose of the research and your specific role in the research, including the extent to which you worked independently and/or as part of a team, and what you learned from your research. In your statement, distinguish between undergraduate and graduate research experience. 

%If you have no direct research experience, describe any activities that you believe have prepared you to undertake research. At the end of your statement, list any publications and/or presentations made at national and/or regional professional meetings. 

%%%%%%%%%% Opening %%%%%%%%%%%%

\section*{Previous Research}
%% Needs work
% Make this a paragraph about research interests and overview
% Make last paragraph a summary of other activities 
\textbf{Current work.}  Since summer 2006, I have worked as a research assistant for Alexander Gray's FASTlab.  I have explored fast algorithms for problems in computational geometry, including convex hulls, path finding, and minimum spanning trees.  I have spent much of my time investigating scientific applications for these algorithms, especially in biology and chemistry.  As a graduate student, I am investigating ways to apply my earlier techniques to designing faster and more accurate computational chemistry algorithms.  

%I have participated in research during my last two years as an undergraduate.  I have studied fast algorithms for several geometry and graph problems and explored applications of these algorithms.  I participated in the NSF Research Experience for Undergraduates at Georgia Tech in Summer 2005, where I explored fast algorithms for online graph coloring.  Since the Summer of 2006, I have been working with Alex Gray on fast algorithms for machine learning and scientific applications.  I have also been investigating applications of these algorithms in science, particularly computational biology and chemistry.  

\textbf{Past research.}  Previously, during Summer 2005, I participated in the NSF Research Experience for Undergraduates program at Georgia Tech.  I studied online graph coloring algorithms with Prof. Trotter in the School of Mathematics.  This experience gave me valuable experience with reading papers, organizing a complex research topic, and exploring new ideas.  However, I found mathematics too far removed from the scientific applications I was interested in, which led me to my current work.

%Maybe need some kind of transition into this. . .
% Maybe not, to save space

%%%%%%%% Hierarchical Clustering %%%%%%%%%%
\vspace{-0.1in}
\section{Euclidean Minimum Spanning Trees}

\textbf{Context.}  Euclidean Minimum Spanning Trees are fundamental structures in computational geometry and are widely applied in network design, optimization, and computer vision.  Additionally, the EMST can be used to find a hierarchical clustering of the underlying points.  This method of clustering is commonly applied throughout science and is particularly popular for clustering gene microarray data \cite{eisen_cluster} and objects in deep-space surveys \cite{barrow}.  

The EMST problem is well-studied, and there are effective algorithms for many cases, such as in two dimensions.  However, none of the existing algorithms are truly scalable to massive data sets and arbitrary dimensions.  New algorithms are necessary to handle large-scale data gathering efforts, such as the Sloan Digital Sky Survey and the Human Genome Project.  Working with Alexander Gray and his FASTlab, I was able to develop and implement the fastest existing algorithm for EMST in arbitrary dimensions \cite{my_emst}.
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-0.2in}
\includegraphics[width=0.45\textwidth]{5_3_all_matlab.eps}
\vspace{-0.3in}
\caption{Running-time comparison on clustered synthetic data.}
\label{runtime}
\end{wrapfigure}


% These paragraphs need some revision
\vspace{-0.18in}
\textbf{Teamwork.}  My first work with the FASTlab was on extending some of the lab's previous work to design a fast algorithm for the EMST problem.  Since I was new, I relied on discussions with the lab's graduate students to become familiar with existing approaches and learn the code base.  Throughout the project, I often discussed ideas and difficulties with Dr. Gray and the other students in the lab.  These interactions often allowed me to find ways around difficulties.


\textbf{Individual work.}  I did most of the work individually.  I searched the existing literature to determine the best existing algorithms.  I determined the link between the previous work and the EMST problem and worked out the details of my own EMST algorithm.  I wrote the code for my algorithm and the competitors, then organized and carried out the comparisons.  I have submitted a conference paper on my algorithm which is currently under review.  

%Include a picture of a kd-tree, etc.  All the details really exist to set up the proposal.
% Details the proposal needs:
% dual- and multi-tree recursion - stick to dual here, bring up multi in the proposal
% Some idea of approximation, hard to introduce here
%
% Here: dual-tree recursion, space partitioning trees
% Proposal: dual leads to multi-tree, approximations, multipole expansions

% Quick, Ryan-like explanation of dual-tree, bounds, iterative refinement
% Applied here to quickly find the nearest neighbors of components

% I don't like the layout of this much, see what Alex thinks
% Need to explain multi-tree recursion better

\textbf{Algorithm details.}  I applied an existing framework, known as \emph{generalized $N$-body problems} \cite{gray_nbody}, to finding EMST's.  This framework has been previously used to create fast algorithms for many problems in statistics and machine learning including the $N$-point correlation, all-nearest-neighbors, and kernel density estimation.  Using these ideas, I was able to design and implement the fastest existing algorithm for EMST's in any dimension.  
 
My algorithm, \textsc{DualTreeBoruvka}, accelerates Bor\r{u}vka's algorithm.  Bor\r{u}vka's algorithm maintains a spanning forest and connects each component to its nearest neighbor in each step.  I applied the space-partitioning trees and multi-tree recursion ideas to quickly compute the nearest neighbor of each component.  In particular, the algorithm uses a \emph{$kd$-tree}, which maintains a hyper-rectangle or bounding-box around each node.


%The algorithm uses a \emph{$kd$-tree} to organize the points.   Each node of the tree consists of a hyper-rectangle or bounding box containing a subset of the data.  The root node contains the entire data set.  Children are created as follows: choose the longest dimension of the current node's bounding box, partition the data along the midpoint in this dimension, and form two new, smaller bounding boxes to cover the subsets.  The tree forms leaves when a node contains fewer than some specified number of points.  

\textbf{Dual-tree Recursion.}  The simplest way to compute the nearest neighbor of a component $Q$ is to find the nearest neighbor of all points $q \in Q$ and keep the closest.  For each $q$, we store the distance to the nearest neighbor found so far, $d^u(q)$, as an upper bound on the true distance.  Using the $kd$-tree, the algorithm can compare a point $q$ with an entire node $R$.  Using the bounding box of $R$, the algorithm can compute a lower bound $d^l(q, R) < d(q, r)$ for all $r \in R$.  Then, if $d^l(q, R) > d^u(q)$ for some $R$, it can \emph{prune} any further consideration of points in $R$.  Otherwise, recursively consider the children of $R$.

This idea can be extended even further.  Since Bor\r{u}vka's algorithm needs the nearest neighbor of each component $Q$, my algorithm exploits the fact that these points are also grouped in the $kd$-tree.  it maintains an upper bound $d^u(Q)$ on $d^u(q)$ for all points $q \in Q$.  It compares nodes $Q$ and $R$, computes the minimum distance between their bounding boxes: $d^l(Q,R)$, and prunes if $d^l(Q, R) > d^u(Q)$.  Otherwise, it recursively compares the children of $Q$ with the children of $R$.  By accounting for points in the same component, this method can quickly and efficiently solve the problem of finding each component's nearest neighbor.  



\textbf{Results}
This method proved to be the fastest known algorithm for finding EMST's in general metric spaces.  I compared it on a range of synthesized data to several well known MST algorithms, including \textsc{GeoMST2} \cite{narzhuzac:00}, the previous fastest algorithm.  Since this research was motivated by scientific applications, especially cosmology and computational biology, I also tested my algorithm on two large data sets from these areas.  These experiments clearly demonstrated the scalability of my algorithm in terms of running time and storage requirements.
\begin{wraptable}{r}{0.55\textwidth}
\vspace{-0.2in}
\begin{tabular}{|c|c|c|c|c|} \hline
dim & $N$ & \textsc{GeoMST2} & \textsc{DTB} & Speedup \\ \hline \hline
3 & 389354 & 78.0 & 16.9 & \textbf{4.6} \\ \hline 
12 & 320000 & 702 & 62.3 & \textbf{11.3} \\ \hline 
\end{tabular}
\vspace{-0.1in}
\caption{\emph{Running times on three-dimensional spectral data from the SDSS and compressed representation of protein folding simulations.}}
\end{wraptable}

%What did I learn?  How does it help the proposal? 
% Should this go here, or earlier
% If it doesn't go here, how will I conclude this one?
\vspace{-0.18in}
\textbf{Experience.}  Through my research involvements, particularly designing my EMST algorithm, I 
gained experience with all aspects of algorithm design.  I searched and compared the existing methods and explored the scientific literature for applications.  I gained experience with powerful algorithmic techniques and the challenges associated with applying them to new problems.  I also have worked on communicating my research through a conference submission.  These experiences, particularly with these algorithmic methods, are crucial to my current work with fast algorithms for computational chemistry problems.


%%%%%% List publications and presentations %%%%%%%%
%\section{Publications}

%EMST paper (submitted).


%\bibliographystyle{abbrv}
%\bibliography{NSF_previous}

\begin{thebibliography}{99}

\footnotesize
\bibitem{my_emst}
W. March and A. Gray.  \emph{Large Scale Euclidean MST and Hierarchical Clustering.}  Submitted to SIAM International Conference on Data Mining 2008.

\bibitem{barrow}
J. Barrow \emph{et al.}  \emph{Minimal spanning trees, filaments and galaxy clustering.}  MNRAS 216 (1985).  pp.~17--35.

\bibitem{eisen_cluster}
M. Eisen \emph{et al.}  \emph{Cluster analysis and display of genome-wide expression patterns.}  PNAS 95 (1998).  pp.~14863--14868.
%@article{eisen_cluster,
%	Author = {Eisen, Michael B. and Spellman, Paul T. and Brown, Patrick O. and Botstein, David},
%	Date-Added = {2007-11-02 10:35:54 -0400},
%	Date-Modified = {2007-11-02 10:35:54 -0400},
%	Doi = {10.1073/pnas.95.25.14863},
%	Eprint = {http://www.pnas.org/cgi/reprint/95/25/14863.pdf},
%	Journal = {PNAS},
%	Number = {25},
%	Pages = {14863-14868},
%	Title = {{Cluster analysis and display of genome-wide expression patterns}},
%	Url = {http://www.pnas.org/cgi/content/abstract/95/25/14863},
%	Volume = {95},
%	Year = {1998}}


\bibitem{narzhuzac:00}
G. Narasimhan, J. Zhu, and M. Zachariasen.  \emph{Experiments with Computing Geometric Minimum Spanning Trees.}  In Proceedings of ALENEX'00.  (2000)  pp.~183--196.
%@inproceedings{narzhuzac:00,
%	Author = {G. Narasimhan and J. Zhu and M. Zachariasen},
%	Booktitle = {Proceedings of ALENEX'00},
%	Date-Added = {2007-10-31 13:55:17 -0400},
%	Date-Modified = {2007-10-31 13:55:17 -0400},
%	Keywords = {EMST},
%	Pages = {183-196},
%	Title = {{Experiments with Computing Geometric Minimum Spanning Trees}},
%	Year = {2000}}


\bibitem{gray_nbody}
A. Gray and A. Moore.  \emph{N-body problems in statistical learning.}  In NIPS 13.  (2001)
%@inproceedings{gray_nbody,
%	Author = {A. Gray and A. W. Moore},
%	Booktitle = {NIPS 13},
%	Date-Added = {2007-10-29 16:01:04 -0400},
%	Date-Modified = {2007-10-29 16:01:04 -0400},
%	Editor = {Leen, Todd K. and Dietterich, Thomas G. and Tresp, Volker},
%	Keywords = {N-body},
%	Local-Url = {file://localhost/Users/Antoninus/Desktop/general%20FASTLAB/nips-final.pdf},
%	Title = {N-Body Problems in Statistical Learning},
%	Year = {2001}}

\end{thebibliography}
\end{document}