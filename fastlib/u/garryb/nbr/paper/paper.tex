% To-do list
%
% NEED INTERNET:
% - SUBMIT PAPER'S ABSTRACT
% - **What kind of infiniband????
% - Call Grandma Boyer
% - Vacation time
% /- Install gnuplot
% /- Ask Ryan to ping Matt about Maquis
% /- Acknowledge LCDM
% /- Acknowledge SDSS
% /- Acknowledge TIMIT
% /- Ask Ryan to contact Frey and Dueck
% - Intel donations - check website and/or ask Wolf
% /- Megahertz
% ...- Grab uniprocessor results
% ...- Timit experiments
% /- Timit dimensionality is 10
% ...- Find a computer with more than 2 processors
% - Results section
% ...- Fix references
% - Allnn experiments
% ?/- Remove LRU/FIFO
% NO INTERNET
% - consistent $h$ and $w$ for two-point correlation
% - Look at ag-nips for introduction help, possibly cite Gray's [2,6] for mrkd tree, with cached sufficient statistics talked about in [8]

%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[twoside,leqno,twocolumn]{article}  
\usepackage{siamdm} 
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\newcommand{\authornote}[1]{\footnote{Note to self: #1}}
\newcommand{\authorsnote}[1]{\authornote{#1}}
\newcommand{\com}[1]{{\small \textit{((#1))}}}

\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\newcommand{\Union}{\bigcup}
\newcommand{\Intersect}{\bigcap}
\newcommand{\bigvec}[1]{\mathop{\overrightarrow{#1}}}

%\newcommand{\otimeshat}{\widehat{\otimes}}
%\newcommand{\odothat}{\widehat{\odot}}

\newcommand{\prefsplit}[2]{#1 \succ #2}
\newcommand{\summary}{\delta}
%hat{\sigma}}

\DeclareMathOperator*{\map}{map}
\DeclareMathOperator*{\worst}{worst}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\TWOPT}{TWOPOINT}
\DeclareMathOperator{\cardinality}{cardinality}
\DeclareMathOperator{\hrect}{hrect}
\DeclareMathOperator{\child}{child}
\DeclareMathOperator{\visited}{visited}
\DeclareMathOperator{\unvisited}{unvisited}
\DeclareMathOperator{\prune}{prune}
\DeclareMathOperator{\IF}{if}
\DeclareMathOperator{\ATDISCRETION}{}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\eqn}[1]{Equation~\ref{eqn:#1}}

\newcommand{\Gnp}{\Psi}
\newcommand{\gnp}{\psi}

%\newcommand{\psty}{\scriptstyle}
\newcommand{\psty}{}
\newcommand{\X}{\\ \psty}
\newcommand{\x}{\X \hspace{0.13in}}
\newcommand{\xx}{\X \hspace{0.26in}}
\newcommand{\xxx}{\X \hspace{0.39in}}
\newcommand{\xxxx}{\X \hspace{0.52in}}

%\newcommand{\mysubsub}[1]{{\bf #1.}  }
%\newcommand{\mysub}[1]{{\bf #1.}  }
\newcommand{\mysubsub}[1]{\subsubsection{#1} }
\newcommand{\mysub}[1]{\subsection{#1} }

\newcommand{\defterm}[1]{{\bf #1}}
%\newcommand{\nbody}{$N$-body}

\newcommand{\kdroot}[1]{#1^{\text{root}}}
\newcommand{\kdleft}[1]{#1^{\!L}}
\newcommand{\kdright}[1]{#1^{\!R}}
\newcommand{\kdparent}[1]{#1^{\!P}}

\newcommand{\lo}[1]{#1^{l}}
\newcommand{\up}[1]{#1^{u}}
\newcommand{\distlo}{\lo{d}}
\newcommand{\distup}{\up{d}}
\newcommand{\dist}[2]{d(#1,#2)}

%\newcommand{\myOp}[1]{\mathop{\bigotimes\nolimits\hspace{-0.045in}_{#1}}}
\newcommand{\nameOp}[2]{\mathop{#1\nolimits\!\!_{#2}}}
\newcommand{\nameop}[2]{#1_{\!#2}}
\newcommand{\myOp}[1]{\nameOp{\bigotimes}{#1}}
%\newcommand{\myop}[1]{\otimes\hspace{-0.04in}_{#1}\hspace{0.03in}}
\newcommand{\myop}[1]{\nameop{\otimes}{#1}}
\newcommand{\myOutop}[1]{\nameOp{\bigodot}{#1}}
\newcommand{\myoutop}[1]{\nameop{\odot}{#1}}

\newcommand{\letterglob}{\psi}
\newcommand{\outglob}{\Psi}
\newcommand{\inglob}{\psi}
\newcommand{\Opglob}{\myOp{\letterglob}}
\newcommand{\opglob}{\myop{\letterglob}}
\newcommand{\fglob}{f_{\letterglob}}
\newcommand{\gglob}{g_{\letterglob}}
\newcommand{\canpruneglob}{C_{\letterglob}}
\newcommand{\deltaglob}{\summary_{\letterglob}}

\newcommand{\letterqr}{v}
\newcommand{\outqr}{V}
\newcommand{\inqr}{v}
\newcommand{\Opqr}{\myOp{\letterqr}}
\newcommand{\opqr}{\myop{\letterqr}}
\newcommand{\fqr}{f_{\letterqr}}
\newcommand{\gqr}{g_{\letterqr}}

\newcommand{\letterqrv}{v}
%\newcommand{\outqrv}{\vec{\rho}}
\newcommand{\inqrv}{v}
%\newcommand{\fqrv}{f_{\letterqrv}}
%\newcommand{\gqrv}{g_{\letterqrv}}
\newcommand{\deltaqrv}{\summary_{\letterqrv}}
\newcommand{\canpruneqrv}{C}%_{\letterqrv}}
\newcommand{\varqr}{\hat{\letterqr}}
%\newcommand{\varqrv}{\letterqrv^{\:C\!}}
\newcommand{\varqrvparent}{\letterqrv^{P}}

\newcommand{\lettermu}{e}
\newcommand{\inmu}{e}
\newcommand{\Outopmu}{\nameOp{\bigodot}{\lettermu}}%\mathop{\widehat{\bigodot\nolimits}\!\scriptstyle{\mu}}}
\newcommand{\outopmu}{\myop{\lettermu}}
\newcommand{\Opmu}{\myOp{\lettermu}}
\newcommand{\opmu}{\myop{\lettermu}}
\newcommand{\fmu}{f_{\lettermu}}
\newcommand{\fmuv}{f_{\lettermu}}
\newcommand{\deltamu}{\summary_{\lettermu}}
\newcommand{\canprunemu}{C}
\newcommand{\heurqr}{H}
\newcommand{\varmuchild}{\hat{\lettermu}}
%\newcommand{\varmuchild}{\lettermu^{C}}
\newcommand{\varmuparent}{\lettermu^{U\!}}

%\newcommand{\muparent}{\inmu_{\text{coarse}}}
%\newcommand{\muchild}{\inmu_{\text{children}}}
%\newcommand{\muvisit}{\inmu_{\text{visited}}}
%\newcommand{\muall}{\inmu_{\text{all}}}

%\newcommand{\hatpi}{\hat{\outpi}}
%\newcommand{\piparent}{\outpi_{\text{parent}}}

\newcommand{\letterstat}{s}
\newcommand{\namestat}[1]{\sigma_{\text{#1}}}
\newcommand{\outstat}{\sigma}
\newcommand{\instat}{s}
\newcommand{\Opstat}{\myOp{\letterstat}}
\newcommand{\opstat}{\myop{\letterstat}}
\newcommand{\fstat}{f_{\letterstat}}
\newcommand{\gstat}{g_{\letterstat}}

% Affinity propagation


\newcommand{\eqspace}{\!\!\!\!}
\newcommand{\true}{\text{true}}
\newcommand{\ocpos}[1]{c^{+}_{#1}}
\newcommand{\ocneg}[1]{c^{-}_{#1}}
\newcommand{\cpos}[2]{\ocpos{#1 \neq #2}}
\newcommand{\cneg}[2]{\ocneg{#1 \neq #2}}

\newcommand{\respo}[2]{R_{#1#2}}
\newcommand{\avail}[2]{A_{#1#2}}
\newcommand{\simil}[2]{S_{#1#2}}

\newcommand{\vecrho}{\vec{\rho}}
\newcommand{\vecalpha}{\vec{\alpha}}
\newcommand{\frho}[1]{\rho_{#1}}
\newcommand{\falpha}[1]{\alpha_{#1}}
\newcommand{\falphaj}[2]{\alpha_{#1[#2]}}

\newcommand{\falphamax}{\alpha^{u}}
\newcommand{\falphamin}{\alpha^{l}}
\newcommand{\frhomax}{\rho^{u}}
\newcommand{\frhomin}{\rho^{l}}

\newcommand{\alphacand}{v}


%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
%\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

%\title{Framework for Fast Parallel Generalized N-Body Methods}
\title{A Parallel $N$-Body Data Mining Framework}

\author{Garrett F. Boyer, Ryan N. Riegel, Alexander G. Gray
\\ Georgia Institute of Technology
\\ Computational Science and Engineering
\\ 801 Atlantic Drive, Atlanta, GA
\\ \{garryb,rriegel,agray\}@cc.gatech.edu
\\
% For a paper whose authors are all at the same institution, 
% omit the following lines up until the closing ``\}''.
% Additional authors and addresses can be added with ``\and'', 
% just like the second author.
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
The multi-tree approach for accelerating data mining methods has spurred some of the fastest known solutions for a significant class of fundamental methods previously considered impractical for massive data sets, including kernel density estimation, nearest-neighbor search, and spatial statistics.
We present a standard mathematical model that allows these problems to be scaled further via parallelization, without significant extra programmer effort.
With the framework, we derive a strategy for parallelization and describe our implementation, Tree High-Order Reduce or THOR.
We demonstrate its effectiveness on three realistic applications: kernel density estimation on the Sloan Digital Sky Survey, two-point correlation of three-dimensional positions of galactic halos, and nearest neighbor search on a high-dimensional speech manifold.
\end{abstract}

\section{Introduction}

Many problems in data mining nominally require an all-pairs analysis of metric\authorsnote{May need to explain what metric data is -- don't want to lose the reviewers on the very first sentence.} data.
For a significant subclass of these problems, which we call \defterm{generalized $N$-body problems (GNP's)}, a higher-order divide and conquer technique can avoid nearly all computation by skipping combinations of regions with little mutual interaction.
% TODO: balltrees
Higher-order divide and conquer on metric data typically utilizes metric trees, such as $kd$-trees \cite{preparata_kdtrees}, ball trees \cite{anchors_balltrees_moore}, and cover trees \cite{covertrees}, to assist in dividing the data set into regions.
Such employment of trees is frequently called the \defterm{dual-tree} or the more generalized \defterm{multi-tree} approach.
This approach is used by some of the fastest known serial algorithms for many statistical and physical problems on metric data, such as bounded-error kernel density estimation \cite{gray_kde, lee_gauss1, lee_gauss2}, $n$-point correlation \cite{gray_nbody}, nearest neighbor search \cite{gray_nbody}, and more.

Nonetheless, as multi-core processors begin to dominate even commodity markets, even the fastest serial algorithms may not compete with a trivially parallelizable brute-force algorithm.
Additionally, and perhaps more importantly, some data sets are too massive to be practical on a single processor -- given enough processors, these problems may become worthwhile.
Fortunately, multi-tree algorithms, owing to their divide-and-conquer nature, are in fact quite parallelizable.
We show that techniques that have long been used to parallelize the physical $N$-body force calculation methods can be, to an extent, applied to all GNP's.

Parallel programming is well-known for its significant cost in terms of development time, which is frequently far greater and more expensive than any achievable speedup.
However, we show that all second-order GNP's can be phrased as an instance of a generalized $N$-body algorithm, leading to a novel solution: Invest in a parallel solver for the general problem, requiring algorithm developers to provide only problem-specific portions.
In practice, this approach leads to efficient parallel and serial performance, and furthermore, the parallel algorithm in some cases takes less time to develop than a from-scratch serial algorithm.

%We describe in this paper both the formal problem generalization, and our implementation THOR that executes dual-tree algorithms with two different expansion patterns.
%\authorsnote{Make sure it's clear at this point what is novel in my paper.  What should be clear is: (a) the mathematical model, and (b) its implementation in parallel.}

We demonstrate a formal, unified model that solves all second-order generalized $N$-body problems, and respectively allows most dual-tree algorithms.
Next, we argue that programming in this unified model is not costly in developer time.
Finally, we show the inherent parallelism of our model by examining an implementation that uses well-known parallelization techniques.
\authorsnote{I must try to appeal to people who don't really care about dual-tree algorithms, but just want raw parallelism.  Am I successful?}

\section{Fast Dual-Tree Algorithms}

\begin{figure}[t]
  \centering
  \begin{minipage}{3.2in}
    \begin{minipage}{1.05in}
      \includegraphics[width=1.0in,height=1.2in]{kdtree-level2.ps}
    \end{minipage}
    \begin{minipage}{1.05in}
      \includegraphics[width=1.0in,height=1.2in]{kdtree-level4.ps}
    \end{minipage}
    \begin{minipage}{1.0in}
      \footnotesize{\caption{\label{fig:kdtree} A two-dimensional
          $kd$-tree at (left) level 2 and (right) level 4.}}
    \end{minipage}
  \end{minipage}
\end{figure}


\mysub{Foundations}
We briefly introduce the concept of metric trees and define some fundamental notation that we use throughout the paper.
Our notation considers tree node $X$ in a metric tree $\kdroot{X}$ to be a set of points.
These points lie in a metric space, such that a distance metric $\dist{x}{y}$ is commutative, satisfies the triangle inequality $\dist{x}{z} \leq \dist{x}{y} + \dist{y}{z}$, and is zero iff $x = y$.
For example, a $kd$-tree \cite{preparata_kdtrees}, as in \fig{kdtree}, is a binary tree that recursively divides the data along coordinate dimensions, and works with $L$ distance metrics such as Euclidean $L_2$.
Unlike a grid, a $kd$-tree is adaptive, and can thus concentrate on lower-dimensional manifolds when present.
Ball trees \cite{anchors_balltrees_moore} are binary and use bounding radii instead, and work with any metric space.
%TODO: Cite ball trees and cover trees
Cover trees \cite{covertrees} are not binary and have a variable branching factor depending on properties of the data.
Quad-trees and oct-trees recursively bisect all coordinate dimensions for two- and three-dimensional vector spaces.
The choice of tree is often problem-specific, but generally results in a constant-factor rather than asymptotic complexity difference.
For clarity, our mathematical notation focuses on binary trees, although the notation is trivially extended to arbitrary $n$-ary trees.
Every internal node is partitioned $X = \kdleft{X} \union \kdright{X}$, with parent $\kdparent{X} \supset X$.
Conceptually, a leaf is a single point $\{x\}$, though in practice multiple points may be grouped to amortize recursion overhead.

In all these trees, each node $X$ has summary information we label $\outstat(X)$.
Summary information includes the tree's bounding structure such as a hyper-rectangle or hyper-sphere, along with arbitrary summary statistics, such as mean, various moments, or other problem-specific information.
Bounds are a fundamental part of multi-tree algorithms, so we universally use superscripts $l$ and $u$ refer respectively to lower and upper bounds, which are often, as in the case of distance, obtained from the node's bounding structure.

\mysub{Single-tree versus dual-tree}
A common program structure, which we label \defterm{query-reference}, loops at the top level over a sequence of query points, and for each of those, considers all reference points with regard to the query.
An example, nearest neighbors, loops over many query points, and for each, loops over reference points to find the nearest one for that query.

A \defterm{single-tree} algorithm replaces the inner loop with a tree traversal, skipping parts of the reference tree that are irrelevant to the query, much like search in a binary search tree.
A \defterm{dual-tree} algorithm merges the two loops together and considers the entire computation a traversal over tree node pairs, forming a hierarchy over the Cartesian product of points.
The dual-tree algorithm \defterm{prunes} node pairs from consideration when possible, using summary information.
Although dual-tree algorithms can solve problems such as two-point correlation that have no actual differentiation between a query or reference point, the query-reference nomenclature simplifies our discourse.
In many practical problems, the amount of computation that can be pruned is immense, reducing the complexity to $O(N log N)$, $O(N)$, or $O(N^{1+\epsilon})$.
% for $0 < \epsilon < 1$.
%Although not tree-based algorithms, Strassen's\cite{strassen} matrix multiplication and Baeza-Yates\cite{baeza_yates} set-intersection algorithms also exhibit key features of higher-order divide and conquer.

For each query point, a single-tree algorithm must commence at the root of the tree and traverse towards the leaves or until pruning occurs, and is usually at best $O(N log N)$.
A dual-tree algorithm instead considers the root-level nodes only with other root-level nodes, therefore amortizing the cost of considering the root-level nodes, and can be $O(N)$ in practice.
Empirically, dual-tree algorithms often significantly overtake similar single-tree algorithms for data sets larger than a threshold.
Such asymptotic performance encourages the use of dual-tree algorithms over alternatives to scale data mining of massive data sets beyond a single processor.

\mysub{Two-point correlation and range count}
The fundamental two-point-correlation statistic conceptually measures ``clumpiness'' and fractal dimension.
Two-point correlation\footnote{A common formulation of two-point correlation skips redundant pairs, however, converting between the two is elementary algebra.} calculates the number of pairs of points within fixed radius $h$.
For data set $X$, radius $h$, and indicator function $I$, two-point correlation is
\[\sum_{x \in X} \sum_{y \in X} I(d(x, y) \leq h).\]
\noindent
The dual-tree algorithm in \fig{tpc} considers subset (node) pair $X$ and $Y$, returning immediately if all points are completely inside or outside the radius.
The asymptotic speedup of this dual-tree algorithm, of the $O(N^{1+\epsilon})$ variety, was demonstrated empirically in \cite{gray_nbody}.

Two-point correlation is not necessarily a query-reference problem, since only a single result is required.
Its analog, range count, considers the number of points within the radius for each query,
\[\map_{q \in Q} \sum_{r \in R} I(d(q, r) \leq h).\]
Range count has a similar, but slightly more complex, dual-tree algorithm, that we enumerate in a later section.

\begin{figure}[t]
  \begin{displaymath}
    \begin{array}[t]{l}
      \X \text{function tpc}(X, Y)
      \x \text{if }\distup(\outstat(X), \outstat(Y)) > h\text{, return }0
      \x \text{if }\distlo(\outstat(X), \outstat(Y)) \leq h\text{, return } |X| \cdot |Y|
      \x \text{else if }|X| \geq |Y|\text{,}
      \xx \text{return tpc}(\kdleft{X}, Y, h) + \text{tpc}(\kdright{X}, Y, h)
      \x \text{else,}
      \xx \text{return tpc}(X, \kdleft{Y}, h) + \text{tpc}(X, \kdright{Y}, h)
    \end{array}
  \end{displaymath}
  \caption{\label{fig:tpc} Dual-tree two-point correlation.
  $d$ signifies distance, and superscripts $l$ and $u$ refer, respectively, to lower and upper bounds.}
\end{figure}

\begin{figure}[t]
  \begin{displaymath}
   \begin{array}[t]{l}
     \X \text{for each node }Q\text{ in tree } \kdroot{Q}\text{, }a(Q) \gets \infty
     \X \text{procedure allnn}(Q,R)\text{,}
     \x \text{if }a(Q) < \distlo(\outstat(Q), \outstat(R))\text{, return}
     \x \text{else if }Q = \{q\} \text{ and } R = \{r\}\text{,}
     \xx a(\{q\}) \gets \min(a(\{q\}), \dist{q}{r})
     \x \text{else if }|Q| \geq |R|\text{,}
     \xx \text{allnn}(\kdleft{Q}, R); \text{ allnn}(\kdright{Q}, R)
     \xx a(Q) \gets \max(a(\kdleft{Q}), a(\kdright{Q}))
     \x \text{else, prioritize by node distance,}
     \xx \text{allnn}(Q, \kdleft{R}); \text{ allnn}(Q, \kdright{R})
   \end{array}
  \end{displaymath}
  \caption{\label{fig:allnn} Dual-tree nearest-neighbor search.}
\end{figure}

\mysub{All nearest-neighbors}
In applications from manifold learning to classification, one computes the nearest point from reference set $R$ for each of a batch of queries $Q$.
We express this computation,
\[\map_{q \in Q} \argmin_{r \in R} d(q,r).\]
\noindent To achieve speedup, the dual-tree algorithm in \fig{allnn} maintains for each query tree node $Q$ the furthest candidate neighbor distance, $a(Q)$.
If a set of references is farther away than this distance, no further exploration is required.

\mysub{Kernel density estimation (KDE)}
By placing a kernel such as a multivariate Gaussian or Epanechnikov kernel at each data point, it is possible to estimate the probability density function from a sample.
Given a kernel $K$, metric data set $R$, and query points $Q$, calculate
\[\map_{q \in Q} \sum_{r \in R} K(q, r).\]
One of many dual-tree algorithms \cite{gray_kde, lee_gauss1, lee_gauss2} can be used to give a bounded relative-error approximations for each query point, by pruning subcomputations with acceptable error.

%\mysub{Affinity propagation}

%Affinity propagation is a recent clustering technique that identifies exemplars in a data set, taking into account all point-to-point similarities\cite{affinity}.
%The similaritiy $\simil{i}{j}$ between points $x_i$ and $x_j$ is the negative squared distance or some other quantity to be maximized, with diagonals $\simil{i}{i}$ set to a parameter that controls the number of clusters.
%The algorithm maximizes the sum of similarities between each point and its chosen exemplar.
%If a metric space is defined over the points, one use the dual-tree algorithm in \cite{ryan_nips} to solve a modified version of the affinity propagation update steps,
%\[ \begin{array}{l}
%  \vecalpha \gets \map_{i} \argmin^2_{j} \!\left( \cpos{i}{j}(\cpos{i}{j}(\simil{i}{j} + \falphaj{i}{j}) - \frho{j}) - \simil{i}{j} \right)\! ,
%  \\
%  \vecrho \gets \map_{j} \sum_{i} \!\left( \cpos{i}{j}(\simil{i}{j} + \falphaj{i}{j}) \right)\!,
%\end{array} \]
%\noindent with $\ocpos{v}(x)$ being $\min(x, 0)$ for $v$ true and $x$ otherwise, and $\falphaj{i}{j}$ being the first minimum of $\falpha{i}$ when $i \neq j$ and the second otherwise.
%%These vectors are computed iteratively until convergence.
%%Exemplars are those points $j$ with $\frho{j}$ positive and each point belongs to the cluster defined by the exemplar closest to it.
%The algorithm uses $O(N)$ space and empirically took $O(N^{1.3})$ on three-dimensional points from a gravitational $N$-body particle simulation, compared to the existing $O(N^2)$ time and space method.

\mysub{$N$-body Particle Simulations}

$N$-body particle simulations comprise some of the best-studied parallel computing problems.
Most algorithms for efficient simulation use trees, and parallelization of these tree codes offers indispensable insight into that of generalized $N$-body problems.
The physical $N$-body problem models $N$ particles in an evolving system where each particle exerts force on all other particles.
Computational solutions break the simulation into discrete time steps, during which force is calculated as a query-reference generalized $N$-body problem,
\[\map_{q \in D} \sum_{r \in D} f(q,r),\]
\noindent where $f(q,r)$ is typically an inverse-squared-distance gravitational or Coulombic force.
Direct computation of the sum is $O(N^2)$ and has been one avenue of pursuit \cite{grape}.

However, the famous Barnes-Hut algorithm \cite{barnes_hut} and Fast Multipole Method \cite{greengard_fmm} achieve speedup by using trees to approximate distant interactions, and have been subject to enormous parallelization efforts.
Barnes-Hut \cite{barnes_hut} traverses an oct-tree\footnote{An oct-tree hierarchically divides all 3 dimensions at the midpoint.} to approximate force on each particle.
If query particle $q$ is further than parameter $\theta$ times the spatial width of the considered node, the force calculation is approximated by the node's center of mass.
Although Barnes-Hut is a single-tree algorithm, many aspects of its parallelization are applicable to the dual-tree case.

The Fast Multipole Method (FMM) \cite{greengard_fmm} achieves $O(N)$ on uniform distributions, but with a higher constant factor, by considering node-node interactions.
The FMM is indeed a special case of dual-tree algorithms due to its node-node computations.
However, its traversal pattern, often called a ``downwards pass,'' is quite specific and does not sufficiently capture the diversity of dual-tree problems.
Still, key parallelization insights are still applicable to dual-tree algorithms.

\mysub{Generalization}
All of the above problems have the form of nested commutative, associative operators applied over an inner function.
We later demonstrate how this property is critical to dual-tree algorithms and their parallelization.

% \cite{singh95load} \cite{nyland93dataparallel} \cite{washington_nesl}

\section{Parallel Programming Productivity}

When approached with a potentially parallelizable problem, the development time is often far more expensive than the CPU time saved.
Thus, it is a common goal to simplify the development of parallel programs\footnote{Nadeem says I need citations here.  I think I could alternatively weaken my claim.}.

One approach is to create parallel programming languages or language extensions that allow a user to write serial or minimally parallel code and have it run on automatically on many processors.
These languages or extensions may provide terrific performance for problems with regular access patterns, but leave the developer at the mercy of the language's abilities.
Generalized $N$-body problems rarely exhibit the type regularity of which these languages can take advantage.

NESL is a very high-level parallel programming language, semantically resembling ML, that is designed to handle irregular access pattern.
NESL sports parallel implementations of Barnes-Hut and the FMM \cite{blelloch_nbody}.
However, its explicit goal of rapid algorithm development for the benefit of teaching and experimentation overshadows the goal of fine-tuned production code.
We find that finer implementation control over the underlying implementation is advantageous for considering different communication or scheduling strategies, incorporating efficient machine-level code, and considering additional features like fault tolerance.
%Although it is fair to say that parallel programming languages have improved dramatically since then, there is still sparse literature on parallelizing $N$-body algorithms this way.

It seems paradoxical to desire both fine implementation control yet still simplify the development process, but both are achievable when targeting a specific class of problems.
One develops an arbitrarily fine-tuned implementation of a generalized problem in a fast language, but allow the user to plug in the algorithmic specifics.
Map-Reduce \cite{mapreduce} is a famous example for solving problems that process massive data sets through the three-step process of preprocessing, reorganization, and reduction.
By targeting only this class of problems, Map-Reduce can use optimally efficient data decomposition methods, out-of-core sorting algorithms, and mechanisms for fault-tolerance.
Additionally, very little work on the part of the programmer is needed, such that writing the parallel algorithm can be simpler than writing the serial algorithm.

Warren and Salmon in \cite{warren95portable} made a similar argument over a decade ago when building a software framework for parallelizing oct-tree algorithms using their Hashed Oct-Tree method.
They created a fine-tuned implementation of single-tree algorithms based on previous Barnes-Hut work, but abstracted key points that can be replaced with custom code for different problems.
We do build on major points from both authors, prime figures in distributed-memory $N$-body algorithms.
However, dual-tree algorithms differ fundamentally from single-tree algorithms, and the Hashed Oct-Tree method is specific to oct-trees.

We show dual-tree algorithms have a novel generalization, of which we take advantage in order to create parallel code.
The developer needs only to focus on the interactions that occur among data, not on the details of the tree exploration or its parallelization.
We choose to implement THOR in C++ owing to the availability of efficient compilers, low-level access to hardware, and efficient templating system.
Additionally, since programmers prefer to use common constructs such as vectors and lists over static bit-copyable structures, THOR provides a scheme that takes advantage of C++ templates to implement serialization and deserialization of acyclic pointer graphs.
Thus, we present THOR as a ``quick and easy'' way to generate efficient parallel dual-tree algorithms.

\section{Dual-Tree Algorithm Framework}

Previously, we mentioned generalized $N$-body problems to be a subclass of all-pairs computations\footnote{Talk about first-order reduce problems?}.
We begin to describe an extreme, though computationally unreasonable, generalization of these $N$-body problems.

The class of \defterm{second-order reduce problems} is a type of all-pairs computation that is a super set of generalized $N$-body problems.
For commutative, associative operators $\bigotimes$ and $\bigodot$, postprocessing functions $h$ and $g$, inner function $f$, and data sets $X$ and $Y$, one computes,
\[\begin{array}{l}
  \displaystyle \Gnp(X, Y) = h(\gnp(X, Y)),
  \\
  \displaystyle \gnp(X, Y) = \bigodot_{x \in X} g\!\left(x, \bigotimes_{y \in Y} f(x, y) \right).
\end{array}\]
\noindent
For example, two-point correlation with radius $w$ is has,

\[\begin{array}{ccccl}
  %\mathcal{X} &\equiv& \mathcal{Y} &\equiv& \mathbb{R}^3
  %\\
  \bigodot &\equiv& \bigotimes &\equiv& \sum
  \\
  \multicolumn{3}{r}{f(x,y)} &\equiv& I(d(x,y) < w)
  \\
  \multicolumn{3}{r}{g(x, a)} &\equiv& a
  \\
  \multicolumn{3}{r}{h(a)} &\equiv& a
\end{array}\]

A second-order reduce problem is also a \defterm{generalized $N$-body problem} if for any partitioning $\kdleft{Y} \union \kdright{Y} = Y$,
\[\gnp(X,Y) = \gnp(X,\kdleft{Y}) \otimes \gnp(X,\kdright{Y}).\]
\noindent Note that similar decomposition of $X$ over $\bigodot$ is trivial by commutativity and associativity.
A class of generalized $N$-body problems including two-point correlation are \defterm{single-operator problems}, having $\bigodot = \bigotimes$ and a null $g$.

If $\bigodot$ is the operator $\map$ and $g$ is null, it is a \defterm{query-reference GNP}, and the above property is formally satisfied by rephrasing results as sets of key-value pairs.
For the sake of simplicity, we omit such formality.

It is clear that {\em any} second-order reduce problem can be transformed into a query-reference GNP by replacing the outer operator with $\map$, and then treating both $g$ and $\bigodot$ as a postprocessing step.
Therefore, any framework that solves query-reference problems necessarily solves any second-order reduce problem.
Although THOR's implementation does provide native support for both single-operator and query-reference problems, our discussion focuses on the more general query-reference problems.
%In practice, the transformation to the query-reference analog rarely results in an asymptotic difference in performance.
%Nonetheless,
We have shown that all generalized $N$-body problems can be expressed as query-reference problems, and proceed to show that the computation and pruning of most dual-tree algorithms can be generalized.

\mysub{The Generalized $N$-Body Algorithm}
Generalized $N$-body problems have a hierarchical computation,
\[
\gnp(X,Y) = \left\{ \begin{array}{lrr}
    f(x,y) & \multicolumn{2}{r}{\text{if }X = \{x\}\text{ and }Y = \{y\}\text{,}}
    \\
    \multicolumn{2}{l}{\gnp(\kdleft{X},Y) \otimes \gnp(\kdright{X}, Y)} & \text{if }\prefsplit{X}{Y}\text{,}
    \\
    \multicolumn{2}{l}{\gnp(X,\kdleft{Y}) \odot \gnp(X,\kdright{Y})} & \text{otherwise,}
  \end{array}
\right.
\]
\noindent
where $\prefsplit{X}{Y}$ specifies a recursion preference.
A single-tree algorithm is a special case where $\prefsplit{X}{Y} \equiv |X| > 1$, that is, $X$ is divided until individual queries are considered, after which $Y$ is recursed.
A dual-tree execution may use a heuristic such as $|X| \geq |Y|$ to prefer node pairs with approximately equal cardinality.
The generalized $N$-body algorithm initially considers the expression $\gnp(\kdroot{X}, \kdroot{Y})$, continually replacing any subexpression $\gnp(X, Y)$ with its aforementioned expansion.
The final expression forms an \defterm{expression tree} where leaves are of the form $f(x,y)$ and internal nodes are either $\odot$ or $\otimes$.

In the above, we only require that $X = \kdleft{X} \union \kdright{X}$ be a valid partitioning, not necessarily tree branches.
Trees are commonly used since they hierarchically group points that are likely to prune simultaneously as a group, i.e. a high correlation in pruning behavior.

Dual-tree algorithms can use at-node statistics $\outstat(\cdot)$ to prove facts about node pairs at a higher level, and \defterm{prune} part of the expression tree.
A prune based only on statistics is an \defterm{intrinsic prune}.

In some cases, the results of computations outside of the currently considered pair can provide necessary evidence to prune.
For example, in all nearest-neighbors, a distant pair of nodes can be excluded if the query node's previously-calculated worst-case candidate neighbor is closer.
This prune is called an \defterm{extrinsic prune}.

%Sometimes the summary results of the entire problem imply that a subproblem needs no further consideration.
%Summary results can be aggregated bottom-up in the same manner the problem has been decomposed using some summary composition operator $A \otimeshat B \supseteq \{a \otimes b | a \in A, b \in B\}$ and likewise for $\odothat$.
%Unfortunately, these formal sets are infeasible to construct in practice, but can be bounded by considering single values that a represent a bound for a relevant part of the computation.
%An example is the upper bound nearest neighbor distance kept by nearest neighbors for each query node.
%For all-nearest-neighbors, this set is the set of all sets of possible key-value pairs representing query points and the corresponding possible nearest neighbor distances, each of which ranges continuously from 0 (the best distance possible) to the best candidate nearest neighbor distance.
%A prune is made by intersecting this with the similar set returned by $\summary(\outstat(X),\outstat(Y))$, and if the summary result set is unaffected, $(X,Y)$ may be skipped entirely.
%Unfortunately, it is infeasible to represent this construct directly in practice, so typically a single upper and/or lower bound is stored to summarize worst-case bounds.

%The generalized $N$-body algorithm begins with $\gnp(\kdroot{X},\kdroot{Y})$, and repeatedly replacing some $\gnp(X,Y)$ with one of its expansions.
%There is no one required exploration pattern.

It is worth noting that the generalized algorithm is agnostic to expansion pattern, that is, the order in which different subexpressions are replaced.
However, choice of expansion pattern can have a significant, sometimes asymptotic, effect on run time.
With intrinsic pruning only, $\prefsplit{X}{Y}$ alone determines the asymptotic run time, so expansion pattern is irrelevant.
Extrinsically pruning problems that concentrate on spatially nearby computations may prefer a depth-first expansion along with a local ordering heuristic.
Others achieve good performance with a breadth-first expansion, which holistically refines the problem space.

Nearly all dual-tree algorithms we have encountered can be viewed as the algorithm above with intrinsic and extrinsic pruning.
However, the generalized algorithm does not explain the rich ``flow'' of data throughout an actual computation.
We next show, for query-reference problems, a set of generalized ``rules'' enumerating allowable ways in which results can be computed.

%Our paper treats parallel execution as an additional expansion pattern by noting that these subproblems can be considered independent and solved in parallel, given careful design choices.

%Ongoing research attempts to characterize all generalized N-body problems with rigorous mathematical formality; however, such rigorous formalization is outside the scope of this paper.
%Here, we postulate that all dual-tree algorithms fall into one or both of two categories: a single scalar or vector result summarizing all pairs of inputs, or an independent result for each query point computed against the entire reference set.

%A global reduction computes a single scalar or a vector of results that is constant with the number of data points.
%This type of problem is typically characterized by a single commutative, associative operator applied to pairs of inputs.
%Such problems include 2-point correlation, kernel-based data likelihood estimates, and the closest-pair problem.

%A query-reference problem alternatively computes a result for each point in a query set.
%For each point in the query set, execute a commutative, associative operator over every element in a reference set.
%These problems include such fundamental problems as all-nearest-neighbors, kernel density estimation, the testing phase of most nonparametric classification methods, matrix-vector multiplication, and many more.

%\subsection{Single-Operator Reduce Problems}
%
%A \defterm{single-operator reduce problem} computes a single scalar or relatively small vector of results, taking as input two data sets and applying an aggregate commutative and associative operator to all pairs,
%\begin{eqnarray*}
%\outglob(X, Y) &=& \gglob(\inglob(X, Y)),
%\\
%\inglob(X, Y) &=& \Opglob_{(x, y) \in X \times Y} \fglob(x, y).
%\label{eqn:defglob}
%\end{eqnarray*}
%
%\noindent where $\gglob$ is a post-processing function (usually identity), $\Opglob$ is a commutative and associative operator, and $\fglob$ is an inner function.
%We can express two-point correlation,
%\[\begin{array}{rcl}
%\TWOPT(X, r) &\equiv& \inglob_r(X, X),
%\\
%\opglob &\equiv& +
%\\
%\fglob(x, y) &\equiv& I(d(x, y) < h).
%\end{array}\]
%
%\noindent We decompose $\inglob$ with the identities,
%\begin{eqnarray*}
%\inglob(X, Y) &=& \inglob(\kdleft{X}, Y) \opglob \inglob(\kdright{X}, Y),
%\\
%\inglob(X, Y) &=& \inglob(X, \kdleft{Y}) \opglob \inglob(X, \kdright{Y}).
%\label{eqn:divideglob}
%\end{eqnarray*}
%
%\noindent and can be computed bottom-up $\instat(X) = \instat(\kdleft{X}) \opstat \instat(\kdright{X})$.
%Intrinsic prunes are applied with the following rule,
%\begin{equation*}
%\text{if } \canpruneglob(\outstat(X), \outstat(Y)) \text{, then } \inglob(X, Y) = \deltaglob(\outstat(X), \outstat(Y))
%\label{eqn:intrinsic}
%\end{equation*}
%\noindent where $\canpruneglob$ is a Boolean indicator that a prune is possible i.e. the initial summary set is singleton, and $\deltaglob$ is the summary set's only element.
%
%The rules shown so far allow some software system to execute single-operator reduce problems with intrinsic prunes given definitions for the relevant functions and operators.
%For instance, a $kd$-tree-based two-point correlation $\TWOPT(X, h)= \outglob(X,X)$ is defined:
%\begin{eqnarray*}
%\label{eqn:tpc_gglob}
%\gglob(\inglob) &\equiv& \inglob
%\\
%\label{eqn:tpc_opglob}
%\opglob &\equiv& +
%\\
%\label{eqn:tpc_fglob}
%\fglob(x,y) &\equiv& I(\dist{x}{y} < h)
%\\
%\label{eqn:tpc_canpruneglob}
%\canpruneglob(\sigma(X), \sigma(Y))
%&\equiv&
%\begin{array}{l}\distup(\outstat(X),\outstat(Y)) < h \\ \vee \distlo(\outstat(X),\outstat(Y)) \geq h\end{array}
%\\
%\label{eqn:tpc_deltaglob}
%\deltaglob(\outstat(X),\outstat(Y)) &\equiv& \left\{ \begin{array}{l} 0 \text{ if } \distup(\outstat(X),\outstat(Y)) < h \\ |X|\cdot|Y| \text{ if } \distlo(\outstat(X),\outstat(Y)) \geq h \end{array}\right.
%\\
%\label{eqn:tpc_fstat}
%\fstat(x) &\equiv& (x,x)
%\\
%\label{eqn:tpc_opstat}
%\opstat(x) &\equiv& \left( \bigvec{\min} , \bigvec{\max} \right)
%\end{eqnarray*}
%\noindent The first three summarize the problem itself, with the next two specifying the pruning condition.
%The last two specify the construction of a bounding box, although this is automatic with $kd$-trees.
%Given a parallel execution system, the above is all the programmer needs to specify to have a fully parallelized two-point correlation.
%

%The model shown so far is limited to very simple dual-tree algorithms.
%First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
%Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
%Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{list more}.

\subsection{Simple query-reference problems}
A query-reference problem computes for each query a \defterm{query result} $\outqr(q, \kdroot{R})$, expressed
\begin{eqnarray}
\outqr(q, R) &=& \gqr(q, \inqrv(\{q\}, R)),
\\
\inqrv(\{q\}, R) &=& \Opqr_{r \in R} \fqr(q, r).
\label{eqn:qrdef}
\end{eqnarray}

\noindent
In addition to the classic $N$-body force calculation problem, this encompasses all nearest-neighbors, nearest-neighbor classification, nonparametric Bayes classification, kernel density estimation, and more.

Although each query is independent, speedup is achievable by considering queries {\it en masse}; that is, the contribution of a set of references might be shown to have an exact value for an entire set of queries.
This value, a \defterm{mass result} $\inqrv(Q, R)$, exists only if it is acceptable to use that same value for each query given a reference node; in other words, only if the prune criterion is satisfied.
Otherwise, $\inqrv(Q,R)$ is undefined.
Hierarchically, we express,
\begin{equation}
\text{if } \kdparent{Q} \supset Q \text{ and }\inqrv(\kdparent{Q}, R)\text{ is defined, } \inqrv(Q, R) \gets \inqrv(\kdparent{Q}, R).
\label{eqn:qrvparent}
\end{equation}

\noindent
Recall that $\inqrv$ is defined for singleton queries in Equation~\ref{eqn:qrdef}, and a single-tree algorithm conceptually treats $\inqrv$ only in this way.
It is important to note that although a dual-tree algorithm recursively descends the query tree, there is no explicit data dependency within the query tree, except the ability to incorporate mass results pruned from a parent.
Our mathematical model leaves to the implementation how the query set is divided.
Both types of algorithms, though, perform divide and conquer with the reference tree,
\begin{equation}
\inqrv(Q, R) = \inqrv(Q, \kdleft{R}) \opqr \inqrv(Q, \kdright{R}),
\label{eqn:qrvcompose}
\end{equation}
\noindent perhaps with an intrinsic prune,
\begin{equation}
\text{if } \canpruneqrv(\outstat(Q), \outstat(R)) \text{, then } \inqrv(Q, R) = \deltaqrv(\outstat(Q), \outstat(R)).
\label{eqn:qrvprune}
\end{equation}

\noindent
with node statistics $\outstat(Q)$ and $\outstat(R)$, an intrinsic prune indicator function $\canpruneqrv$, and intrinsic prune value $\deltaqrv$.
Statistics are frequently built from commutative, associative operators, and may also be built bottom-up,
\begin{eqnarray}
\outstat(X) &=& \gstat(\instat(X)),
\\
\instat(X) &=& \Opstat_{x \in X} \fstat(x),
\label{eqn:defstat}
\end{eqnarray}

\noindent
Range count, the query-reference analog to two-point correlation, is fully defined,
\begin{eqnarray*}
\gqr(q, \inqr) &\equiv& \inqr
\\
\opqr &\equiv& +
\\
\fqr(q,r) &\equiv& I(\dist{q}{r} < h)
\\
\canpruneqrv(\outstat(Q), \outstat(R))
&\equiv&
\begin{array}{l}\distup(\outstat(Q),\outstat(R)) < h \\ \vee \distlo(\outstat(Q),\outstat(R)) \geq h\end{array}
\\
\deltaqrv(\outstat(Q),\outstat(R)) &\equiv& \left\{ \begin{array}{l} 0 \text{ if } \distup(\outstat(Q),\outstat(R)) < h \\ |R| \text{ if } \distlo(\outstat(Q),\outstat(R)) \geq h \end{array}\right.
\end{eqnarray*}

\noindent
The two-point correlation is then the sum of $\outqr$ for all queries.
In summary, Equations \ref{eqn:qrdef} through \ref{eqn:defstat} express the data flow of any query-reference problem and allow intrinsic prunes.
A noteworthy omission from the query-reference model is an inverse to the operator $\Opqr$.
Many useful commutative, associative operators, such as maximum, have no inverse.
As a matter of fact, floating-point addition, due to roundoff error, has no inverse in the general case.
Thus, whereas traditional dual-tree methods often accumulate ``tentative'' results and refine those coarse results with an undo operation, THOR is careful to maintain definitive results separately from coarse results.

\subsection{Query-Reference Extrinsic Prunes}

Extrinsic pruning in a query-reference problems is a more complex process, but can still be represented in a general fashion.
When considering an extrinsic prune for a query and reference node pair, it is necessary to view that set of queries in the context of {\em all} reference points, even if they have not been visited.
We define \defterm{query summary results} $\inmu(Q, R)$ as a compact bound or summary of the final result for that query and reference pair, with the goal of computing $\inmu(Q, \kdroot{R})$ to achieve extrinsic prunes.
After all computation has completed, an \defterm{ideal} value may be computed,
\begin{equation*}
\inmu(Q, R) \gets \Outopmu_{q \in Q} \fmu(q, \inqr(q, R)).
\end{equation*}

\noindent for $\Outopmu$ a commutative and associative operator that merges representative summary results among queries.
For nearest neighbors, $\fmu(q, \inqr(q, R)) \equiv \inqr(q, R)$ and $\Outopmu \equiv \max$, in other words, the greatest candidate nearest neighbor distance.
%Although the generalized $N$-body algorithm states that summary results are built in the same structure as the refinement pattern, practical considerations encourage storing at each query node the best possible bounds for that node; for this reason, we start with this rather contrived definition.
We can merge subcomputations for query sets trivially,
\begin{equation}
\inmu(Q, R) \gets \inmu(\kdleft{Q}, R) \outopmu \inmu(\kdright{Q}, R).
\label{eqn:muchild}
\end{equation}

\noindent
The value of $\inmu(Q, \kdroot{R})$ is then used to make pruning decisions via the extrinsic prune check $\canprunemu(\outstat(Q), \outstat(R), \inmu(Q, \kdroot{R}))$.

During computation, the ideal value is not attainable since the final computation's value $\outqr(q, \kdroot{R})$ is obviously not known.
Before exploring a node pair, it is necessary to bound $\inmu$ from summary statistics,
\begin{equation}
\inmu(Q, R) \gets \deltamu(\outstat(Q), \outstat(R)).
\label{eqn:mudelta}
\end{equation}

\noindent
where $\deltamu$ is a pessimistic bound on pruning information.
For all nearest neighbors, this might be an upper-bound node-to-node distance, although the previous nearest-neighbors implementation treats unseen node pairs as being infinite distance apart.

Next, if we know $\inmu$ for disjoint reference sets, we may pessimistically compose them in a way analogous to the original $\Opqr$, using a commutative associative operator $\Opmu$,
\begin{equation}
\inmu(Q, R_1 \union R_2) \gets \inmu(Q, R_1) \opmu \inmu(Q, R_1).
\label{eqn:mucompose}
\end{equation}

\noindent
The above composition may be quite pessimistic.
$\Outopmu$ takes the worst-case values from each query, and the above would compose those values even if the two values occurred at different query points.
In practice, this pessimism is a necessary evil to avoid deeper traversal.
%For example, if $\inmu$ is an upper bound on a probability density contribution, summing the upper bounds neglects the possibility of having the upper bound appear at different query points for the two subcomputations.
%\authornote{Elaborate more, or make less comments.  Actually, this might want a proof.}

Next, an exact $\inqrv(Q, R)$ may be defined because this computation was pruned.
Ultimately $\inqrv(Q, R)$ will be propagated to individual queries, but until that happens, we bound the prune's worst-case contribution,
\begin{equation}
\inmu(Q, R) \gets \fmuv(\outstat(Q), \inqrv(Q, R)).
\label{eqn:muprune}
\end{equation}

\noindent
A simple matter, the summary information for any set of queries must necessary reflect all contained queries.
When no other information is available, we must resort to this rule, especially when performing depth-first expansions,
\begin{equation}
\inmu(Q, R) \gets \inmu(\kdparent{Q}, R) \text{ if } \kdparent{Q} \supset Q.
\label{eqn:muparent}
\end{equation}

\mysubsub{Quality}
A final note is that extrinsic pruning requires ``high-quality'' values of $\inmu$, that is, values that approach the aforementioned ideal value.
Although the quality of $\lettermu$ generally increases as more node pairs are explored, the order of exploration may help the value converge much faster.
The distribution of useful information over the space of node pairs is highly non-uniform in efficient dual-tree problems.
Prioritization is then used to concentrate on node pairs with higher potential information yield.
We introduce a heuristic function $\heurqr(\outstat(Q),\outstat(R))$ that returns a real value, such that lower values are explored first.

Our experience shows the above rules are sufficient to capture the extrinsic pruning in most dual-tree problems.
For instance, finding all nearest neighbor can be expressed,
\begin{eqnarray*}
\gqr(q, \inqr) &\equiv& \inqr
\\
\opqr &\equiv& \min
\\
\fqr(q,r) &\equiv& \dist{q}{r}
\\
\heurqr(\outstat(Q),\outstat(R)) &\equiv& \distlo(\outstat(Q), \outstat(R))
\\
\canprunemu(\outstat(Q), \outstat(R), \lettermu)
 &\equiv& \lettermu < \distlo(\outstat(Q), \outstat(R))
\\
\deltaqrv(\outstat(Q),\outstat(R)) &\equiv& \infty
\\
\outopmu &\equiv& \max
\\
\opmu &\equiv& \min
\\
\fmu(q, \letterqr) &\equiv& \letterqr
\\
\fmuv(\outstat(Q), \letterqrv) &\equiv& \letterqrv
\\
\deltamu(\outstat(Q),\outstat(R)) &\equiv& \distlo(\outstat(Q),\outstat(R))
\end{eqnarray*}

\noindent One small note is that in depth-first nearest-neighbor implementation, $\deltamu$ is typically ignored, i.e. treated as $\infty$, since it rarely provides useful information.
However, in breadth-first and other patterns, $\deltamu$ is essential.

\subsection{Execution}

To this point, we have defined many different ``rules'' that are assumed to be valid when computing a query-reference GNP.
THOR must utilize an expansion strategy that applies these rules in order to solve a problem.

Previously, we showed locally-ordered depth-first implementations of two-point correlations and nearest neighbors in Figures \ref{fig:tpc} and \ref{fig:allnn} that recursively divide along the two trees.
\fig{DepthFirst} shows the generalized depth-first dual-tree expansion algorithm that takes as input the data and functions of the generalized $N$-body problem.
This pseudo-code, though heavily translated into C++, is present in THOR to execute a locally-ordered depth-first expansion.

Some dual-tree algorithms achieve performance gain by maintaining a queue of node pairs to expand, rather than recursing immediately; however, this has worst-case $O(N^2)$ space.
THOR implements a novel partially-recursive variant to reduce this to $O(N)$\footnote{
  In practice, pruning prevents the queues from approaching either of these worst-case sizes.}.
This expansion pattern mathematically differs from depth-first in the way summary results are computed.
Summary results are accumulated only from the list of considered reference nodes and from previous prunes.
Summary result quality is achieved by considering all references holistically at an increasingly fine granularity.
On the other hand, depth-first expansion builds quality by visiting individual points first.
Although further discussion of the merits is warranted, it is beyond the scope of this paper\footnote{
  In short, the expansion strategy considers a query node with a list of reference nodes.
  While processing the list, creates similar lists for each of the query node's children.
  Each child is then explored recursively with its respective list.}.

\begin{figure}
  \vspace{-0.3in}
\[
  \begin{array}[t]{l}
    \\ \text{Input:}\left(
        \begin{array}[c]{l}\kdroot{Q}, \kdroot{R}, \gqr, \opqr, \fqr, \deltaqrv, \\ \heurqr, \canprunemu, \outopmu, \opmu, \fmuv, \deltamu\end{array}\right)
    \X \text{\com{Initialize mass results and summary results.}}
    \X \text{for each node } Q \text{ in tree } \kdroot{Q}\text{, } \varmuchild(Q) \gets \text{identity of }\Opmu
    \X \text{for each point } q \in \kdroot{Q}\text{, } \varqr(q) \gets \text{identity of }\Opqr
    \X \text{DepthFirst}(\kdroot{Q}, \kdroot{R}, \text{identity of }\Opmu)
    \x \text{\com{postprocess each point}}
    \X \text{for each point } q \in \kdroot{Q}\text{, } \outqr(q, \kdroot{R}) \gets \gqr(q, \varqr(\{q\}))
    \X
    \X \text{procedure DepthFirst}(Q, R, \varmuparent)\text{, \com{explore a node pair}}
    \x \text{\com{compute extrinsic pruning information, Eqn~\ref{eqn:mucompose}}}
    %\x \!\!\!\begin{array}{lllll}
    %     \psty\inmu(Q,\kdroot{R}) &\psty\!\!\gets\!\!& \psty\varmuchild(Q) &\psty\!\!\opmu\!\!& \psty\fmuv(\outstat(Q), \varqrv(Q))
    %     \\              &\psty\!\!\opmu\!\!& \psty\varmuparent   &\psty\!\!\opmu\!\!& \psty\deltamu(\outstat(Q), \outstat(R))
    %   \end{array}
    \x \!\!\!\begin{array}{llll}
         \psty\lettermu &\psty\!\!\!\!\gets\!\!\!\!& \psty\varmuchild(Q)                     & \!\!\!\text{\com{bottom-up, via Eqn~\ref{eqn:muchild}}}
         \\             &\psty\!\!\!\!\opmu\!\!\!\!& \psty\deltamu(\outstat(Q), \outstat(R)) & \!\!\!\text{\com{node pair's, Eqn~\ref{eqn:mudelta}}}
         \\             &\psty\!\!\!\!\opmu\!\!\!\!& \psty\varmuparent                       & \!\!\!\text{\com{unvisited, Eqn~\ref{eqn:muparent}}}
       \end{array}
    \x \text{if } Q = \{q\}\text{ and }R = \{r\}\text{ singletons, \com{leaf-leaf pair}}
    \xx \text{\com{compute value directly, Eqns \ref{eqn:qrdef}, \ref{eqn:qrvcompose}}}
    \xx \varqr(q) \gets \varqr(q) \opqr \fqr(q, r)
    \xx \varmuchild(\{q\}) \gets \fmu(q, \varqr(q))
    \x \text{else if } \canprunemu(\outstat(Q), \outstat(R), \lettermu)\text{, \com{node-to-node prune}}
    \xx \text{\com{update points' results, Eqns \ref{eqn:qrvparent}, \ref{eqn:qrvprune}}}
    \xx \text{for each point }q \in Q\text{, }\varqr(q) \gets \varqr(q) \opqr \deltaqrv(\outstat{Q}, \outstat(R))
    \xx \text{for each point }q \in Q\text{, }\varmuchild(\{q\}) \gets \fmu(q, \varqr(q))
    \xx \text{\com{update the bottom-up summary results, Eqn \ref{eqn:qrvcompose}}}
    \xx \text{for each node }P\text{ in tree }Q\text{, }\varmuchild(P) \gets \varmuchild(\kdleft{P}) \outopmu \varmuchild(\kdright{P})
    \x \text{else if } |Q| \geq |R|\text{,}
    \xx \text{DivideQueries}(Q, R)
    \x \text{else,}
    \xx \text{DivideReferences}(Q, R)
    
    \X \text{procedure DivideQueries}(Q, R)\text{,}
    \x \text{for each } P = \kdleft{Q}, \kdright{Q}
    \xx \text{\com{recurse, by nature of dual-tree}}
    \xx \text{DepthFirst}(P, R, \varmuparent)
    \x \text{\com{recompute $\varmuchild(Q)$ bottom-up, Eqn~\ref{eqn:muchild}}}
    \x \varmuchild(Q) \gets \varmuchild(\kdleft{Q}) \outopmu \varmuchild(\kdright{Q})
%    \x \!\!\!\begin{array}{lll}
%         \psty \varmuchild(Q) &\psty\!\!\gets\!\!&\psty (\varmuchild(\kdleft{Q})  \opmu \fmuv(\outstat(\kdleft{Q}), \varqrv(\kdleft{Q})))
%         \\          &\psty\!\!\outopmu\!\!&\psty (\varmuchild(\kdright{Q}) \opmu \fmuv(\outstat(\kdright{Q}), \varqrv(\kdright{Q})))
%        \end{array}

    \X \text{procedure DivideReferences}(Q, R)\text{,}
    \x \text{\com{explore reference children in heuristic order}}
    \x (R_a, R_b) \gets \text{sort }(\kdleft{R}, \kdright{R})\text{ by }\lambda v ~ \heurqr(\outstat(Q), \outstat(v))
    \x \text{\com{bound unvisited $\lettermu$ with statistics, Eqns \ref{eqn:mudelta}, \ref{eqn:mucompose}, \ref{eqn:muparent}}}
    \x \text{DepthFirst}(Q, R_a, \varmuparent \opmu \deltamu(\outstat(Q), \outstat(R_b))
    \x \text{\com{use with parent's value of $\lettermu$, Eqn \ref{eqn:muparent}}}
    \x \text{DepthFirst}(Q, R_b, \varmuparent)
    %    \X \text{procedure PushDown}(Q, \varqrvparent)\text{,}
    %    \x \text{\com{propagate mass results to leaves}}
    %    \x \varqrv(Q) \gets \varqrv(Q) \opqr \varqrvparent
    %    \x \text{if } Q = \{q\}\text{ singleton,}
    %    \xx \outqr(q, \kdroot{R}) \gets \gqr(q, \varqrv(\{q\}))
    %    \x \text{else,}
    %    \xx \text{PushDown}(\kdleft{Q}, \varqrv(Q))
    %    \xx \text{PushDown}(\kdright{Q}, \varqrv(Q))
  \end{array}
\]
\caption{\label{fig:DepthFirst} The locally-ordered depth-first expansion algorithm for query-reference problems.
This subroutine, with few modifications, is utilized in THOR.
}
\end{figure}

So far, we have justified our first two claims about THOR.
We have supported the completeness of our unified model with the existence of two generalized expansion patterns and two example algorithms, and will later show additional algorithms that have been ported.
Second, the ability to test different expansion patterns in a way that does not involve rewriting the code -- in fact, without any modifications at all -- shows that THOR can actually be an aid in creating serial algorithms.
Next, we address the claim that THOR parallelizes these algorithms, with parallelization simply another execution style of the unified model.

\section{Parallel Execution}

Parallelization of {\em any} algorithm requires addressing how work is divided, what communication is necessary, and how work imbalances are kept to a minimum.
Decades of research into the parallelization of the Barnes-Hut and Fast Multipole algorithms gives insight into the parallelization of dual-tree algorithms.
However, we make significant deviations because of fundamental differences of dual-tree algorithms and the generality of both data structures and the problems solved.

\subsection{Task Decomposition}

The full execution of a dual-tree algorithm has two phases: tree building and computation of the GNP.
We focus on parallelizing the latter, since tree building is typically very fast compared to execution of the GNP.
Nonetheless, there eixst parallel $kd$-tree algorithms such as in \cite{alfuraih00parallel}, and in many cases, the tree can be stored to disk and be reused for many trials.

A second-order generalized $N$-body problem is, by definition, decomposable in two orthogonal ways.
We may consider parallelization as a type of allowable expansion permitted by the Generalized $N$-body Algorithm.
$\gnp(\kdroot{Q}, \kdroot{R})$ may be expanded into subproblems $\gnp(Q_1,R_1), \gnp(Q_2,R_2), ..., \gnp(Q_k, R_k)$ such that $\Union_{1 \leq i \leq k} (Q_i \times R_i) = \kdroot{Q} \times \kdroot{R}$.
Each subproblem can be executed in parallel, with results merged using $\odot$ ($\map$ for query-reference problems) and $\otimes$.
Since the natural unit of subdivision for dual-tree algorithms is a tree node, we concentrate on decompositions where $Q_i$ and $R_i$ are tree nodes.

Although many decompositions are possible, we decompose the problem only along the query tree into tasks $\gnp(Q_1, \kdroot{R}), \gnp(Q_2, \kdroot{R}), ..., \gnp(Q_k, \kdroot{R})$.
Since $\odot$ is $\map$, all writable information is query-specific.
Thus, the query-based decomposition results in zero writes to shared data, and thus no write synchronization is necessary.
Furthermore, extrinsic pruning is highly dependent on the quality of query summary results for node $Q$, that is, $\inmu(Q, \kdroot{R})$.
Considering the entire reference tree on a single processor is advantageous because a high-quality value of $\lettermu$ can be computed on the local processor.

For single-operator problems like two-point correlation, one might consider decomposing both the query and reference tree because the only required communication is a single number.
Unfortunately, the execution time of distant node pairs tends to be orders of magnitude less than that of nearby node pairs; such variance is extremely problematic for a generalized scheduler.
Our experience shows that a query-based decomposition scheme is typically quite manageable for even simple schedulers.
Finally, it is worth noting that previous work parallelizing both Barnes-Hut and the FMM similarly divide only the queries among processors \cite{salmon_thesis, singh_thesis, liu94experiences}.

We define a single task to be the work of computing results for a subtree of queries.
These queries are processed using a standard serial algorithm, such as the aforementioned depth-first expansion in \fig{DepthFirst}.
Typically, each processor executes several tasks over the course of the computation in order to allow dynamic task migration.

\subsection{Communication}

Communication is one of the fundamental factors inhibiting scalability of parallel algorithms, and experience has showed it can be the primary scalability inhibitor for generalized $N$-body methods.
Although a query-based decomposition allows all writes to be independent, each task may need to consider interactions with arbitrary portions of the reference tree.
Consistent with previous literature \cite{salmon_thesis, singh_thesis}, we refer to this subset of reference nodes and points as a \defterm{locally essential tree}.
This is in contrast to the problem's \defterm{domain decomposition}, an assignment of tasks or query nodes to processors.

\mysubsub{Domain decompositions}
A good domain decomposition for a single-tree algorithm promotes maximal overlap among the locally essential trees of the query points considered\authorsnote{Figure would be nice}, thereby minimizing the locally essential tree for that processor.
Salmon's earlier distributed implementations of Barnes-Hut \cite{salmon_thesis} decompose via orthogonal recursive bisection (ORB), or $kd$-tree that is balanced by estimated run time.
This tree is distinct from the oct-tree used by Barnes-Hut.
Each processor is assigned to a leaf in the orthogonal recursive bisection.
The spatial contiguity of the leaf hyper-rectangles generally leads to good overall spatial locality.

Singh {\it et al.} describe an alternate \defterm{cost-zones} decomposition that instead sorts the query points along a sequential ordering of the oct-tree \cite{singh95load}.
The points are then divided into contiguous segments with roughly equal estimated run time.
\cite{singh95load} shows that this technique typically performs better than the ORB decomposition since its decomposition is based on the same data structure used by Barnes-Hut.

Not surprisingly, locality issues in single-tree algorithms are very similar to those of dual-tree algorithms.
THOR combines the strengths from both ideas by simply building a $kd$-tree balanced in a way that each processor can be assigned a tree node.

A lurking point is that if the query and reference data sets are not the same, it may be helpful to distribute both data sets using the same spatial divisions.
Our initial version builds each tree independently, but in the common case that there are significantly more queries than references, the extra communication overhead is quite small.
In a later section, we discuss how a domain decomposition is used as a starting point for scheduling tasks, and describe dynamic adaptations that are used when information about the relative amount of work is unavailable.

\mysubsub{Nature of communication}

In THOR, interprocessor communication has two flavors.
Within a single machine, each processor may have its own cache, but must communicate over the bus to shared memory.
In small-scale SMP systems, we find that within a single query task, dual-tree algorithms have enough locality to ensure high cache hit rates, and thus low enough need for memory bandwidth, that little care is needed to reduce memory contention.
%Modern multicore machines such as Intel's Core brand processors share L2 caches between consecutive nodes, and to maximize cache usage, it is best to assign these pairs similar chunks of the tree.

Singh in his thesis \cite{singh_thesis} argues that the high data requirements of $N$-body methods prevent efficient distributed-memory implementations, and instead recommends costly shared-memory architectures to achieve parallelization for even large numbers of processors.
Nonetheless, several authors \cite{salmon_thesis, liu94experiences, salmon97parallel} have made message-passing implementations of both Barnes-Hut and the Fast Multipole Method.

It is also important to acknowledge that generalized $N$-body problems have widely varying computational characteristics.
In general, the most time-consuming dual-tree algorithms have relatively lower communication requirements relative to computation.
For example, a low-dimensional nearest neighbor computation for 16 million three-dimensional points can be computed in under a minute on a single processor, whereas transferring that data set over a network connection would take several times as long.
For the same data, a two point-correlation with a wide bandwidth may take hours, and thus the communication cost is relatively lower.

Initially, Salmon \cite{salmon_thesis} and Liu and Bhatt \cite{liu94experiences}, in their distributed-memory implementations of Barnes-Hut and FMM, promote the idea of sender-initiated communication.
Each machine precomputes which of its own points and nodes belong to the locally essential trees of other machines, and sends them before computation begins.
By predicting needed data, no latency costs due to requesting remote data are paid, and therefore transfer time is the only limitation.
Such computation is possible since Barnes-Hut's pruning criterion is based only on the bounding box and distances of the cells.

Dual-tree algorithms may have diverse pruning criteria that cannot be predicted -- an example is bounded-error kernel density estimation in \cite{gray_kde}, that bases pruning decisions on lower bound density obtained during the computation.
Such a blocking strategy is also the underlying mechanism behind shared-memory systems.
For these reasons, THOR utilizes an on-demand block-transfer strategy.

\mysubsub{Distributed cache system}

THOR's fundamental abstraction of shared data is a cached array.
A cached array models an array of fixed-sized elements, divided into fixed-sized blocks, which are distributed among machines.
One array contains points, one contains tree nodes, and another contains results.
An element is accessed by explicitly locking and unlocking its block into memory.
When a block is first accessed, it is requested from its owner, and a local replica is kept.

THOR uses a simple system to maintain consistency among replicas.
Simultaneous reads and writes to the same block are permitted if they are restricted to explicitly declared disjoint contiguous regions.
Consistency is ensured only at global synchronization points, occurring only before and after the GNP executes.
THOR's internals simply ensure that writes occur to independent regions of the tree between these synchronization points.

%To avoid lock contention and caching overhead, each thread has a small (about 64 blocks) FIFO cache of blocks locked into the main cache.
%THOR's dual-tree expansion code locks blocks into the FIFO cache\footnote{The FIFO cache's size places a hard limit on the number of active blocks.  64 blocks is quite generous for tree traversals.}.
%Blocks are evicted to the set-associative cache in a round-robin fashion, skipping locked blocks.

%\begin{figure}
%  \hspace{-.17in}
%  \includegraphics[width=3.5in,height=2.3in]{DistributedArchitecture.eps}
%  \vspace{-.3in}
%  \
%  \caption{\label{fig:cache} A single distributed cache in THOR.}
%\end{figure}

%The main cache is a set-associative LRU cache\footnote{We use 16-block sets, although we omit for brevity empirical evidence suggesting diminished returns beyond 4 blocks.} that is shared among all threads.

\mysubsub{Data layout}
THOR stores points in the order of the tree, similar to a Morton ordering.
Additionally, the $kd$-tree is built so that splits are aligned to block boundaries whenever possible, ensuring that each block corresponds to exactly one tree node.
Together, these help ensure that points nearby in space occupy the same block.
Tree nodes are stored pre-order, ensuring that subtrees correspond to contiguous ranges of nodes, and thus can be manipulated using the simple consistency model.
Nodes and points are then distributed block-by-block via the aforementioned domain decomposition.

\subsection{Scheduling}

Previous work \cite{salmon_thesis, singh_thesis} on scheduling $N$-body force calculation problems assume that relative run-time can be predicted from prior iterations.
Since THOR aims to solve many problems that do not involve iteration, we make no assumption that prior information is available.
Additionally, we do not assume the existence of a analytical estimator of run time.
Instead, we dynamically assign tasks to processors during the course of computation.
%Each worker requests an item of work rom a central scheduler upon becoming idle\authorsnote{See other schedulers in literature (embedded as comments in the TeX file).}.
%\authorsnote{grama95scalable - Scalable Parallel Formulations of Barnes-Hut Method for N-Body Simulations (Static random load balance)}
%\authorsnote{gray94load - Load Balancing the Parallel Fast Multipole Algorithm (Dynamic load balance - tech report)}
%\authorsnote{hummel95scalability - On the Scalability of Dynamic Scheduling}
%\authorsnote{hummel12fractiling - Fractiling: A Method for Scheduling Parallel Loops on NUMA Machines}
%\authorsnote{li93locality - Locality and Loop Scheduling on NUMA Machines}
%\authorsnote{markatos92using - Using Processor Affinity in Loop Scheduling on Shared Memory Multiprocessors}
%\authorsnote{c. kruskal and a. weiss, allocating independent subtasks on parallel processors, ieee trans software engineering}
%\authorsnote{m d durand, t montaut, l kervella and w jalby, impact of memory contention on task duration in self-scheduled programs}
%\authorsnote{ioana's 26, 25, 12 -- "some experimentation with schemes that combine static techniques that promote data locality with dynamic scheduling techniques"}
%\authorsnote{markatos 9 - "A comparison of task scheduling strategies for multiprocessor systems"}

%We choose $\hat{k}$ to be a small constant factor times the total number of threads.

Within a machine, uniform-sized chunking is used \cite{kruskal85allocating}.
A shared work queue contains $k$ query nodes of roughly equal cardinality.
A granularity parameter, $g$, determines the minimum expected number of tasks per processor, with $p$ total processors.
These tasks are quickly enumerated by performing a depth-first traversal, terminating at subtrees with at most $\frac{n}{gp}$ points.
Each thread then removes a task from the central queue.
The overhead of this scheduler is small -- $O(k)$ for the decomposition, and $O(k)$ for the work assignment.
With all threads on a single machine, this method alone allows close to optimal performance in practice.

%\authorsnote{At this point, I begin to describe a distributed scheduler that is different than the one THOR currently uses.
%I soon plan on implementing it.  The above section is correct, however.}
When multiple machines are used, the scheduler must manage two concerns: minimizing communication, but also ensuring a balanced load.
% TODO: Add banicescu back in if I end up citing her for another reason.
Several papers \cite{markatos92using, salmon_thesis} mention the difficulty in achieving both goals simultaneously.

THOR uses the domain decomposition is a starting point for work assignment.
Each machine's domain is divided into tasks like previously mentioned.
When a worker becomes idle, it requests a work item from the central scheduler.
Tasks are preferentially assigned from the worker's domain, but if a worker exhausts its own work, an overflow policy is used.
THOR's current overflow policy greedily assigns the unfinished task that is closest spatially to the centroid of the worker's previously completed work items.
In practice, the overflow policy is usually invoked for about 10\% of tasks, usually in regions where queries are more computationally intensive.
Since these regions are usually concentrated, lack of an overflow policy results in a minority of the processors dominating the overall run time.

The current scheduler has a limitation that all workers must consult a central scheduler for each task.
In future versions, we plan to incorporate concepts from Affinity Scheduling in \cite{markatos92using} to create a fully distributed scheduling mechanism.
In practice, however, the overhead of centralized scheduling is insignificant compared to the data transfer.


% This is talking about the scheduler THOR might want to use, but doesn't use.
%We borrow techniques from two papers.
%Ioana Banicescu's work in \cite{banicescu} using a method called fractiling, based on self-similarity of oct-trees, to load-balance the FMM.
%This work is inspired partly by Affinity Scheduling in \cite{markatos92using}, because scheduling is distributed, and it addresses both the communication and load balance problems.
%Our scheduler more closely represents Affinity Scheduling, however, we use some of the advantageous aspects of the fractiling method such as low bookkeeping information.

%Each machine begins running work items from its local queue, using the uniform chunking method.
%If its local queue is exhausted, it contacts a central scheduler, and is assigned to share a busy machine's work queue.
%Affinity Scheduling, however, simply picks the most loaded machine and removes a portion of its remaining work inversely proportional to the number of machines.
%We make a few modifications based on some additional assumptions we can make about the nature of dual-tree algorithms and THOR's cache system.

%Rather than removing a chunk of work from remote queues, THOR instead assigns machines to share a remote work queue.
%Both local and remote threads remove tasks one by one from the queue, and since we choose tasks to be significantly larger than the number of points in a block, the scheduler's overhead is small compared to existing communication overhead.
%When a remote machine begins sharing a remote work queue, an initial flurry of communication is needed to bring into cache the local work queue.
%After the first work item, further communication is relatively low until the work queue is exhausted, and the machine must be reassigned.

%THOR further attempts to minimize communication in the way the central scheduler assigns machines to remote work queues.
%Since reassigning a processor is expensive due to the initial cache misses, effort is taken to avoid scheduling many processors to a work queue.
%The central scheduler finds the work queue with the most work remaining per the number of workers sharing that queue.

%In order for the central scheduler to know the status of each worker, each worker nominally needs to update the scheduler with the number of remaining tasks.
%As an approximation, machines only send this information when the number of remaining items is a power of two.

%Finally, THOR's scheduler avoids a particularly troubling pitfall of dynamic scheduling that we have experienced.
%If a slight load imbalance exists, assigning a remote processor may result in a gigantic imbalance because of the initial communication from the remote processor!
%If a closed-form solution to the exact communication versus computational costs were available, one can determine with elementary algebra whether assigning a remote processor would actually result in such an imbalance.
%Since this is not practical, a parameter $0 < t < 1$ controls the maximum work queue imbalance permitted.
%If a work queue's number of unscheduled tasks is at most $t$ times the initial number of tasks, no new workers are permitted.
%\authorsnote{To reiterate, the aforementioned distributed scheduler isn't implemented.}


%Such a heuristic should attempt to find a work item whose locally essential tree maximally overlaps with those of previous and future work items.

%In severely nonuniform load, this centroid should gradually migrate to the new tasks that are assigned.
%Optimally the set of work should have relatively low "surface area" compared to its "volume" -- a centroid that migrates too slowly opportunistically finds further and further "holes," but a centroid that migrates too quickly carves a finger-like path.
%Our implmentation takes the mean of the midpoint of each task that has already been assigned to a machine.
%Better heuristics may be possible, especially ones which take into account a more accurate model of correlation between locally essential trees for the particlar algorithm -- for instance, algorithms whose locally essential tree is almost exactly the same as the query tree would benefit only from extremely close work items, whereas the difference between somewhat or extremely distant work items is negligible.

%The centroid scheduler has overhead $O(\hat{k})$ for tree decomposition, since it is also just a simple tree recursion.
%To accelerate work assignment, which is worst-case $O(\hat{k}$ per assignment or $O(\hat{k}^2)$ total, we use a single-tree nearest-neighbor search.
%However, since scheduling is such a small cost compared to the dual-tree computation, we are unable to measure this overhead.

%Our initial demonstrations use simple bag-of-tasks scheduling algorithm that adapts to problems where the execution time is bounded $a|Q| \leq t \leq aw|Q|$ where $a$ is the minimum ratio of query tree size to execution time, and $w$ is the maximum constant factor deviance from proportionality.

%In a bag-of-tasks system, each worker requests a new item of work from a central scheduler every time it becomes idle.
%The total time is then $\frac{(k-1)t}{p}+wt$, compared to the ideal run time $\frac{(k-1+w)t}{p}$.
%The utilization $u$ is then $\frac{k-1+w}{k-1+wp}$.
%For a minimum utilization of $u$, one needs at least $k = \frac{w+wpu}{1-u}+1$ tasks, asymptotically $\frac{wp}{1-u}$.
%For 50\% utilization with run times deviating from proportionality by a factor no more than 10, one needs a few more than $20p$ tasks.
%In practice, utilization is far better given this number of tasks.

%Extending the bag of tasks system, we attempt to minimize the amount of extra information a processor requires to take on its next work item.
%Since spatial locality mirrors data locality, we present a simple greedy algorithm.
%When a machine requests a work item, the next work item is the closest chunk of the query tree to the work items it has already performed.

%- haven't really implemented this, i'll get back as to the effectiveness

%- link spatial locality with data locality
%- phrase locality nicely
%The next query node that is assigned to a worker .

%So far, we have detailed in entirety the functionality of our multithreaded parallel software.

\section{Experimentation}

%\authorsnote{Ensure we explain that the current version of THOR doesn't parallelize tree-building.
%Once again, this section is a stub, we haven't done any experimentation.}
The ideal goal of a parallel system such as THOR is to use all available clock cycles to perform useful computation.
In practice, this is unachievable, so we must quantify to the extent that resources are utilized.
After explanation of our experimental design, we quantify THOR's three primary opportunities for under-utilization: overhead due to software complexity, load imbalances, and communication time.


\subsection{Implementation Notes}

In this section, we concentrate on parallelizing the execution of the generalized $N$-body problem and not of tree building, which is parallelizable as shown in \cite{alfuraih00parallel}.
After the tree is built or loaded from file, it is distributed initially among machines via the domain decomposition.
All THOR code is written in C++, with heavy use of template programming.

\subsection{Design}

% you just need to state that in a positive light: the framework is geared towards breaking up larger problems; smaller problems can be solved effectively in serial

Our experimental design carefully focuses on problems of practical impact.
%We consider three major independent variables: the algorithm to run, the probability distribution or data set of origin, and the number of points.
We present three different applications, each having a different algorithm and data set.
%We choose to solve generalized $N$-body problems with data distributions that represent a practical use case, while ensuring a diversity of problems.

%The data set size is a tricky issue, and it is imperative to pick sizes such that parallelization is advantageous.
%Consistent with both practical applications and the law of diminishing returns, we acknowledge larger data sets are a better match for greater parallelism.
%For each problem and data distribution, we then use two sizes for two common usages.
%Relatively short runs from seconds to minutes can take advantage of a multi-core workstation to dramatically increase interactivity.
%
%On the other hand, some data mining computations are so time-consuming that they are completely impractical on a single processor, and must be run on a massively parallel system.
%Our rule of thumb for these is to aim for a few minutes of run time.
%For example, if a problem takes fifteen minutes on a single processor, then we aim for a single minute on 16 processors; we hope to scale a two-hour problem to about a minute on 100 processors.
%
%Table~\ref{tab:setup} shows problems we have selected, and their respective run-times in THOR on a single processor for small-scale problems, or 8 processors for problems impractical for a single processor.
%

The LCDM data set contains three-dimensional positions of galactic halos from the Virgo Supercomputing Consortium's Lambda Cold Dark Matter simulation.
Astronomers use the two-point and three-point correlations to examine cosmic structure and thus evaluate cosmological models.
We look at the two-point correlation for a high bandwidth, where 50\% of pairs fall within the bandwidth, and a low bandwidth, with 15\%.
The former has both high computational and high data requirements, whereas the latter has relatively lower computational and communication requirements.

Next, we examine four-dimensional color data obtained from the Sloan Digital Sky Survey of celestial bodies, some of which are quasars.
Since kernel methods are commonly used to determine if unknown bodies are quasars\authorsnote{Citation?}, we evaluate the finite-difference kernel density estimation algorithm in \cite{gray_kde} on subsets of these data.

We also analyze frequency-domain MFCC features of speech data, extracted using the freely available code from Voicebox.
The ten-dimensional points are believed to lie on a lower-dimensional manifold, so we run a nearest neighbor search, a technique commonly used in manifold learning, on this data.


%\begin{table*}
%  \centering
%  \begin{tabular}{|l|l|l|r||r|r||r|r|}
%    \hline
%    Algorithm   & Data set & Nominal    & Parameters    &  ``Interactive'' & 1-processor &``Massive''   & 8-processor
%    \\
%    Name        & Name    & Dimensions &               &  \# Instances    & Run-Time    & \# Instances    & Run-Time
%    \\ \hline \hline
%    Neighbors   & Speech  & ??         &               &  ??           & ??     & 100,000      & ??
%    \\ \hline
%    KDE         & Red-shift  & 4          &               &  ??          & ??     & 1,000,000    & ??
%    \\ \hline
%    Range Count & LCDM  & 3          & $h$ @ 50\%ile &  ??         & ??     & 3,000,000    & ??
%    \\ \hline
%    Range Count & LCDM  & 3          & $h$ @ 10\%ile &  ??         & ??     & 16,000,000   & ??
%    \\ \hline
%  \end{tabular}
%  \caption{
%  Brief overview of algorithms and data sets tested in THOR.
%  We separate two problem sizes, one geared for interactivity and one for tractability.
%  ((We omit single-processor performance for the latter.))
%  All experiments run on a 3000 immodium 236 exploding space modulator with 9123 gigamartians of ramada.
%  }
%\end{table*}

%optimal bandwidth: mock galaxy, 0.25

\subsection{Single-Threaded Overhead}


\authorsnote{Fix some factual errors in this subsection.}
Introducing parallelism requires some necessary overhead.
THOR introduces overhead in three ways.
First, each access to tree data is accompanied by bookkeeping to maintain the caches.
Second, the templated programming model may inhibit effective compiler optimization and require the programmer to write suboptimal code.
Last, dividing the query tree into sections may introduce redundant computations for higher levels of the tree.

To test all three points, we compare the THOR nearest-neighbor finder with a hand-written nearest neighbor finder. both using the algorithm in \fig{allnn}.
The hand-optimized version is identical except that data and trees are allocated on the heap rather than in cached arrays, and the query tree is not broken into tasks.
The THOR version has all of the additional infrastructure in place, but is run on only a single thread.

To measure the effect of bookkeeping and the abstraction layer, we have THOR execute a single task and compare to the hand-written code.
For a dozen data sets of dimensionality between 2 and 10, and between $10^5$ and $10^7$ points, the THOR version typically takes between 10\% and 20\% longer.
To measure additional slowdown due to query tree division, we begin to divide the query tree into pieces but still execute on a single processor.
For the same data sets, no more than 1\% additional time is required compared to single-task THOR if the query tree is divided into 100 tasks, and up to 5\% is exhibited when dividing into 1000 tasks.
We believe these overheads are quite reasonable given the advantages of THOR.

%\authorsnote{Consider leaving out this paragraph.} To test the first and second possible causes of overhead, we compared kernel density estimation to a pre-existing C code that solves the same problem.
%On all data sets tried, THOR code was actually slightly faster than the existing code.
%Likely, the speedup is due to the way THOR handles many parameters such as kernel type with inlined template functions with a top-level dispatcher, versus the C code, that handles configuration parameters within each kernel evaluation.
%Unfortunately, the authors were unable to hand-code all of the tested algorithms in C.

%For each test algorithm, we obtained fine-tuned serial C\authorsnote{Do I need to cite Auton here?} code from the original authors of the respective test algorithm papers \cite{various, algorithms, etc} in the depth-first expansion pattern.
%\fig{overhead} compares these to moderately-tuned THOR C++, executing 16 equally-sized query tasks on a single thread with 8 KiB blocks of points and 2 KiB blocks of nodes.
%In all cases, the depth-first algorithms run at quite comparable speeds, with the best slowdown 12\% for XYZ, and the worst 50\% for ZYX algorithm.
%The addition of the small FIFO caches typically decrease by one or more orders of magnitude the number of accesses to the LRU cache, which would entail expensive set-associative cache maintenance and a mutual-exclusion lock.
%Although THOR implements only the depth-first expansion at this time, the fundamental decomposability of generalized $N$-body problems allows alternate expansion patterns in a future version.

\subsection{Multi-Threaded}

Waiting on response from Matt...

If that fails, I still have dual-core results...

%We consider a workstation-grade SMP implementation, keeping in mind the ``interactivity'' use-case.
%For each problem set and data distribution, we ran our code using from 1 to 8 processors on an eight-core system.
%In \fig{pthreads} we show virtually linear speedup is achieved for the problems considered.
%There is relatively low competition for memory bandwidth since dual-tree algorithms' inherent data locality allow them to make excellent use of the CPU cache.
%Instead, the scalability depends primarily on the scheduler to perform adequate load-balancing.

\subsection{Distributed Memory}


\begin{table*}
  \centering
  \begin{tabular}{|l|l|r|r||r|r|r||r|r|}
    \hline
    Algorithm               & Data set  & Number     & 1-proc. & 32-proc.  & 64-proc.  & 90-proc.  & 90-proc. & 90-proc
    \\
    Name                    & Name      & Instances  & Minutes & \% Ideal & \% Ideal & \% Ideal & Minutes & Speedup
    \\ \hline \hline
    Neighbors               & Speech    & 2,000,000  & 78  & 95 & 85 & 79 & 1.09 & 71
    \\ \hline
    KDE                     & Red-shift & 500,000    & 123 & 95 & 96 & 87 & 1.57 & 78
    \\ \hline
    Range Count (High $h$)  & LCDM      & 3,000,000  & 118 & 99 & 92 & 88 & 1.48 & 80
    \\ \hline
    Range Count (Low $h$)   & LCDM      & 16,777,216 &     & ?? & ?? & ?? & ?? &
    \\ \hline
  \end{tabular}
  \caption{
  \label{tab:distributed}
  Distributed-memory scaling for selected problems.
  We show the actual single-processor run time and percentage of ideal speedup achieved with parallelism.
  Run on a cluster of 45 machines each with two NetBurst-class Intel Xeon processors at 3.2 GHz, connected via Infiniband 4X.
  }
\end{table*}

To demonstrate the distributed-memory features of THOR, we parallelize some larger scale data mining problems to a cluster of 90 processors.
Rather than attempt to parallelize small problems with marginal realistic speedup, we wish to parallelize data mining applications that are so time-consuming that they are impractical on a single processor.
However, since we must demonstrate single-processor performance for rigor, we choose problem sizes that take hours on a single processor, but should ideally take a minute or two on a cluster.

Table \tab{distributed} shows the three test applications and the chosen data set sizes parallelize from one to 90 processors.
All three showed nearly ideal scaling for the entire size of the cluster, with up to an 80-fold speedup on 90 processors\authorsnote{Perhaps add some commentary here}.

%Consistent with both practical applications and the law of diminishing returns, we acknowledge larger data sets are a better match for greater parallelism.
%On the other hand, some data mining computations are so time-consuming that they are completely impractical on a single processor, and must be run on a massively parallel system.
%Our rule of thumb for these is to aim for a few minutes of run time.
%For example, if a problem takes fifteen minutes on a single processor, then we aim for a single minute on 16 processors; we hope to scale a two-hour problem to about a minute on 100 processors.

%test on a cluster of 45 machines with two Intel Xeon processors each, a total of 90 processors.

%With the recent advent of quad-core processors, we anticipate future supercomputers to place eight or more cores per machine.
%We perform our experiments on a cluster of 16 such eight-core machines, scaling from 8 to 128 total processors.
%The problems we consider are impractical for a single processor, and thus we regretfully omit such performance.

%\fig{distributed} shows varying degrees of parallelism in our code.
%However, two-point correlation, due to its relatively low computational requirements, .

\section{Future Directions}

This paper presents an initial implementation of THOR that allows efficient parallelization of dual-tree algorithms in many cases.
It is reasonable to believe that a modified computational model would allow execution of 3-tree, 4-tree, and multi-tree algorithms, to solve problems such as 3-point correlation that currently have scant efficient parallel implementations.

The potential for higher-quality implementations may also allow THOR to scale to thousands of processors.
Often, this would require specialized knowledge of the organization of the cluster and the process at hand, much in the way algorithms have been fine-tuned in the past; however, the fine tuning can be developed as a core part of THOR rather than in specialized user code.

In a large data center environment where the original data is already distributed among processors, it would be imperative to parallelize the tree-building process with the algorithm in \cite{alfuraih00parallel}.
If the dual-tree algorithm is run each time step in a simulation, data often already reside on the correct machine for the domain decomposition, and the tree can be incrementally updated rather than rebuilt.
There also exist distributed scheduling techniques, such as Affinity Scheduling in \cite{markatos92using}, that promise to reduce the need for a centralized bottleneck.
With thousands of computers, it is highly probably that at least some hardware failure will be experienced.
Fortunately, since each task is already independent, fault tolerance is achievable by replicating tree data and rescheduling failed tasks.

\section{Acknowledgments}

{\small
The cluster used in the experiments is due to a grant from Intel, Microsoft, and Dell.\authorsnote{NEED TO OBTAIN OFFICIAL CITATION FROM MATT}

FASTlab is funded by XX, YY, ZZ.\authorsnote{Who funds fastlab?}

The LCDM data used in this paper is a simulation carried out by the Virgo Supercomputing Consortium using computers based at Computing Centre of the Max-Planck Society in Garching and at the Edinburgh Parallel Computing Centre. The data are publicly available at www.mpa-garching.mpg.de.

The speech data set uses MFCC coefficients of frames from TIMIT, computed from code freely available from Voicebox.
We take 12 MFCC from each 25 msec. frame with 50\% overlap.

The red-shift data are from the Sloan Digital Sky Survey.
Funding for the Sloan Digital Sky Survey (SDSS) and SDSS-II has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, the U.S. Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, and the Max Planck Society, and the Higher Education Funding Council for England.
The SDSS Web site is http://www.sdss.org/.

The SDSS is managed by the Astrophysical Research Consortium (ARC) for the Participating Institutions.
The Participating Institutions are the American Museum of Natural History, Astrophysical Institute Potsdam, University of Basel, University of Cambridge, Case Western Reserve University, The University of Chicago, Drexel University, Fermilab, the Institute for Advanced Study, the Japan Participation Group, The Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, the Chinese Academy of Sciences (LAMOST), Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy (MPIA), the Max-Planck-Institute for Astrophysics (MPA), New Mexico State University, Ohio State University, University of Pittsburgh, University of Portsmouth, Princeton University, the United States Naval Observatory, and the University of Washington.
}


%\nocite{ex1,ex2}
\bibliographystyle{abbrv}
\bibliography{paper}

%  \begin{appendix}
%  
%  \section{Notation}
%  
%  We're still working on our notation (in fact, all of the symbols we use are a user-defined command in LaTeX).
%  
%  \begin{table*}
%  \begin{tabular}{|l|l|l|}
%  \hline
%  Symbol & Macro & Description, Rationale
%  \\ \hline $\kdroot{X}$ & kdroot & root node of spatial tree
%  \\ $\kdleft{X}$ & kdleft & left child of spatial tree node
%  \\ $\kdright{X}$ & kdright & right child of spatial tree node
%  \\ $\kdparent{X}$ & kdparent & parent of spatial tree node
%  \\ \hline $\Theta$ & Theta & a second-order reduce problem
%  \\ $\Gnp$ & Gnp & postprocessed second-order reduce problem
%  \\ $\gnp$ & gnp & second-order reduce problem computation before postprocess
%  \\ $\summary$ & summary & initial summary results
%  \\ \hline $\letterglob$ & letterglob & single-operator reduce 'problem space'
%  \\ $\outglob$ & outglob & single-operator reduce after postprocessing
%  \\ $\inglob$ & inglob & single-operator reduce before postprocessing
%  \\ $\Opglob$ & Opglob & single-operator reduce operator
%  \\ $\fglob$ & fglob & single-operator reduce inner function
%  \\ $\gglob$ & gglob & single-operator reduce postprocess
%  \\ $\canpruneglob$ & canpruneglob & single-operator reduce prune function
%  \\ $\deltaglob$ & deltaglob & single-operator reduce prune value
%  \\ \hline $\letterqr$ & letterqr & query-reference 'problem space'
%  \\ $\outqr$ & outqr & query-reference after postprocessing
%  \\ $\inqr$ & inqr & query-reference before postprocessing
%  \\ $\Opqr$ & Opqr & query-reference operator
%  \\ $\fqr$ & fqr & query-reference inner function
%  \\ $\gqr$ & gqr & query-reference postprocess
%  \\ $\letterqrv$ & letterqrv & mass result 'problem space'
%  \\ $\inqrv$ & inqrv & mass result function
%  \\ $\deltaqrv$ & deltaqrv & mass result prune value
%  \\ $\canpruneqrv$ & canpruneqrv & query-reference prune check
%  \\ $\varqr$ & varqrv & accumulator to build up $\inqrv$
%  \\ $\varqrvparent$ & varqrvparent & 'parent' values of $\inqrv$ from upwards prunes
%  \\ \hline $\lettermu$ & lettermu & query-reference summary values
%  \\ && 'problem space'
%  \\ $\inmu$ & inmu & summary result function
%  \\ $\Outopmu$ & Outopmu & summary result query-summarization
%  \\ $\Opmu$ & Opmu & summary result reference-joining
%  \\ $\fmu$ & fmu & inner function converting $\inqr$ to $\inmu$
%  \\ $\fmuv$ & fmuv & inner function converting $\inqrv$ to $\inmu$
%  \\ $\deltamu$ & deltamu & 'initial' summary results
%  \\ $\canprunemu$ & canprunemu & prune check for extrinsic prunes
%  \\ $\heurqr$ & heurqr & expansion heuristic
%  \\ $\varmuchild$ & varmuchild & high-quality, built bottom-up with $\Outopmu$
%  \\ $\varmuparent$ & varmuparent & low-quality, built top-down with $\Opmu$
%  \\ \hline $\letterstat$ & letterstat & statistic 'problem space'
%  \\ $\outstat$ & outstat & statistic after postprocessing
%  \\ $\instat$ & instat & statistic before postprocessing
%  \\ $\Opstat$ & Opstat & combining statistics
%  \\ $\fstat$ & fstat & statistic inner function
%  \\ $\gstat$ & gstat & statistic postprocessing function
%  \\ \hline
%  \end{tabular}
%  \caption{Notation used throughout the paper.}
%  \label{tab:notation}
%  \end{table*}

%\section{SIAM Guidelines (copied from website)}

%Papers submitted to this conference must not have been accepted or be under review by another conference with a published proceedings or by a journal.
%The work may be either theoretical or applied, but should make a significant contribution to the field.
%Full papers should have a maximum length of 12 pages, short papers and poster papers a maximum length of 6 pages (single-spaced, 2 column, 10 point font, and at least 1" margin on each side).
%Authors should use US Letter (8.5" x 11") paper size.
%Papers must have an abstract with a maximum of 300 words and a keyword list with no more than 6 keywords.
%Authors are required to submit their papers electronically in PDF format (postscript files can be converted using standard convertors).

\end{document}

