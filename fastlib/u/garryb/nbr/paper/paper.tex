
%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{icdm07}
\usepackage{times}
\usepackage{amsmath}

\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\DeclareMathOperator*{\map}{map}
\DeclareMathOperator*{\worst}{worst}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\TWOPT}{TWOPOINT}
\DeclareMathOperator{\cardinality}{cardinality}
\DeclareMathOperator{\hrect}{hrect}
\DeclareMathOperator{\dfs}{dfs}
\DeclareMathOperator{\child}{child}
\DeclareMathOperator{\all}{all}
\DeclareMathOperator{\visited}{visited}
\DeclareMathOperator{\unvisited}{unvisited}
\DeclareMathOperator{\prune}{prune}
\DeclareMathOperator{\IF}{if}
\DeclareMathOperator{\ATDISCRETION}{}
\newcommand{\muv}{\union^{\oplus}}
\newcommand{\bigmuv}{\union^{\oplus}}
\newcommand{\muh}{\oplus^{\union}}
\newcommand{\bigmuh}{\bigoplus^{\union}}
\newcommand{\dist}{d}
\newcommand{\wholeR}{\mathcal{R}}
\newcommand{\wholeQ}{\mathcal{Q}}
\newcommand{\x}{\hspace{0.15in}}
\newcommand{\xx}{\hspace{0.3in}}
\newcommand{\xxx}{\hspace{0.45in}}

\newcommand{\nbody}{$n$-body}

\newcommand{\kdleft}{\hspace{-0.02in}^L}
\newcommand{\kdright}{\hspace{-0.02in}^R}
\newcommand{\kdparent}{\hspace{-0.02in}^P}

\newcommand{\myOp}[1]{\mathop{\bigotimes\nolimits\hspace{-0.045in}_{#1}}}
\newcommand{\myop}[1]{\otimes\hspace{-0.04in}_{#1}\hspace{0.03in}}

\newcommand{\allpsi}{\Psi}
\newcommand{\Oppsi}{\myOp{\Psi}}
\newcommand{\oppsi}{\myop{\Psi}}
\newcommand{\fpsi}{f_{\Psi}}
\newcommand{\gpsi}{g_{\Psi}}
\newcommand{\canprunepsi}{C_{\Psi}}
\newcommand{\deltapsi}{\delta_{\Psi}}

\newcommand{\allpi}{\pi}
\newcommand{\Oppi}{\myOp{\pi}}
\newcommand{\oppi}{\myop{\pi}}
\newcommand{\fpi}{f_{\pi}}
\newcommand{\gpi}{g_{\pi}}
\newcommand{\canprunepi}{C_{\pi}}
\newcommand{\deltapi}{\delta_{\pi}}

\newcommand{\allrho}{\rho}
\newcommand{\Oprho}{\myOp{\rho}}
\newcommand{\oprho}{\myop{\rho}}
\newcommand{\frho}{f_{\rho}}
\newcommand{\grho}{g_{\rho}}

\newcommand{\allmu}{\mu}
\newcommand{\Opmu}{\myOp{\mu}}
\newcommand{\opmu}{\myop{\mu}}
\newcommand{\fmu}{f_{\mu}}
\newcommand{\gmu}{g_{\mu}}

\newcommand{\allsigma}{\sigma}
\newcommand{\Opsigma}{\myOp{\sigma}}
\newcommand{\opsigma}{\myop{\sigma}}
\newcommand{\fsigma}{f_{\sigma}}

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Framework for Fast Parallel Generalized N-Body Methods}

\author{Garrett F. Boyer, Ryan N. Riegel, Alexander G. Gray
\\ Georgia Institute of Technology
\\ Computational Science and Engineering
\\ 801 Atlantic Drive, Atlanta, GA
\\ garryb@cc.gatech.edu
\\
% For a paper whose authors are all at the same institution, 
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'', 
% just like the second author.
\and
\\ Robert Nichol
\\ University of Portsmouth
\\ bob.nichol@port.ac.uk
}

\maketitle
\thispagestyle{empty}

\begin{abstract}

Comments to myself:
\footnote{Use ``element'' instead of ``point''}
\footnote{Make some definitions}

(two different unmerged abstracts follow)
The multi-tree approach to accelerating data mining methods has proven successful at making practical a large class of fundamental methods previously considered computationally intractable for massive datasets in many cases, including kernel density estimation, all-nearest-neighbor search, spatial statistics, and many others.
In this paper we pursue further acceleration by parallelizing this class of efficient serial algorithms by first developing a framework for characterizing these algorithms, and demonstrating the effectiveness of a parallel implementation of the framework.
We present how these algorithms yield an elegant strategy for parallelizing the workload and demonstrate their added efficiency on massive synthetic and real datasets.
We consider both shared-memory and cluster implementations.

We find it is possible to express many dual-tree algorithms in a standard programming model.
Such a model allows flexible implementations, such that many different expansion patterns, including parallel ones, are allowed.
Our particular choices are motivated by ongoing yet unpublished work to formalize the characteristics of all generalized $N$-body problems.
\end{abstract}

\section{Introduction}

I will rewrite this intro -- this is the old one.

Parallelism is an approach to speeding up algorithms that will become one of the ultimate sources of speedup.
Already, this approach has made it in laptop computers and gaming consoles, and work is being done to create chips with as many as 80 cores.
In this paper we will look at ways to exploit this resource for both a single computer and for clusters of computers.

{\bf Fast multi-tree algorithms.}
We here denote a class of algorithms, called \emph{dual-tree} or \emph{multi-tree} algorithms.
These which compute results on two or more hierarchial spatial decomposition trees such as $kd$-trees or ball trees.
The class of problems these algorithms solve is referred to as the \emph{generalized $N$-body problems} because of their close resemblance to the classic physical $N$-body problem\cite{gray_nbody}.
Currently, multi-tree algorithms offer the fastest practical serial algorithms for many statistical and physical problems, such as kernel density estimation, $n$-point correlation, nearest neighbors, Gaussian process and local polynomial regression, and several others.\
% Add citations
These multi-tree algorithms have so far not been parallelized, although as we shall show, they expose great parallelization potential.

{\bf Parallel trees.} There is no shortage of research on the parallelization of spatial tree
algorithms.
In fact, the classical physical $N$-body algorithms, the Barnes-Hut\cite{barnes_hut} and Fast Multipole Method\cite{greengard_fmm}, have had massively parallel implementations for well over a decade\cite{singh_loadbal, grama_para_bh, amor_para_bh}.

In this paper, we present dual-tree algorithms for three different problems, each of which demonstrates variation in mechanical properties of multi-tree methods.
The analysis of these examples yields strategies that may be reused across the board for multi-tree algorithms.
Although we briefly mention strategies for massive parallelization, this paper focuses on the core potential for parallelization of these multi-tree algorithms rather than the implementations themselves.

%TO MENTION:
%
%- Constant-time operators
%
%
%Big big big big big
%
%my paper - parallel algorithms with application...
%
%
%My claims:
%  Generalized N-body Problems have locality
%  Not trivial to parallelize, but their generalization makes parallelization
%     trivial
%
%What I must show:
% - characterize what dual-tree algorithms are
% - what are query/reference problems, and what is a pure pairwise map-reduce
% - first: characterize problems with global results, and go over intrinsic
% and extrinsic prunes
%
%
%INTRO
%
%  dual tree algorithms are x


%----------------------------------------------------------------------------
%
%P02 What is a generalized N-body problem?
%
%----------------------------------------------------------------------------
%
%P00 What is a dual tree algorithm?  Why is it useful?
%dep: P02
%
%----------------------------------------------------------------------------
%
%P01 What are we showing?
%dep: P00, P02
%
%We find it is possible to express many dual-tree algorithms in a standard programming model.
%Such a model allows flexible implementations, such that many different expansion patterns, including parallel ones, are allowed.
%Our particular choices are motivated by ongoing yet unpublished work to formalize the characteristics of all generalized $N$-body problems.
%
%----------------------------------------------------------------------------
%
%P03 What is a global reduction and how do we generalize?
%dep: P01

\section{Background}

\subsection{Parallel \nbody Simulations}

Among the most extensively studied parallelization problem for tree-based algorithms is the \nbody force approximation algorithm developed by Barnes and Hut in \cite{barneshut}, and to some extent, the Greengard's Fast Multipole Method in \cite{fmm}.
The \nbody simulation problem attempts to model the movement of $N$ particles in an evolving system, during which each particle exerts a certain amount of force on all of the other particles.
Computational solutions break the simulation into many discrete time steps, during which the total force exerted upon each particle is calculated:

 $$\map_{q \in D} \sum_{r \in D} f(q, r)$$

\noindent where $q$ and $r$ are points in the system $D$, and $f(\cdot, \cdot)$ is a pairwise force function such as gravitational or Coulombic force.
Executing the sum directly, the execution is $O(N^2)$ where $N$ = $|D|$.

begin(appel is not really dual tree)

The earliest tree-based algorithm by Appel \cite{appel} is a dual-tree algorithm that approximates these force contributions.
Unfortunately, the parallelization community has shied away from this algorithm due to claimed implementational difficulty and the popularity of competing methods.

end

The algorithm by Barnes and Hut, presented in \cite{barneshut}, is order $O(N log N)$.
For each query point $q$, the force contribution is approximated by recursively traversing an oct-tree\footnote{An oct-tree recursively divides all 3 dimensions in half spatially until the node is sufficiently small.}.
When recursing, if the point $q$ is further than parameter $\theta$ times the spatial width of the node, the force calculation is approximated by placing all points at the node's center of mass.
Greengard's Fast Multipole Method \cite{greengard_fmm} has some elements of dual-tree computation in its downwards pass, in which the force contribution for an entire node of queries is estimated for an entire node of references.
The Fast Multipole Method is considered to have a higher constant factor; thus, most research focuses on the single-tree Barnes-Hut\cite{fmm_slower}.

--------

Effective parallelization of an algorithm requires a problem decomposition coupled with load balancing, along with minimized communication.

Parallelization on particule simulations have several strengths that aid in parallelism.
Since each force calculation step is part of a larger time series, points move little in each step.
It is therefore suitable to distribute the points among the processors.

%  \begin{verbatim}
%  - Pangfeng Liu & Sandeep N. Bhatt n-body paper
%    - compute nodes which are required
%      - we can't do this in general
%      - first, we are not a simulation in general; only one time step
%      - to avoid adding programmer complexity we don't require any information about necessary interactions
%        - their algorithms assume only *intrinsic* prunes
%      - we also can't predict load balancing
%      - since I'm actually *generalizing* their work, I have to emphasize and back up this claim
%  - "Oh yeah"
%    - Define "map" operator and the rest of our notation
%  \end{verbatim}

\section{Dual-Tree Algorithm Framework}

Here, we characterize most dual tree algorithms as belonging to one of two categories, and show how each category can be generalized.

A global reduction computes a single scalar or a vector of results that is constant with the number of data points.
This type of problem is typically characterized by a single commutative, associative operator applied to pairs of inputs.
Such problems include 2-point correlation, kernel-based data likelihood estimates, and the closest-pair problem.

On the other hand, a query-reference problem requires a result for each point in a query set.
For each point in the query set, execute a commutative, associative operator over every element in a reference set.
These problems include such fundamental problems as all-nearest-neighbors, kernel density estimation, the testing phase of most nonparametric classification methods, matrix-vector multiplication, and many more.

In the next subsections, we formalize a mathematical framework for solving both of these types of problems, and briefly explain how these two types of problems can be unified.

\subsection{Just the math}

% WALDO: MIssing background: twopoint

A global reduce computes a single scalar or relatively small vector of results, which are the application of an aggregate commutative and associative operator to all pairs of inputs.
We define a global reduce as:
\begin{equation}
\allpsi(X, Y) = \Oppsi_{(x, y) \in X \times Y} \fpsi(x, y)
\label{eqn:defpsi}
\end{equation}

\noindent where $\allpsi$ is the result, $\Oppsi$ is a commutative and associative operator, and $\fpsi$ is the inner function.
For instance, the two-point correlation problem $\TWOPT(X, r) = \sum_{(x, y) \in X \times X} I(d(x - y) < r)$ can be expressed with $TWOPT(X, r) = \allpsi_r(X, X)$, $\Oppsi = \sum$, and $\fpsi(x, y) = I(d(x, y) < r)$.

%WALDO: Missing: abelian monoid

Since $\Oppsi$ is commutative and associative, we can break its computation arbitrarily.
Consider a tree decomposition by partitioning set $X$ into subsets $X\kdleft$ and $X\kdright$.
Using this decomposition, we can decompose $\allpsi$ using one of two identities:
\begin{eqnarray}
\allpsi(X, Y) &=& \allpsi(X\kdleft, Y) \oppsi \allpsi(X\kdright, Y)
\\
\allpsi(X, Y) &=& \allpsi(X, Y\kdleft) \oppsi \allpsi(X, Y\kdright)
\label{eqn:dividepsi}
\end{eqnarray}

Dual-tree algorithms are typically executed in a recursion such that both transformations are applied simultaneously; that is, both $X$ and $Y$ are divided into subtrees.

Another feature of these problems is that the inner function $\fpsi$ can be bounded for an entire subproblem, and sometimes lead to the ability to {\it prune} a sub-computation.
In two-point correlation, if $\left(\min_{(x, y) \in X \times Y} d(x, y)\right) > r$, i.e. all pairs of points are within a radius, then $\allpsi(X, Y) = |X| \times |Y|$.
Unfortunately, this maximum distance is difficult to compute directly.
Instead, it is reasonable to compute an upper bound for this distance by pessimistically using the hyper-rectangle for each node: $\left(\max_{(x, y) \in X \times Y} d(x, y)\right) < d^{\max}_{\hrect}(\allsigma_{\hrect}(X), \allsigma_{hrect}(Y))$.

The meta-function $\allsigma$ corresponds to any relevant summary statistic.
In addition to the commonplace statistic $\allsigma_{\hrect}$, two-point correlation additionally uses the cardinality statistic $\allsigma_{\cardinality}(X) = |X|$ if two subsets are within a radius.
Algorithms that utilize multipole expansions may require the mean, variance, or other moment statistics.
We then generalize this type of prune as a {\it intrinsic prune}, because it is applicable based only on summary statistics of $X$ and $Y$, based on the following rule:
\begin{equation}
\text{if } \canprunepsi(\allsigma(X), \allsigma(Y)) \text{, then } \allpsi(X, Y) = \deltapsi(X, Y)
\label{eqn:intrinsic}
\end{equation}

\noindent where $\canprunepsi$ is a Boolean indicator that a prune is possible, and $\deltapsi$ summarizes the result of a subproblem.
For two-point correlction $\deltapsi(X, Y)$ is either $0$ or $|X| \times |Y|$.

As an aside, $\allsigma$ is computed in a similar manner, as a function of commutative and associative operators:
\begin{equation}
\allsigma(X) = \Opsigma_{x \in X} \fsigma(x)
\label{eqn:defsigma}
\end{equation}

\noindent and can be computed bottom-up via the rule $\allsigma(X) = \allsigma(X\kdleft) \opsigma \allsigma(X\kdright)$.

\noindent The previous substitution rules are sufficient for the dual-tree execution of some generalized $N$-body problems.
A recursive execition will apply Equation \ref{eqn:dividepsi} until both sets are sufficiently small, and directly compute \ref{eqn:naivepsi}.
However, many other executions, including parallel, are permitted by these rules.
Thus, by separating the computation of the recurrence relationship from the problem-specific details, parallelization of all these algorithms requires only the up-front cost of a single parallel execution mechanism.

The model shown so far is limited to very simple dual-tree algorithms.
First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{WALDO list more}.

\subsection{Simple query-reference problems}

A query-reference problem performs a computation over an entire reference $R$ set independently for each point in a query set $Q$:
\begin{equation}
\allrho(q, R) = \grho \left( \Oprho_{r \in R} \frho (q, r) \right)
\end{equation}

\noindent This type of problem includes, in addition to the classic \nbody force calculation, the problems of all-nearest-neighbors, k-nearest-neighbors classification, kernel density estimation, and more.
Although each query is independent, speedup is achievable by looking at queries {\it en masse}.
For instance, one the contribution of one set of references on an entire set of queries might be known exactly, or easily approximated.

We apply a small modification to the problem to allow for the problems where the distributive property applies between $\frho(\cdot, \cdot)$ and $\Oprho$\footnote{As an example, some problems that are computable via polar expansions can accumulate the multipole moments first for an entire subtree of queries, propgate the moments over all the queries, and evaluate the expression independently for each query.}.
Some parts of $\frho$ can be moved into a post-processing step $\gpi$ if $\gpi$ can be made aware of the query point:
\begin{equation}
\allrho(q, R) = \gpi \left(q, \Oppi_{r \in R} \fpi(q, r) \right)
\end{equation}

\noindent If $\frho$ and $\Oprho$ completely distributive, then the inner function $\fpi(q, r)$ is a function of $r$ only; the information about the references can be combined at a larger scale before being distributed to each query.
If they are not distributive, then the outer function $\gpi(q, x) = x$ and $\fpi(q, r) = \frho(q, r)$.
In a dual-tree algorithm $\allrho$ can be rewritten, renaming the inner computation as $\allpi$:
\begin{equation}
\allpi(\{q\}, R) = \Oppi_{r \in R} \fpi(q, r)
\label{eqn:defpi}
\end{equation}

\noindent The first argument to $\allpi$ is actually a set.
For simplicity, we will first discuss the single tree approach where the first argument is only a single query, $Q = \{q\}$.
Then, $\allpi$ can be computed recursively:
\begin{equation}
\allpi(Q, R) = \allpi(Q, R\kdleft) \oppi \allpi(Q, R\kdright)
\label{eqn:dividepi}
\end{equation}

\noindent An intrinsic prune can be defined for query-reference problems:
\begin{equation}
\text{if } \canprunepi(\allsigma(Q), \allsigma(R)) \text{, then } \allpi(Q, R) = \deltapi(\allsigma(Q), \allsigma(R))
\label{eqn:prunepi}
\end{equation}

In a dual-tree problem, somtimes $\allpi$ can be defined for a large set of queries.
For instance, in kernel density summation for a finite kernel, $\allpi(Q, R)$ is zero if $d^{\max}_{\hrect}(\allsigma_{\hrect}(Q), \allsigma_{\hrect}(R))$ is greater than the bandwidth.
When a prune happens over a large span of queries, we can apply this prune to each query.
Expressed formally:
\begin{equation}
\text{if prune occurs for } Q\kdparent \supset Q \text{, then } \allpi(Q, R) = \allpi(Q\kdparent, R)
\label{eqn:parentpi}
\end{equation}

\subsection{Query-reference extrinsic prunes}

Query-reference problems may also prune computations based on previous results computed for a query\footnote{Global reduction problems have this kind of prune too, but we have omitted it for space.}.

In the all nearest neighbors problem, if $d^{\min}_{\hrect}(\allsigma_{\hrect}(Q), \allsigma{\hrect}(R))$ is greater than any of the candidate nearest neighbors found so far, then the computation can be skipped.
This type of prune is due to properties of $\Oprho$, i.e., that $\min(a, b) = a$ if $a < b$.
Some prunes are due to properties of $\grho$, in that it is possible to prove that an entire query point has completed computation.
For instance, for $\rho(q, R) = I(\sum_{r \in R} K(q, r) < t)$.

We can define $\allmu(Q, R)$ as the worst-case results of all query points in a set:
 \begin{equation}
\allmu(Q, R) = \worst_{q \in Q} \allrho(q, R)
\label{eqn:defmu}
\end{equation}

\noindent We here overload the term $\worst$ to refer to the the most pessimistic value.
The pessimistic value is essential in making pruning decisions.
This value can become an additional input of the prune indication function $\canprunepi$ and the prune computation function $\deltapi$.

Expressed recursively, we have:
\begin{equation}
\allmu(Q, R) = \worst(\allmu(Q\kdleft, R), \allmu(Q\kdright, R))
\label{eqn:dividemu}
\end{equation}

\noindent However, it is not necessary we have the exact value of $\allmu$.
Instead, we only need a worst-case bound for it, and thus can bound $\allmu$ at any stage during computation.
If we know $\allmu$ for two different references, we can use the worst-case value in each, composing them in a way analagous to the original $\Oprho$:
\begin{equation}
\allmu(Q, R) \leq \allmu(Q, R\kdleft) \Opmu \allmu(Q, R\kdright)
\label{eqn:combinemu}
\end{equation}

\noindent Although the above seems obvious due to commutativity and associativity, it is overly pessimistic: the worst-case value for $R\kdleft$ may occur at a different query point from that for $R\kdright$.
Next, if a prune occured and $\pi(Q, R)$ is defined, then we may bound its worst-case contribution:
\begin{equation}
\allmu(Q, R) \leq \gpi^{\worst}(\allsigma(Q), \allpi(Q, R))
\label{eqn:pimu}
\end{equation}

\noindent Finally, we realize that the worst-case bounds for a large set of queries is necessarily more pessimistic than that for a subset of queries.
If information is not available at a particular granularity, we may always use a coarser granularity:

\begin{equation}
\allmu(Q, R) \leq \allmu(Q\kdparent, R) \text{ if } Q\kdparent \supset Q
\end{equation}

\noindent In the end, we have defined several different rules that allow the computation of bounding information on the results for a set of queries.


\subsection{Combining the Two}

Query-reference problems, compared to global-reductions, are far more complicated.
Nonetheless, it is possible to support both types of algorithms with a united framework.
For both types of problems, we will hereby refer to one of the two trees as a query tree, and the other as a reference, regardless of whether th is is accurate for the problem.
Although this label is inaccurate, it assists in unifying the two types of problems.

For instance, query-reference problems can compute a global result $\gamma$, although this global result may simply be null, with null operators.
Similarly, global reduce problems may compute a null per-query result $\allrho$.

Another issue to resolve is the overloaded operator $\delta$.
For global reductions, $\delta(\allsigma(Q), \allsigma(R))$ is the contribution to the total result; in query-reference problems, $\delta(\allsigma(Q), \allsigma(R))$ summarizes the worst-case effects of a set of reference points on the set of query points.
These definitions are in fact quite related, especially if we apply the query-reference nomenclature to the inputs of the global reduce problem.

The complete algorithmic model we use in our C++ implementation contains all the elements from both of the discussed models, with a few additions to allow more efficient base case computation.
Since each recurrence is over-specified, there are a multitude of ways that these can be executed.
Our goal in implementation is to choose the execution pattern that maximizes performance for a given problem and for a given set of resources.

\section{Example Algorithms in the Framework}

\section{Recursion Patterns}

DFS meta-algorithm for query-reference problems:
(Maybe I should show global reduce problems instead since it's simpler)

\begin{equation*}
  \begin{array}{l}
    \text{for all nodes/points } Q \in \wholeQ, \hatmu(Q, \wholeR) \gets 0_{\Opmu}
    \\
    \text{for all nodes/points } Q \in \wholeQ, \hatpi(Q, \prune) \gets 0_{\Oppi}
    \\
    \mbox{procedure dfs}(Q, R, \hatmu\kdparent)
    \\
    \x \muvisit \gets \muchild \opmu \deltapi(\allsigma(Q), \pi(Q, \prune))
    \\
    \x \hatmu(Q, \all) \gets \hatmu(Q, \visited) \opmu \delta(\allsigma(Q), \allsigma(R) \opmu \mu\kdparent
    \\
    \x \mbox{if } C(\allsigma(Q), \allsigma(R), \hatmu(Q, \all)))
    \\
    \xx \pi(Q, \prune) \gets \pi(Q, \prune) \oplus^{1} F^{1}(\allsigma(Q), \allsigma(R))
    \\
    \x \mbox{elif } |Q| \geq |R|
    \\
    \xx \mbox{for } c = L, R
    \\
    \xxx \pi(Q_c, \prune) \gets \pi(Q_c, \prune) \oplus^{1} \pi(Q, \prune)
    \\
    \xxx \mbox{dfs}(Q_c, R, \mu^{*})
    \\
    \xx \mbox{end for}
    \\
    \xx \mu(Q, \child) \gets \mu(Q_L, \visited) \muv \mu(Q_R, \visited)
    \\
    \xx \pi(Q, \prune) \gets 0^{\oplus^{1}}
    \\
    \x \mbox{else}
    \\
    \xx (R_a, R_b) \gets \mbox{prioritize}(\allsigma(Q), R_L, R_R)
    \\
    \xx \dfs(Q, R_a, \mu^{*} \muh \delta(\allsigma(Q), \allsigma(R_b))
    \\
    \xx \dfs(Q, R_b, \mu^{*})
    \\
    \x \mbox{end if}
    \\
    \mbox{end dfs}
    \\
    \mbox{function fixup}(Q, \pi^{*})
    \\
    \x \pi(Q, \all) \gets \pi^{*} \oplus^{1} \pi(Q, \prune)
    \\
    \x \mbox{if } |Q| = 1
    \\
    \xx \allrho(q) \gets f^{2}(q, \pi(Q, \all))
    \\
    \x \mbox{else}
    \\
    \xx \mbox{fixup}(Q_L, \pi(Q, \all))
    \\
    \xx \mbox{fixup}(Q_R, \pi(Q, \all))
    \\
    \x \mbox{fi}
    \\
    \mbox{end fixup}
  \end{array}
\end{equation*}

\section{Parallel Execution}

We have already shown the general character of 



----------------------------------------------------------------------------

P05 Are we better than mapreduce?

----------------------------------------------------------------------------




Ongoing unpublished research attempts to
characterize all generalized N-body problems with rigorous mathematical
formality; however, such rigorous formalization is outside the scope of this
paper.  Here, we postulate that all dual-tree algorithms fall into one or
both of two categories: a single scalar or vector result summarizing all
pairs of inputs, or an independent result for each query point computed
against the entire reference set.

The former category can be generalized by a few custom functions that are
unique to the problem at hand, with many possible implementations.






\subsection{Global Reductions}

A global reduce computes a single scalar or relatively small vector of results, which are the application of an aggregate commutative and associative operator to all pairs of inputs.
The simplest form of global reduce can be expressed as a recurrence relation, where all subcomputations are entirely independent.
A notable example is the two-point correlation problem:
\begin{equation*}
  \TWOPT(X, r) = \sum_{(x, y) \in X \times X} I(d(x, y) < r)
\end{equation*}

\noindent where $I$ is the indicator function, $r$ is a constant radius, and $d(x, y)$ is a distance metric.
Suppose then we can partition any set $X$ into two subsets $X_L$ and $X_R$ both of cardinality $\Theta(|X|)$, such as in a balanced $kd$-tree.
A simple yet over-specified recurrence relation exists for $\TWOPT$, if we allow ourselves to think of $\TWOPT$ as finding the correlation between two different data sets (omitting the radius):
\begin{equation}
 \begin{array}{ll}
  \lefteqn{\TWOPT(X, Y) =}
  \\
  & \left\{
    \begin{array}{lr}
      0 & \IF d^{\min}(X, Y) \geq r
      \\
      |X||Y| & \IF d^{\max}(X, Y) \leq r
      \vspace{0.07in}
      \\
      \multicolumn{2}{l}{\TWOPT(X_L, Y) + \TWOPT(X_R, Y)}
      \\
      \multicolumn{2}{c}{\scriptstyle{\mbox{\small{or}}}}
      \\
      \multicolumn{2}{l}{\TWOPT(X, Y_L) + \TWOPT(X, Y_R)}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent where $d^{\max}(X, Y)$ is the maximum distance between any two points in $X$ and $Y$, likewise for $d^{\min}$.
Unfortunately, direct computation of actual the maximum distance not practical.
Summary statistics to allow the constant-time\footnote{with respect to number of points, not dimensions} computation of an upper bound for the actual maximum distance.
In a $kd$-tree, one suitable summary statistic is the bounding hyper-rectangle for each node:
\begin{equation}
d^{\max}(X, Y) \leq d^{\max}_{\hrect}(\allsigma_{\hrect}(X), \allsigma_{\hrect}(Y))
\end{equation}

\noindent The meta-function $\allsigma$ corresponds to any relevant summary statistic, which can vary from problem to problem.
In addition to the commonplace statistic $\allsigma_{\hrect}$, two-point correlation additionally uses the cardinality statistic $\allsigma_{\cardinality}(X) = |X|$.
Algorithms that utilize multipole expansions may require the mean, variance, or other moment statistics.

We can now generalize the entire recurrence relation for simple global reduce problems.
We claim that problems of the form:
\begin{equation}
  \gamma(X, Y) = \bigoplus_{(x, y) \in X \times Y} f(a,b)
\end{equation}

\noindent where $\bigoplus$ is any commutative and associative operator, can be expressed as:
\begin{equation}
\label{eqn:global_intrinsic}
 \begin{array}{ll}
  \lefteqn{\gamma(X, Y) =}
  \\
  & \left\{
    \begin{array}{lr}
      \delta \left( \allsigma(X), \allsigma(Y) \right) & \IF C(\allsigma(X), \allsigma(Y))
      \\
      \gamma(X_L, Y) \oplus \gamma(X_R, Y) & \mbox{if convenient}
      \\
      \gamma(X, Y_L) \oplus  \gamma(X, Y_R) & \mbox{if convenient}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent under the following model:
\begin{itemize}
  \item $\gamma(X, Y)$ - The result of the global reduce, on the entire problem or on a subproblem.
  \item $\allsigma(X)$ - Summary statistics over a range of data.
  \item $\delta(\allsigma(X), \allsigma(Y))$ - The estimated contribution of a sub-computation using only summary statistics.
  \item $C(\allsigma(X), \allsigma(Y))$ - An indicator if a prune is possible, and that the subproblem $\gamma(X, Y)$ can be substituted by $\delta(\allsigma(X), \allsigma(Y))$.
  \item $\bigoplus$ - The reduce operator.
\end{itemize}

\noindent We also note that in most cases $\allsigma$ is in the form:
\begin{equation}
  \allsigma(X) = \bigoplus^{\allsigma}_{x \in X} f^{\allsigma}(x)
\end{equation}

\noindent for some commutative, associative $\bigoplus^{\allsigma}$, and can be calculated efficiently in a bottom-up pass while building a $kd$-tree:
\begin{equation}
\label{eqn:sigma}
 \begin{array}{ll}
  \lefteqn{\allsigma(X) =}
  \\
  & \left\{
    \begin{array}{lr}
      f^{\allsigma}(X) & \IF |X| = 1
      \\
      \allsigma(X_L) \oplus^{\allsigma} \allsigma(X_R) & \mbox{otherwise}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent The previous functions define the execution of some generalized $N$-body problems in a form that is easily implemented as a dual-tree algorithm\footnote{In practice, very small subsets may be computed exhaustively to reduce the overhead of treating individual points as entire subsets.}.

There is significant freedom in how to compute the thse recurrence relationships.
The $\gamma$ recurrence permits many types of expansion, such as depth-first, breadth-first, and importantly, parallel expansions.
Thus, by separating the computation of the recurrence relationship from the problem-specific details, parallelization of all these algorithms requires only the up-front cost of a single parallel execution mechanism.

The model shown so far is limited to very simple dual-tree algorithms.
First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{WALDO list more}.

\subsection{Query-Reference Problems}

% Depends: Global Reductions

A query-reference problem considers computes a result independently for each point in a query set.
These problems take on the general form:
 
  $$\map_{q \in Q} \bigoplus_{r \in R} f(q, r)$$

Although each query is independent, speedup can still be achieved by looking at queries {\it en masse}.
It may be possible to show that one set of references is irrelevant to an entire set of queries within a computation, and thus prune the subproblem.

We define $\allrho(q, R)$ as each element of the map, i.e. $\allrho(q, R) = \bigoplus_{r \in R} f(q, r)$.
Next, we decompose $f$ into two separate functions $f^1$ and $f^2$ such that $f^2(q, f^1(q, r)) = f(q, r)$.
We make this decomposition to allow us to take advantage of distributivity for problems where $f(\cdot, \cdot)$ distributes over $\bigoplus$.
In the dual-tree algorithm, $\allrho$ can be computed as follows:
\begin{equation}
  \label{eqn:rho}
  \allrho(q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R)) = f^{2}(q, \pi(\{q\}, R))
  \end{equation}
  
\noindent where $\pi$ is the {\it postponed information} from prunes and/or exhaustive computations:
\begin{equation}
\label{eqn:pi}
 \begin{array}{ll}
  \lefteqn{\pi(Q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R))}
  \\
  & \left\{
    \begin{array}{lr}
      F^{1}(\allsigma(Q), \allsigma(R)) & \IF C(\allsigma(Q), \allsigma(R), \mu(Q))
      \\
      \pi(Q^{*}, R) & \mbox{if defined for } Q^{*} \supset Q
      \\
      \pi(Q, R_L) \oplus^{1} \pi(Q, R_R) & \mbox{otherwise}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent where the {\it on-line per-query-node pruning information} is defined:
\begin{equation}
\label{eqn:mu}
 \begin{array}{ll}
  \lefteqn{\mu(Q, R) =}
  \\
  & \left\{
    \begin{array}{lr}
      f(q, r) & \IF Q = \{q\} \mbox{ and } R = \{r\}
      \\
      \delta(\allsigma(Q), \allsigma(R)) & \mbox{if convenient}
      \\
      F^2(\allsigma(Q), \pi(Q, R)) & \mbox{if defined}
      \\
      \mu(Q_L, R) \muv \mu(Q_R, R) & \mbox{if convenient}
      \\
      \mu(Q, R_L) \muh \mu(Q, R_R) & \mbox{if convenient}
      \\
      \mu(Q^{*}, R) & \mbox{otherwise, for } Q^{*} \supset Q
    \end{array}
  \right.
 \end{array}
\end{equation}

under the model\footnote{In practice for Equation \ref{eqn:mu}, rules 1 and 4 are most accurate but only used in depth-first expansions, rule 6 is used only when no other information is available, and rule 5 is used primarily in breadth-first expansions.}:
\begin{itemize}
  \item $F^1(\allsigma(Q), \allsigma(R))$ - A pairwise function to allow for distributivity only valid when pruning is possible, identical to $f^1(\cdot, \cdot)$ when $|Q| = |R| = 1$.
  If $f(\cdot, \cdot)$ distributes over $\bigoplus$, then $f^1(\allsigma(R))$ is often the same as $\allsigma(R)$.
  Otherwise, it usually corresponds to a value whose effect on the overall problem is indistinguishable from $\forall_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
  \item $F^2(\allsigma(Q), \pi(Q, R))$ - A post-processing function that determines the bounds of the effects of $\pi$ on a region of queries.
  \item $\bigoplus^{1}$ - A commutative and associative operator that operates on the results of $f^1$ before $f^2$ is applied, simulating the effects of the original $\bigoplus$ as specified in Equation \ref{eqn:rho}.
  \item $\delta(\allsigma(Q), \allsigma(R))$ - A function that computes pessimistic bounds on the contribution of all of $R$ to any element of $Q$.
  Formally, succinct information bounding the set of results $\union_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
  For kernel density estimation, this is the pair $(\min_{q \in Q} \sum_{r \in R} K(q, r), \max_{q \in Q} \sum_{r \in R} K(q, r))$.
  \item $\muv$ - An operator to relax the bounds for a query region.
  Given $\mu(Q_L, R)$ and $\mu(Q_R, R)$, we can create more pessimistic pruning information for the worst-case effect of $R$ on any of the query points $Q$.
  In many algorithms, this operator takes the minimum of some lower bound and maximum of some upper bound.
  \item $\bigmuh$ - An operator that pessimistically merges the contributions of two disjoint reference subsets on the same query subset.
  For kernel summation problems, this sums the lower and upper bounds.
  \item $C$ - A Boolean function indicating if a prune is possible, using information from $\mu$.
  The operators applied in order to compute $\mu$ depend on the expansion pattern and the sequence of node pairs that have already been visited.
  It is entirely dependent on the type of problem being solved which expansion pattern, whether it be depth-first or breadth-first, favors the most pruning.
 For some problems such as exact kernel density estimation, $C$ does not use $\mu$ at all, and any expansion pattern can be used.
\end{itemize}

It is necessary to clarify in Equation \ref{eqn:pi} the ``if defined'' comment for the second rule.
The recurrence relationship only defines a base case if the prune check $C$ is true; thus, $\pi$ is not defined if the second rule is invoked improperly.

{\bf Theorem}\footnote{use LaTeX to do this}: If the prune check $C(\allsigma(Q), \allsigma(R), \cdot)$ is always true for the base case $|Q| = |R| = 1$, then the recurrence of Equation \ref{eqn:pi} invoked via Equation \ref{eqn:rho} must must be defined even if the second rule is not invoked.

{\bf Proof}: The initial invokation, per Equation \ref{eqn:rho}, begins with $Q$ being the singleton set $\{q\}$.  Further invokations will retain $|Q| = 1$ since $Q$ is not modified by the third rule.  However, invoking on $R_L$ and $R_R$ always decreases the reference set by at least one.  Therefore, $|R| = 1$ eventually, and since $|Q| = 1$ always, the first rule will eventually define the result.

In fact, evaluating $\allrho$ in Equations \ref{eqn:rho} and \ref{eqn:pi} as such is identical to the single-tree algorithm -- recursive subdivision along the reference tree until a prune is possible.
However, dual-tree algorithms may prune multiple queries at once, prompting the existence of the second rule in Equation \ref{eqn:pi}.

It is also noteworthy that Equation \ref{eqn:mu} may evaluate to very different values during the course of the dual-tree computation.
As more of the tree is expanded, it becomes ``convenient'' to expand $\mu$ in a manner more advantageous to pruning.

MOTIVATION FOR THIS MODEL: Every type of information transfer is accounted for.


\nocite{ex1,ex2}
\bibliographystyle{latex8}
\bibliography{icdm07}

\end{document}

