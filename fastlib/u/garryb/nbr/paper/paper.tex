	
%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{icdm07}
\usepackage{times}
\usepackage{amsmath}

\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\DeclareMathOperator*{\map}{map}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\TWOPT}{TWOPOINT}
\DeclareMathOperator{\cardinality}{cardinality}
\DeclareMathOperator{\hrect}{hrect}
\DeclareMathOperator{\dfs}{dfs}
\DeclareMathOperator{\child}{child}
\DeclareMathOperator{\all}{all}
\DeclareMathOperator{\visited}{visited}
\DeclareMathOperator{\unvisited}{unvisited}
\DeclareMathOperator{\prune}{prune}
\DeclareMathOperator{\IF}{if}
\DeclareMathOperator{\ATDISCRETION}{}
\newcommand{\muv}{\union^{\oplus}}
\newcommand{\bigmuv}{\union^{\oplus}}
\newcommand{\muh}{\oplus^{\union}}
\newcommand{\bigmuh}{\bigoplus^{\union}}
\newcommand{\dist}{d}
\newcommand{\wholeR}{\mathcal{R}}
\newcommand{\wholeQ}{\mathcal{Q}}
\newcommand{\x}{\hspace{0.15in}}
\newcommand{\xx}{\hspace{0.3in}}
\newcommand{\xxx}{\hspace{0.45in}}

%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Framework for Fast Parallel Generalized N-Body Methods}

\author{Garrett F. Boyer, Ryan N. Riegel, Alexander G. Gray
\\ Georgia Institute of Technology
\\ Computational Science and Engineering
\\ 801 Atlantic Drive, Atlanta, GA
\\ garryb@cc.gatech.edu
\\
% For a paper whose authors are all at the same institution, 
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'', 
% just like the second author.
\and
\\ Robert Nichol
\\ University of Portsmouth
\\ bob.nichol@port.ac.uk
}

\maketitle
\thispagestyle{empty}

\begin{abstract}

Comments to myself:
\footnote{Use ``element'' instead of ``point''}
\footnote{Make some definitions}

(two different unmerged abstracts follow)
The multi-tree approach to accelerating data mining methods has proven successful at making practical a large class of fundamental methods previously considered computationally intractable for massive datasets in many cases, including kernel density estimation, all-nearest-neighbor search, spatial statistics, and many others.
In this paper we pursue further acceleration by parallelizing this class of efficient serial algorithms by first developing a framework for characterizing these algorithms, and demonstrating the effectiveness of a parallel implementation of the framework.
We present how these algorithms yield an elegant strategy for parallelizing the workload and demonstrate their added efficiency on massive synthetic and real datasets.
We consider both shared-memory and cluster implementations.

We find it is possible to express many dual-tree algorithms in a standard programming model.
Such a model allows flexible implementations, such that many different expansion patterns, including parallel ones, are allowed.
Our particular choices are motivated by ongoing yet unpublished work to formalize the characteristics of all generalized $N$-body problems.
\end{abstract}

\section{Introduction}

I will rewrite this intro -- this is the old one.

Parallelism is an approach to speeding up algorithms that will become one of the ultimate sources of speedup.
Already, this approach has made it in laptop computers and gaming consoles, and work is being done to create chips with as many as 80 cores.
In this paper we will look at ways to exploit this resource for both a single computer and for clusters of computers.

{\bf Fast multi-tree algorithms.}
We here denote a class of algorithms, called \emph{dual-tree} or \emph{multi-tree} algorithms.
These which compute results on two or more hierarchial spatial decomposition trees such as $kd$-trees or ball trees.
The class of problems these algorithms solve is referred to as the \emph{generalized $N$-body problems} because of their close resemblance to the classic physical $N$-body problem\cite{gray_nbody}.
Currently, multi-tree algorithms offer the fastest practical serial algorithms for many statistical and physical problems, such as kernel density estimation, $n$-point correlation, nearest neighbors, Gaussian process and local polynomial regression, and several others.\
% Add citations
These multi-tree algorithms have so far not been parallelized, although as we shall show, they expose great parallelization potential.

{\bf Parallel trees.} There is no shortage of research on the parallelization of spatial tree
algorithms.
In fact, the classical physical $N$-body algorithms, the Barnes-Hut\cite{barnes_hut} and Fast Multipole Method\cite{greengard_fmm}, have had massively parallel implementations for well over a decade\cite{singh_loadbal, grama_para_bh, amor_para_bh}.

In this paper, we present dual-tree algorithms for three different problems, each of which demonstrates variation in mechanical properties of multi-tree methods.
The analysis of these examples yields strategies that may be reused across the board for multi-tree algorithms.
Although we briefly mention strategies for massive parallelization, this paper focuses on the core potential for parallelization of these multi-tree algorithms rather than the implementations themselves.

%TO MENTION:
%
%- Constant-time operators
%
%
%Big big big big big
%
%my paper - parallel algorithms with application...
%
%
%My claims:
%  Generalized N-body Problems have locality
%  Not trivial to parallelize, but their generalization makes parallelization
%     trivial
%
%What I must show:
% - characterize what dual-tree algorithms are
% - what are query/reference problems, and what is a pure pairwise map-reduce
% - first: characterize problems with global results, and go over intrinsic
% and extrinsic prunes
%
%
%INTRO
%
%  dual tree algorithms are x


%----------------------------------------------------------------------------
%
%P02 What is a generalized N-body problem?
%
%----------------------------------------------------------------------------
%
%P00 What is a dual tree algorithm?  Why is it useful?
%dep: P02
%
%----------------------------------------------------------------------------
%
%P01 What are we showing?
%dep: P00, P02
%
%We find it is possible to express many dual-tree algorithms in a standard programming model.
%Such a model allows flexible implementations, such that many different expansion patterns, including parallel ones, are allowed.
%Our particular choices are motivated by ongoing yet unpublished work to formalize the characteristics of all generalized $N$-body problems.
%
%----------------------------------------------------------------------------
%
%P03 What is a global reduction and how do we generalize?
%dep: P01

\section{Background}

\subsection{Parallel N-Body Simulations}

\begin{verbatim}
- Pangfeng Liu & Sandeep N. Bhatt n-body paper
  - compute nodes which are required
    - we can't do this in general
    - first, we are not a simulation in general; only one time step
    - to avoid adding programmer complexity we don't require any information about necessary interactions
      - their algorithms assume only *intrinsic* prunes
    - we also can't predict load balancing
    - since I'm actually *generalizing* their work, I have to emphasize and back up this claim
\end{verbatim}

\section{Dual-Tree Algorithm Framework}

Here, we characterize most dual tree algorithms as belonging to one of two categories, and show how each category can be generalized.

A global reduction computes a single scalar or a vector of results that is constant with the number of data points.
This type of problem is typically characterized by a single commutative, associative operator applied to pairs of inputs.
Such problems include 2-point correlation, kernel-based data likelihood estimates, and the closest-pair problem.

On the other hand, a query-reference problem requires a result for each point in a query set.
For each point in the query set, execute a commutative, associative operator over every element in a reference set.
These problems include such fundamental problems as all-nearest-neighbors, kernel density estimation, the testing phase of most nonparametric classification methods, matrix-vector multiplication, and many more.

In the next subsections, we formalize a mathematical framework for solving both of these types of problems, and briefly explain how these two types of problems can be unified.

\subsection{Global Reductions}

A global reduce computes a single scalar or relatively small vector of results, which are the application of an aggregate commutative and associative operator to all pairs of inputs.
The simplest form of global reduce can be expressed as a recurrence relation, where all subcomputations are entirely independent.
A notable example is the two-point correlation problem:
\begin{equation*}
  \TWOPT(X, r) = \sum_{(x, y) \in X \times X} I(d(x, y) < r)
\end{equation*}

\noindent where $I$ is the indicator function, $r$ is a constant radius, and $d(x, y)$ is a distance metric.
Suppose then we can partition any set $X$ into two subsets $X_L$ and $X_R$ both of cardinality $\Theta(|X|)$, such as in a balanced $kd$-tree.
A simple yet over-specified recurrence relation exists for $\TWOPT$, if we allow ourselves to think of $\TWOPT$ as finding the correlation between two different data sets (omitting the radius):
\begin{equation}
 \begin{array}{ll}
  \lefteqn{\TWOPT(X, Y) =}
  \\
  & \left\{
    \begin{array}{lr}
      0 & \IF d^{\min}(X, Y) \geq r
      \\
      |X||Y| & \IF d^{\max}(X, Y) \leq r
      \vspace{0.07in}
      \\
      \multicolumn{2}{l}{\TWOPT(X_L, Y) + \TWOPT(X_R, Y)}
      \\
      \multicolumn{2}{c}{\scriptstyle{\mbox{\small{or}}}}
      \\
      \multicolumn{2}{l}{\TWOPT(X, Y_L) + \TWOPT(X, Y_R)}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent where $d^{\max}(X, Y)$ is the maximum distance between any two points in $X$ and $Y$, likewise for $d^{\min}$.
Unfortunately, direct computation of actual the maximum distance not practical.
Summary statistics to allow the constant-time\footnote{with respect to number of points, not dimensions} computation of an upper bound for the actual maximum distance.
In a $kd$-tree, one suitable summary statistic is the bounding hyper-rectangle for each node:
\begin{equation}
d^{\max}(X, Y) \leq d^{\max}_{\hrect}(\sigma_{\hrect}(X), \sigma_{\hrect}(Y))
\end{equation}

\noindent The meta-function $\sigma$ corresponds to any relevant summary statistic, which can vary from problem to problem.
In addition to the commonplace statistic $\sigma_{\hrect}$, two-point correlation additionally uses the cardinality statistic $\sigma_{\cardinality}(X) = |X|$.
Algorithms that utilize multipole expansions may require the mean, variance, or other moment statistics.

We can now generalize the entire recurrence relation for simple global reduce problems.
We claim that problems of the form:
\begin{equation}
  \gamma(X, Y) = \bigoplus_{(x, y) \in X \times Y} f(a,b)
\end{equation}

\noindent where $\bigoplus$ is any commutative and associative operator, can be expressed as:
\begin{equation}
\label{eqn:global_intrinsic}
 \begin{array}{ll}
  \lefteqn{\gamma(X, Y) =}
  \\
  & \left\{
    \begin{array}{lr}
      \delta \left( \sigma(X), \sigma(Y) \right) & \IF C(\sigma(X), \sigma(Y))
      \\
      \gamma(X_L, Y) \oplus \gamma(X_R, Y) & \mbox{if convenient}
      \\
      \gamma(X, Y_L) \oplus  \gamma(X, Y_R) & \mbox{if convenient}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent under the following model:
\begin{itemize}
  \item $\gamma(X, Y)$ - The result of the global reduce, on the entire problem or on a subproblem.
  \item $\sigma(X)$ - Summary statistics over a range of data.
  \item $\delta(\sigma(X), \sigma(Y))$ - The estimated contribution of a sub-computation using only summary statistics.
  \item $C(\sigma(X), \sigma(Y))$ - An indicator if a prune is possible, and that the subproblem $\gamma(X, Y)$ can be substituted by $\delta(\sigma(X), \sigma(Y))$.
  \item $\bigoplus$ - The reduce operator.
\end{itemize}

\noindent We also note that in most cases $\sigma$ is in the form:
\begin{equation}
  \sigma(X) = \bigoplus^{\sigma}_{x \in X} f^{\sigma}(x)
\end{equation}

\noindent for some commutative, associative $\bigoplus^{\sigma}$, and can be calculated efficiently in a bottom-up pass while building a $kd$-tree:
\begin{equation}
\label{eqn:sigma}
 \begin{array}{ll}
  \lefteqn{\sigma(X) =}
  \\
  & \left\{
    \begin{array}{lr}
      f^{\sigma}(X) & \IF |X| = 1
      \\
      \sigma(X_L) \oplus^{\sigma} \sigma(X_R) & \mbox{otherwise}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent The previous functions define the execution of some generalized $N$-body problems in a form that is easily implemented as a dual-tree algorithm\footnote{In practice, very small subsets may be computed exhaustively to reduce the overhead of treating individual points as entire subsets.}.

There is significant freedom in how to compute the thse recurrence relationships.
The $\gamma$ recurrence permits many types of expansion, such as depth-first, breadth-first, and importantly, parallel expansions.
Thus, by separating the computation of the recurrence relationship from the problem-specific details, parallelization of all these algorithms requires only the up-front cost of a single parallel execution mechanism.

The model shown so far is limited to very simple dual-tree algorithms.
First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{WALDO list more}.

\subsection{Query-Reference Problems}

% Depends: Global Reductions

A query-reference problem considers computes a result independently for each point in a query set.
These problems take on the general form:
 
  $$\map_{q \in Q} \bigoplus_{r \in R} f(q, r)$$

Although each query is independent, speedup can still be achieved by looking at queries {\it en masse}.
It may be possible to show that one set of references is irrelevant to an entire set of queries within a computation, and thus prune the subproblem.

We define $\rho(q, R)$ as each element of the map, i.e. $\rho(q, R) = \bigoplus_{r \in R} f(q, r)$.
Next, we decompose $f$ into two separate functions $f^1$ and $f^2$ such that $f^2(q, f^1(q, r)) = f(q, r)$.
We make this decomposition to allow us to take advantage of distributivity for problems where $f(\cdot, \cdot)$ distributes over $\bigoplus$.
In the dual-tree algorithm, $\rho$ can be computed as follows:
\begin{equation}
  \label{eqn:rho}
  \rho(q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R)) = f^{2}(q, \pi(\{q\}, R))
  \end{equation}
  
\noindent where $\pi$ is the {\it postponed information} from prunes and/or exhaustive computations:
\begin{equation}
\label{eqn:pi}
 \begin{array}{ll}
  \lefteqn{\pi(Q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R))}
  \\
  & \left\{
    \begin{array}{lr}
      F^{1}(\sigma(Q), \sigma(R)) & \IF C(\sigma(Q), \sigma(R), \mu(Q))
      \\
      \pi(Q^{*}, R) & \mbox{if defined for } Q^{*} \supset Q
      \\
      \pi(Q, R_L) \oplus^{1} \pi(Q, R_R) & \mbox{otherwise}
    \end{array}
  \right.
 \end{array}
\end{equation}

\noindent where the {\it on-line per-query-node pruning information} is defined:
\begin{equation}
\label{eqn:mu}
 \begin{array}{ll}
  \lefteqn{\mu(Q, R) =}
  \\
  & \left\{
    \begin{array}{lr}
      f(q, r) & \IF Q = \{q\} \mbox{ and } R = \{r\}
      \\
      \delta(\sigma(Q), \sigma(R)) & \mbox{if convenient}
      \\
      F^2(\sigma(Q), \pi(Q, R)) & \mbox{if defined}
      \\
      \mu(Q_L, R) \muv \mu(Q_R, R) & \mbox{if convenient}
      \\
      \mu(Q, R_L) \muh \mu(Q, R_R) & \mbox{if convenient}
      \\
      \mu(Q^{*}, R) & \mbox{otherwise, for } Q^{*} \supset Q
    \end{array}
  \right.
 \end{array}
\end{equation}

under the model\footnote{In practice for Equation \ref{eqn:mu}, rules 1 and 4 are most accurate but only used in depth-first expansions, rule 6 is used only when no other information is available, and rule 5 is used primarily in breadth-first expansions.}:
\begin{itemize}
  \item $F^1(\sigma(Q), \sigma(R))$ - A pairwise function to allow for distributivity only valid when pruning is possible, identical to $f^1(\cdot, \cdot)$ when $|Q| = |R| = 1$.
  If $f(\cdot, \cdot)$ distributes over $\bigoplus$, then $f^1(\sigma(R))$ is often the same as $\sigma(R)$.
  Otherwise, it usually corresponds to a value whose effect on the overall problem is indistinguishable from $\forall_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
  \item $F^2(\sigma(Q), \pi(Q, R))$ - A post-processing function that determines the bounds of the effects of $\pi$ on a region of queries.
  \item $\bigoplus^{1}$ - A commutative and associative operator that operates on the results of $f^1$ before $f^2$ is applied, simulating the effects of the original $\bigoplus$ as specified in Equation \ref{eqn:rho}.
  \item $\delta(\sigma(Q), \sigma(R))$ - A function that computes pessimistic bounds on the contribution of all of $R$ to any element of $Q$.
  Formally, succinct information bounding the set of results $\union_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
  For kernel density estimation, this is the pair $(\min_{q \in Q} \sum_{r \in R} K(q, r), \max_{q \in Q} \sum_{r \in R} K(q, r))$.
  \item $\muv$ - An operator to relax the bounds for a query region.
  Given $\mu(Q_L, R)$ and $\mu(Q_R, R)$, we can create more pessimistic pruning information for the worst-case effect of $R$ on any of the query points $Q$.
  In many algorithms, this operator takes the minimum of some lower bound and maximum of some upper bound.
  \item $\bigmuh$ - An operator that pessimistically merges the contributions of two disjoint reference subsets on the same query subset.
  For kernel summation problems, this sums the lower and upper bounds.
  \item $C$ - A Boolean function indicating if a prune is possible, using information from $\mu$.
  The operators applied in order to compute $\mu$ depend on the expansion pattern and the sequence of node pairs that have already been visited.
  It is entirely dependent on the type of problem being solved which expansion pattern, whether it be depth-first or breadth-first, favors the most pruning.
 For some problems such as exact kernel density estimation, $C$ does not use $\mu$ at all, and any expansion pattern can be used.
\end{itemize}

It is necessary to clarify in Equation \ref{eqn:pi} the ``if defined'' comment for the second rule.
The recurrence relationship only defines a base case if the prune check $C$ is true; thus, $\pi$ is not defined if the second rule is invoked improperly.

{\bf Theorem}\footnote{use LaTeX to do this}: If the prune check $C(\sigma(Q), \sigma(R), \cdot)$ is always true for the base case $|Q| = |R| = 1$, then the recurrence of Equation \ref{eqn:pi} invoked via Equation \ref{eqn:rho} must must be defined even if the second rule is not invoked.

{\bf Proof}: The initial invokation, per Equation \ref{eqn:rho}, begins with $Q$ being the singleton set $\{q\}$.  Further invokations will retain $|Q| = 1$ since $Q$ is not modified by the third rule.  However, invoking on $R_L$ and $R_R$ always decreases the reference set by at least one.  Therefore, $|R| = 1$ eventually, and since $|Q| = 1$ always, the first rule will eventually define the result.

In fact, evaluating $\rho$ in Equations \ref{eqn:rho} and \ref{eqn:pi} as such is identical to the single-tree algorithm -- recursive subdivision along the reference tree until a prune is possible.
However, dual-tree algorithms may prune multiple queries at once, prompting the existence of the second rule in Equation \ref{eqn:pi}.

It is also noteworthy that Equation \ref{eqn:mu} may evaluate to very different values during the course of the dual-tree computation.
As more of the tree is expanded, it becomes ``convenient'' to expand $\mu$ in a manner more advantageous to pruning.

MOTIVATION FOR THIS MODEL: Every type of information transfer is accounted for.

\subsection{Combining the Two}

Query-reference problems, compared to global-reductions, are far more complicated.
Nonetheless, it is possible to support both types of algorithms with a united framework.
For both types of problems, we will hereby refer to one of the two trees as a query tree, and the other as a reference, regardless of whether th is is accurate for the problem.
Although this label is inaccurate, it assists in unifying the two types of problems.

For instance, query-reference problems can compute a global result $\gamma$, although this global result may simply be null, with null operators.
Similarly, global reduce problems may compute a null per-query result $\rho$.

Another issue to resolve is the overloaded operator $\delta$.
For global reductions, $\delta(\sigma(Q), \sigma(R))$ is the contribution to the total result; in query-reference problems, $\delta(\sigma(Q), \sigma(R))$ summarizes the worst-case effects of a set of reference points on the set of query points.
These definitions are in fact quite related, especially if we apply the query-reference nomenclature to the inputs of the global reduce problem.

The complete algorithmic model we use in our C++ implementation contains all the elements from both of the discussed models, with a few additions to allow more efficient base case computation.
Since each recurrence is over-specified, there are a multitude of ways that these can be executed.
Our goal in implementation is to choose the execution pattern that maximizes performance for a given problem and for a given set of resources.

\section{Example Algorithms in the Framework}

\section{Recursion Patterns}

DFS meta-algorithm for query-reference problems:
(Maybe I should show global reduce problems instead since it's simpler)

\begin{equation*}
  \begin{array}{l}
    \forall q \in Q, \mu(Q, \child) \gets 0^{\muh}
    \\
    \forall q \in Q, \pi(Q, \prune) \gets 0^{\oplus^{1}}
    \\
    \mbox{procedure dfs}(Q, R, \mu^{*})
    \\
    \x \mu(Q, \visited) \gets \mu(Q, \child) \muh F^{2}(\sigma(Q), \pi(Q, \prune))
    \\
    \x \mu(Q, \all) \gets \mu(Q, \visited) \muh \delta(\sigma(Q), \sigma(R) \muh \mu^{*}
    \\
    \x \mbox{if } C(\sigma(Q), \sigma(R), \mu(Q, \all)))
    \\
    \xx \pi(Q, \prune) \gets \pi(Q, \prune) \oplus^{1} F^{1}(\sigma(Q), \sigma(R))
    \\
    \x \mbox{elif } |Q| \geq |R|
    \\
    \xx \mbox{for } c = L, R
    \\
    \xxx \pi(Q_c, \prune) \gets \pi(Q_c, \prune) \oplus^{1} \pi(Q, \prune)
    \\
    \xxx \mbox{dfs}(Q_c, R, \mu^{*})
    \\
    \xx \mbox{end for}
    \\
    \xx \mu(Q, \child) \gets \mu(Q_L, \visited) \muv \mu(Q_R, \visited)
    \\
    \xx \pi(Q, \prune) \gets 0^{\oplus^{1}}
    \\
    \x \mbox{else}
    \\
    \xx (R_a, R_b) \gets \mbox{prioritize}(\sigma(Q), R_L, R_R)
    \\
    \xx \dfs(Q, R_a, \mu^{*} \muh \delta(\sigma(Q), \sigma(R_b))
    \\
    \xx \dfs(Q, R_b, \mu^{*})
    \\
    \x \mbox{end if}
    \\
    \mbox{end dfs}
    \\
    \mbox{function fixup}(Q, \pi^{*})
    \\
    \x \pi(Q, \all) \gets \pi^{*} \oplus^{1} \pi(Q, \prune)
    \\
    \x \mbox{if } |Q| = 1
    \\
    \xx \rho(q) \gets f^{2}(q, \pi(Q, \all))
    \\
    \x \mbox{else}
    \\
    \xx \mbox{fixup}(Q_L, \pi(Q, \all))
    \\
    \xx \mbox{fixup}(Q_R, \pi(Q, \all))
    \\
    \x \mbox{fi}
    \\
    \mbox{end fixup}
  \end{array}
\end{equation*}

\section{Parallel Execution}

We have already shown the general character of 



----------------------------------------------------------------------------

P05 Are we better than mapreduce?

----------------------------------------------------------------------------




Ongoing unpublished research attempts to
characterize all generalized N-body problems with rigorous mathematical
formality; however, such rigorous formalization is outside the scope of this
paper.  Here, we postulate that all dual-tree algorithms fall into one or
both of two categories: a single scalar or vector result summarizing all
pairs of inputs, or an independent result for each query point computed
against the entire reference set.

The former category can be generalized by a few custom functions that are
unique to the problem at hand, with many possible implementations.







\nocite{ex1,ex2}
\bibliographystyle{latex8}
\bibliography{icdm07}

\end{document}

