
%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, leqno,twocolumn]{article} 
\usepackage{icdm07}
\usepackage{times}
\usepackage{amsmath}

\newcommand{\authornote}[1]{\footnote{Note to self: #1}}

\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\DeclareMathOperator*{\map}{map}
\DeclareMathOperator*{\worst}{worst}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\TWOPT}{TWOPOINT}
\DeclareMathOperator{\cardinality}{cardinality}
\DeclareMathOperator{\hrect}{hrect}
\DeclareMathOperator{\dfs}{dfs}
\DeclareMathOperator{\child}{child}
\DeclareMathOperator{\visited}{visited}
\DeclareMathOperator{\unvisited}{unvisited}
\DeclareMathOperator{\prune}{prune}
\DeclareMathOperator{\IF}{if}
\DeclareMathOperator{\ATDISCRETION}{}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}

\newcommand{\wholeR}{\mathcal{R}}
\newcommand{\wholeQ}{\mathcal{Q}}

\newcommand{\X}{\\ \scriptstyle}
\newcommand{\x}{\\ \hspace{0.13in} \scriptstyle}
\newcommand{\xx}{\\ \hspace{0.26in} \scriptstyle}
\newcommand{\xxx}{\\ \hspace{0.39in} \scriptstyle}
\newcommand{\xxxx}{\\ \hspace{0.52in} \scriptstyle}

\newcommand{\defterm}[1]{{\bf #1}}
\newcommand{\nbody}{$N$-body}

\newcommand{\kdroot}[1]{#1^{\text{root}}}
\newcommand{\kdleft}[1]{#1^{\!L}}
\newcommand{\kdright}[1]{#1^{\!R}}
\newcommand{\kdparent}[1]{#1^{\!P}}

\newcommand{\lo}[1]{#1^{l}}
\newcommand{\up}[1]{#1^{u}}
\newcommand{\distlo}[1]{\lo{d_{\text{hrect}}}}
\newcommand{\distup}[1]{\up{d_{\text{hrect}}}}
\newcommand{\dist}[2]{d(#1,#2)}

%\newcommand{\myOp}[1]{\mathop{\bigotimes\nolimits\hspace{-0.045in}_{#1}}}
\newcommand{\myOp}[1]{\mathop{\bigotimes\nolimits\!\!_{#1}}}
%\newcommand{\myop}[1]{\otimes\hspace{-0.04in}_{#1}\hspace{0.03in}}
\newcommand{\myop}[1]{{\scriptstyle\:}\otimes_{\!#1}}
\newcommand{\myOutop}[1]{\mathop{\bigodot\nolimits\!\!_{#1}}}
\newcommand{\myoutop}[1]{\mathop{\bigodot\nolimits\!\!_{#1}}}

\newcommand{\letterglob}{\psi}
\newcommand{\outglob}{\Psi}
\newcommand{\inglob}{\psi}
\newcommand{\Opglob}{\myOp{\letterglob}}
\newcommand{\opglob}{\myop{\letterglob}}
\newcommand{\fglob}{f_{\letterglob}}
\newcommand{\gglob}{g_{\letterglob}}
\newcommand{\canpruneglob}{C_{\letterglob}}
\newcommand{\deltaglob}{\delta_{\letterglob}}

\newcommand{\outpi}{\Pi}
\newcommand{\Oppi}{\myOp{\outpi}}
\newcommand{\oppi}{\myop{\outpi}}
\newcommand{\fpi}{f_{\outpi}}
\newcommand{\gpi}{g_{\outpi}}
\newcommand{\canprunepi}{C_{\outpi}}
\newcommand{\deltapi}{\delta_{\outpi}}
\newcommand{\gpiworst}{g_{\outpi}^{\worst}}

%\newcommand{\letterrho}{\rho}
%\newcommand{\inrho}{\rho}
%\newcommand{\outrho}{\rho}
%\newcommand{\Oprho}{\myOp{\rho}}
%\newcommand{\oprho}{\myop{\rho}}
%\newcommand{\frho}{f_{\rho}}
%\newcommand{\grho}{g_{\rho}}

\newcommand{\letterqr}{\pi}
\newcommand{\outqr}{\Pi}
\newcommand{\inqr}{\pi}
\newcommand{\Opqr}{\myOp{\letterqr}}
\newcommand{\opqr}{\myop{\letterqr}}
\newcommand{\fqr}{f_{\letterqr}}
\newcommand{\gqr}{g_{\letterqr}}
\newcommand{\deltaqr}{\delta_{\letterqr}}

\newcommand{\letterqrv}{\vec{\pi}}
%\newcommand{\outqrv}{\vec{\rho}}
\newcommand{\inqrv}{\vec{\pi}}
\newcommand{\Opqrv}{\myOp{\letterqrv}}
\newcommand{\opqrv}{\myop{\letterqrv}}
\newcommand{\fqrv}{f_{\letterqrv}}
%\newcommand{\gqrv}{g_{\letterqrv}}
\newcommand{\deltaqrv}{\delta_{\letterqrv}}
\newcommand{\canpruneqrv}{C_{\letterqrv}}

\newcommand{\lettermu}{\mu}
%\newcommand{\inmu}{\mu}
\newcommand{\inmu}{\mu}
\newcommand{\Outopmu}{\myOutop{\mu}}
\newcommand{\outopmu}{\myoutop{\mu}}
\newcommand{\Opmu}{\myOp{\mu}}
\newcommand{\opmu}{\myop{\mu}}
\newcommand{\fmu}{f_{\mu}}
\newcommand{\fmuv}{\vec{f_{\mu}}}
%\newcommand{\gmu}{g_{\mu}}
\newcommand{\deltamu}{\delta_{\mu}}
\newcommand{\canprunemu}{C_{\mu}}
\newcommand{\allowmu}{\preceq}

\newcommand{\muparent}{\inmu_{\text{coarse}}}
\newcommand{\muchild}{\inmu_{\text{children}}}
\newcommand{\muvisit}{\inmu_{\text{visited}}}
\newcommand{\muall}{\inmu_{\text{all}}}

\newcommand{\hatpi}{\hat{\outpi}}
\newcommand{\piparent}{\outpi_{\text{parent}}}

\newcommand{\namestat}[1]{\Sigma_{\text{#1}}}
\newcommand{\outstat}{\Sigma}
\newcommand{\instat}{\sigma}
\newcommand{\Opstat}{\myOp{\instat}}
\newcommand{\opstat}{\myop{\instat}}
\newcommand{\fstat}{f_{\instat}}
\newcommand{\gstat}{g_{\instat}}

% Affinity propagation


\newcommand{\eqspace}{\!\!\!\!}
\newcommand{\true}{\text{true}}
\newcommand{\ocpos}[1]{c^{+}_{#1}}
\newcommand{\ocneg}[1]{c^{-}_{#1}}
\newcommand{\cpos}[2]{\ocpos{#1 \neq #2}}
\newcommand{\cneg}[2]{\ocneg{#1 \neq #2}}

\newcommand{\respo}[2]{R_{#1#2}}
\newcommand{\avail}[2]{A_{#1#2}}
\newcommand{\simil}[2]{S_{#1#2}}

\newcommand{\vecrho}{\vec{\rho}}
\newcommand{\vecalpha}{\vec{\alpha}}
\newcommand{\frho}[1]{\rho_{#1}}
\newcommand{\falpha}[1]{\alpha_{#1}}
\newcommand{\falphaj}[2]{\alpha_{#1[#2]}}

\newcommand{\falphamax}{\alpha^{u}}
\newcommand{\falphamin}{\alpha^{l}}
\newcommand{\frhomax}{\rho^{u}}
\newcommand{\frhomin}{\rho^{l}}

\newcommand{\alphacand}{v}


%\documentstyle[times,art10,twocolumn,latex8]{article}

%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Framework for Fast Parallel Generalized N-Body Methods}

\author{Garrett F. Boyer, Ryan N. Riegel, Alexander G. Gray
\\ Georgia Institute of Technology
\\ Computational Science and Engineering
\\ 801 Atlantic Drive, Atlanta, GA
\\ garryb@cc.gatech.edu
\\
% For a paper whose authors are all at the same institution, 
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'', 
% just like the second author.
\and
\\ Robert Nichol
\\ University of Portsmouth
\\ bob.nichol@port.ac.uk
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
The multi-tree approach to accelerating data mining methods has proven successful at making practical a large class of fundamental methods previously considered computationally intractable for massive datasets in many cases, including kernel density estimation, all-nearest-neighbor search, spatial statistics, and many others.
In this paper we pursue further acceleration by parallelizing this class of efficient serial algorithms by first developing a framework for characterizing these algorithms, and demonstrating the effectiveness of a parallel implementation of the framework.
We present how these algorithms yield an elegant strategy for parallelizing the workload and demonstrate their added efficiency on massive synthetic and real datasets for both shared-memory and cluster implementations.
\end{abstract}

\section{Introduction}

\authornote{Use ``element'' instead of ``point''}
\authornote{Make some definitions}

Many problems in spatial statistics and data mining nominally require an all-$n$-tuples analysis of data points but may be able to take advantage of overall spatial structure for asymptotic computational gain.
Recent research has characterized these {\it generalized $N$-body problems} and shown how a multi-tree algorithmic approach directly follows.
The multi-tree approach treats these problems by recursively dividing the problem over the Cartesian product of points, resulting in the fastest practical serial algorithms for many statistical and physical problems, such as kernel density estimation, $n$-point correlation, nearest neighbors, Gaussian process and local polynomial regression, and several others.
Nonetheless, as multi-core processors begin to dominate even commodity markets, even the fastest serial algorithms may not compete with a trivially parallelizable brute-force algorithm.
Fortunately, multi-tree algorithms are in fact quite parallel, and we show how the techniques that have long been used to parallelize the physical $N$-body force calculation problems can be extended to encompass this broader class of problems.

\section{Fast multi-tree algorithms}

\authornote{KD-tree figure goes here}

\begin{figure*}
  \begin{minipage}{3.88in}
    \begin{minipage}{1.8in}
      \begin{displaymath}
        \begin{array}{l}
          \text{{\bf tpc} - Two-point correlation.}
          \X \text{function tpc}(Y, X)
          \x \text{if }\distup(Y, X) < r\text{: return }0
          \x \text{if }\distlo(Y, X) > r\text{: return } |Y| \cdot |X|
          \x \text{elif }|Y| \geq |X|\text{:}
          \xx \text{return tpc}(\kdleft{Y}, X, r) + \text{tpc}(\kdright{Y}, X, r)
          \x \text{else:}
          \xx \text{return tpc}(Y, \kdleft{X}, r) + \text{tpc}(Y, \kdright{X}, r)
        \end{array}
       \end{displaymath}
       \caption{\footnotesize \label{fig:allnntpc} Pseudocode for two simple dual-tree algorithms.}
      \end{minipage}
      \begin{minipage}{2.0in}
       \begin{displaymath}
        \begin{array}{l}
          \text{{\bf allnn} - All-nearest-neighbors.}
          \X \text{init all nodes }Q \subseteq \kdroot{Q}\text{: }a(Q) \gets \infty
          \X \text{procedure allnn}(Q,R)\text{:}
          \x \text{if }a(Q) < \distlo(Q, R)\text{: return}
          \x \text{elif }Q = \{q\} \text{ and } R = \{r\}
          \xx a(\{q\}) \gets \min(a(\{q\}), \dist{Q}{R})
          \x \text{elif }|Q| \geq |R|\text{:}
          \xx \text{allnn}(\kdleft{Q}, R); \text{ allnn}(\kdright{Q}, R)
          \xx a(Q) \gets \max(a(\kdleft{Q}), a(\kdright{Q}))
          \x \text{else prioritize by min distance:}
          \xx \text{allnn}(Q, \kdleft{R}); \text{ allnn}(Q, \kdright{R})
          \\
        \end{array}
       \end{displaymath}
      \end{minipage}
  \end{minipage}
  \vspace{-.2in}
\end{figure*}

Core to dual-tree algorithms is the concepts of trees and bounds.
Every node $X$ in a spatial tree $\kdroot{X}$ is a set of points; every internal node is partitioned $\kdleft{X} \union \kdright{X} = X$, with each leaf a single point $\{x\}$.
For example, a $kd$-tree\cite{kdtree}, as in \fig{kdtree}, recursively partitions the data along coordinate dimensions.
Each node has a bounding box can also store summary statistics such as means and other moments.
We denote $\dist{\cdot}{\cdot}$ to be a distance metric.
Superscripts $l$ and $u$ to refer respectively to lower and upper bounds, which are often, as in the case of distance, obtained from bounding boxes of points.

\authornote{This paragraph needs some work.}A single-tree algorithm accelerates computation by partitioning the data in half along the tree branches, terminating at pruning at internal nodes when possible, often repeating this traversal for many query points.
A dual-tree algorithm takes this concept further by looking at pairs of nodes, each time partitioning the Cartesian product of data in half.
Whereas a single-tree algorithm must consider every level of the tree for each query point and is at best $O(N \log N)$, a dual-tree algorithm considers the upper-level nodes of the tree only with other upper-level nodes, sometimes resulting in empirical linear performance.

{\bf The Two-point Correlation.} A member of the family of $n$-point correlation functions, collectively the foundation for all spatial statistics, the two-point correlation of data set $X$ for radius $r$ is
$\sum_{y \in X} \sum_{x \in X} I(d(y, x) \leq r)$ for the indicator function $I$.
Figure~\ref{fig:allnntpc} shows a formulation that recursively considers a subset pair $Y$ and $X$, returning immediately if all points in $Y$ and $X$ are completely inside or outside the radius.

{\bf All-nearest-neighbors.} In applications from manifold learning to classification, it is often desirable to find for a batch of queries $Q$ the nearest neighbors from reference set $R$, as $\map_{q \in Q} \argmin_{r \in R} d(q,r)$, with the exception $d(q,q) = \infty$.
To achieve speedup, one maintains for each recursive subset of queries the furthest candidate neighbor distance found for a query node, $a(Q)$.
If a set of references is farther away than this distance, no further exploration is required.

{\bf Affinity propagation.}
Affinity propagation is a recent clustering technique that identifies exemplars in a data set, taking into account all point-to-point similarities\cite{frey_dueck}.
The similaritiy $\simil{i}{j}$ between points $x_i$ and $x_j$ is the negative squared distance or some other quantity to be maximized, with diagonals $\simil{i}{i}$ set to a parameter that penalizes creation of clusters.
The algorithm maximizes the sum of similarities between each point and its exemplar.
If a metric space is defined over the points, one may use the dual-tree algorithm in \cite{ryan_nips} to solve the modified problems,
\[ \begin{array}{l}
  \vecalpha \gets \map_{i} \argmin^2_{j} \!\left( \cpos{i}{j}(\cpos{i}{j}(\simil{i}{j} + \falphaj{i}{j}) - \frho{j}) - \simil{i}{j} \right)\! ,
  \\
  \vecrho \gets \map_{j} \sum_{i} \!\left( \cpos{i}{j}(\simil{i}{j} + \falphaj{i}{j}) \right)\!,
\end{array} \]
\noindent which are repeated iteratively with damping until convergence, with $\ocpos{v}(x)$ being $\min(x, 0)$ for $v$ true and $x$ otherwise.
Empirical $O(N^{1.3})$ scaling was achieved on three-dimensional points from a gravitational $N$-body particle simulation.

\subsection{Parallel \nbody\ Simulations}

The classic physical $N$-body particle simulation problem models the movement of $N$ particles in an evolving system where each particle exerts force on all other particles.
Computational solutions break the simulation into discrete time steps, during which force is calculated,
\[\map_{q \in D} \sum_{r \in D} f(q,r)\]
\noindent where $f(q,r)$ is often an inverse-squared-distance gravitational or Coulombic force.
Directly computing the sum takes $O(N^2)$, but speedup may be achieved by approximating distant interactions.

The earliest tree-based algorithm by Appel \cite{appel} that approximates these force contributions in an approach closely resembling the dual-tree approach.
However, the later popular Barnes-Hut algorithm\cite{barneshut} and Fast Multipole Method\cite{greengard_fmm} received attention from the parallelization community, with the former receiving the most extensive attention.

The algorithm by Barnes and Hut, presented in \cite{barneshut}, is order $O(N log N)$ for uniform distributions\authornote{Uniform?}.
For each query point $q$, the force contribution is approximated by recursively traversing an oct-tree\footnote{An oct-tree recursively divides all 3 dimensions in half spatially until the node is sufficiently small.}.
When recursing, if the point $q$ is further than parameter $\theta$ times the spatial width of the node, the force calculation is approximated by placing all points at the node's center of mass.
%Greengard's Fast Multipole Method \cite{greengard_fmm} has some elements of dual-tree computation in its downwards pass, in which the force contribution for an entire node of queries is estimated for an entire node of references.
%The Fast Multipole Method is considered to have a higher constant factor; thus, most research focuses on the single-tree Barnes-Hut\cite{fmm_slower}.

{\bf Parallelization.}
The effective parallelization of any algorithm requires three important considerations: work decomposition, load balancing, and minimized communication.



--------

Effective parallelization of an algorithm requires a problem decomposition coupled with load balancing, along with minimized communication.

Parallelization on particule simulations have several strengths that aid in parallelism.
Since each force calculation step is part of a larger time series, points move little in each step.
It is therefore suitable to distribute the points among the processors.

\section{Dual-Tree Algorithm Framework}

Ongoing research attempts to characterize all generalized N-body problems with rigorous mathematical formality; however, such rigorous formalization is outside the scope of this paper.
Here, we postulate that all dual-tree algorithms fall into one or both of two categories: a single scalar or vector result summarizing all pairs of inputs, or an independent result for each query point computed against the entire reference set.

A global reduction computes a single scalar or a vector of results that is constant with the number of data points.
This type of problem is typically characterized by a single commutative, associative operator applied to pairs of inputs.
Such problems include 2-point correlation, kernel-based data likelihood estimates, and the closest-pair problem.

A query-reference problem alternatively computes a result for each point in a query set.
For each point in the query set, execute a commutative, associative operator over every element in a reference set.
These problems include such fundamental problems as all-nearest-neighbors, kernel density estimation, the testing phase of most nonparametric classification methods, matrix-vector multiplication, and many more.

First, we introduce some formalities, which we will then extend further.

\subsection{Simple Reduce Problem}

\subsection{Global Reductions}

A \defterm{global reduction} computes a single scalar or relatively small vector of results, taking as input two data sets and applying an aggregate commutative and associative operator to all pairs:
\begin{eqnarray*}
\outglob(X, Y) &=& \gglob(\inglob(X, Y))
\\
\inglob(X, Y) &=& \Opglob_{(x, y) \in X \times Y} \fglob(x, y)
\label{eqn:defglob}
\end{eqnarray*}

\noindent where $\inglob$ is the result, $\gglob$ a post-processing function (usually identity), $\Opglob$ is a commutative and associative operator, and $\fglob$ is the inner function.
For instance, the two-point correlation problem $\TWOPT(X, r) = \sum_{(x, y) \in X \times X} I(d(x, y) < r)$ can be expressed with $TWOPT(X, r) = \inglob_r(X, X)$, $\Opglob = \sum$, and $\fglob(x, y) = I(d(x, y) < r)$.
%WALDO: Missing: abelian monoid

Since $\Opglob$ is commutative and associative, we can break its computation arbitrarily.
Consider a tree decomposition by partitioning set $X$ into subsets $\kdleft{X}$ and $\kdright{X}$.
Using this decomposition, we can decompose $\inglob$ using one of two identities:
\begin{eqnarray}
\inglob(X, Y) &=& \inglob(\kdleft{X}, Y) \opglob \inglob(\kdright{X}, Y)
\\
\inglob(X, Y) &=& \inglob(X, \kdleft{Y}) \opglob \inglob(X, \kdright{Y})
\label{eqn:divideglob}
\end{eqnarray}

\noindent Dual-tree algorithms often apply both simultaneously; that is, both $X$ and $Y$ are divided into subtrees.

A common feature is the ability to bounding the inner function $\fglob$ for an entire subproblem, sometimes leading to the ability to {\it prune} a sub-computation.
In two-point correlation, if $\left(\min_{(x, y) \in X \times Y} d(x, y)\right) > r$, i.e. all pairs of points are farther away than the radius, then $\inglob_{\TWOPT}(X, Y) = |X| \times |Y|$.
Since this maximum distance is difficult to compute directly, it is reasonable to compute an upper bound for this distance pessimistically using the node's bounding box: $\left(\max_{(x, y) \in X \times Y} d(x, y)\right) < \distlo(\namestat{hrect}(X), \namestat{hrect}(Y))$.

To allow efficient computations on a coarse granularity, each node $X$ has statistics $\outstat(X)$.
In addition to the commonplace statistic $\namestat{hrect}$, two-point correlation additionally uses the cardinality statistic $\namestat{cardinality}(X) = |X|$ if two subsets are within a radius.
Algorithms that utilize multipole expansions may require the mean, variance, and other moment statistics.
Statistics are often a function of commutative and associative operators,
\begin{eqnarray}
\outstat(X) &=& \gstat(\instat(X))
\\
\instat(X) &=& \Opstat_{x \in X} \fstat(x)
\label{eqn:defstat}
\end{eqnarray}
\noindent and can be computed bottom-up via $\instat(X) = \instat(\kdleft{X}) \opstat \instat(\kdright{X})$.

Pruning based solely on summary statistics is called an \defterm{intrinsic prune}, under the rule
\begin{equation}
\text{if } \canpruneglob(\outstat(X), \outstat(Y)) \text{, then } \inglob(X, Y) = \deltaglob(X, Y)
\label{eqn:intrinsic}
\end{equation}
\noindent where $\canpruneglob$ is a Boolean indicator that a prune is possible, and $\deltaglob$ summarizes the result of a subproblem.

\noindent The previous substitution rules are sufficient for the dual-tree execution of some generalized $N$-body problems.
A depth-first recursive execution will apply both rules in Equation \ref{eqn:divideglob} until both sets are sufficiently small that a prune or direct computation is possible.
However, many other executions, including parallel, are permitted by these rules.
By separating the computation of the recurrence relationship from the problem-specific details, parallelization of all these algorithms requires only the up-front cost of a single parallel execution mechanism.

%The model shown so far is limited to very simple dual-tree algorithms.
%First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
%Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
%Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{WALDO list more}.

\subsection{Simple query-reference problems}

A query-reference problem performs a computation over an entire reference $R$ set independently for each point $q$ in a query set $Q$,
\begin{eqnarray}
\outqr(q, R) &=& \gqr(q, \inqr(q, R))
\\
\inqr(q, R) &=& \Opqr_{r \in R} \fqr(q, r).
\end{eqnarray}

\noindent where $R$ is initially $\kdroot{R}$.
In addition to the classic \nbody\ force calculation problem, this encompasses all-nearest-neighbors, k-nearest-neighbors classification, kernel density estimation, vectorized affinity propagation, and more.
Although each query is independent, speedup is achievable by considering queries {\it en masse}; for instance, a set of references might be shown to have zero relevance to an entire distant set of queries.
We define a \defterm{mass result} $\inqrv$ such that
\begin{equation}
\forall q \in Q,~~ \gqr(q, \inqrv(Q, R)) = \outqr(q, R).
\end{equation}
\noindent Note $\inqrv(Q,R)$ is undefined if different queries have different values after the postprocessing function is applied, although for a single query $\inqrv(\{q\}, R)$ can always be computed $\inqr(q, R)$.
In a dual-tree algorithm, we may take advantage of $\inqrv$,
\begin{equation}
\text{if prune occurs for } \kdparent{Q} \supset Q \text{, then } \inqrv(Q, R) = \inqrv(\kdparent{Q}, R).
\label{eqn:parentqrv}
\end{equation}

\noindent
A single-tree algorithm treats each query as a separate computation and does not distinguish $\inqrv$ and $\inqr$.
Both algorithms, though, compute values by dividing along the reference tree,
\begin{equation}
\inqrv(Q, R) = \inqrv(Q, \kdleft{R}) \opqrv \inqrv(Q, \kdright{R}),
\label{eqn:dividepi}
\end{equation}
\noindent perhaps with an intrinsic prune,
\begin{equation}
\text{if } \canpruneqrv(\outstat(Q), \outstat(R)) \text{, then } \inqrv(Q, R) = \deltaqrv(\outstat(Q), \outstat(R)).
\label{eqn:prunepi}
\end{equation}

\subsection{Query-reference extrinsic prunes}

Query-reference problems may also prune computations based on previous results computed for a query\footnote{Global reduction problems have this kind of prune too, but since it is less common, we have omitted it for brevity.}.
\footnote{Define this in terms of the GNA.}

We define the abstract $\inmu(Q, R)$ as a summary of the results of a node of query points for an arbitrary set of references,

\begin{equation}
\inmu(Q, R) = \Outopmu_{q \in Q} \fmu(q, \inqr(q, R))
\label{eqn:defmu}
\end{equation}

\noindent where $\Outopmu$ is a commutative and associative operator, which, along with $\fmu$, summarizes pruning-relevant aspects of the results.
One strategy is to treat $\inmu$ as the set of all results, $\fmu(x) = \{x\}$ and $\Outopmu \equiv \union$.
In real-valued problems, such a set may succinctly but slightly inaccurately be represented as a pair of real-valued numbers for the lower and upper bounds.
Expressed recursively, we have,
\begin{equation}
\inmu(Q, R) = \inmu(\kdleft{Q}, R) \outopmu \inmu(\kdright{Q}, R).
\label{eqn:dividemu}
\end{equation}

The value of $\inmu(Q, \kdroot{R})$ is used to make pruning decisions in the modified $\canprunemu(\outstat(Q), \outstat(R), \inmu(Q, \kdroot{R}))$.
Unfortunately, this value is not attainable without the value of $\inqr$ for each query, in fact requiring the computation to have completed.
%Since this is not feasible, we allow for values of $\lettermu$ which do not produce any false positive in the detection of prunes.
To get around this, we define $\allowmu$ such that $\inmu(Q,R) \allowmu v$ if using some value $v$ in the place of $\inmu(Q, R)$ will never result in pruning when it would actually be invalid to do so.
\authornote{This is the same concept that allows use of bounding boxes to compute maximum pairwise distance.}
One may bound $\inmu$ using summary statistics,
\begin{equation}
\inmu(Q, R) \allowmu \deltamu(\outstat(Q), \outstat(R)).
\label{eqn:approxmu}
\end{equation}

\noindent Next, if we know $\inmu$ for two different references, we may compose them in a way analagous to the original $\Opqr$,
\begin{equation}
\inmu(Q, R) \allowmu \inmu(Q, \kdleft{R}) \opmu \inmu(Q, \kdright{R}).
\label{eqn:combinemu}
\end{equation}

\noindent It is important to note that the above may be quite pessimistic.
For example, if $\inmu$ is an upper bound on a probability density contribution, summing the upper bounds neglects the possibility of having the upper bound appear at different query points for the two subcomputations.
\authornote{Elaborate more, or make less comments.  Actually, this might want a proof.}
Next, if $\pi(Q, R)$ is defined because this computation was pruned, then we may bound its worst-case contribution,
\begin{equation}
\inmu(Q, R) \allowmu \fmuv(\outstat(Q), \inqrv(Q, R)).
\label{eqn:pimu}
\end{equation}

\noindent 
If information is not available at a particular granularity of queries, we may always use a coarser granularity.
That is, if a fact can be proven about a large set of queries, no false positives are introduced by using the same information about a subset of those queries,
\authornote{This probably requires proof, or we must somehow build this into the definition of $\Outopmu$.  I might also define this in terms of ``pessimism''.}

\begin{equation}
\inmu(Q, R) \allowmu \inmu(\kdparent{Q}, R) \text{ if } \kdparent{Q} \supset Q
\end{equation}

\noindent In the end, we have defined several different rules that allow the computation of bounding information on the results for a set of queries.

\section{Recursion Patterns}

DFS meta-algorithm for query-reference problems:
(Maybe I should show global reduce problems instead since it's simpler)

\begin{equation*}
  \begin{array}{l}
    \text{for all nodes/points } Q \in \wholeQ, \muchild(Q) \gets 0_{\inmu}
    \\
    \text{for all nodes/points } Q \in \wholeQ, \hatpi(Q) \gets 0_{\outpi}
    \\
    \mbox{procedure dfs}(Q, R, \muparent)
    \\
    \x \muall \gets \muchild(C) \opmu \gpiworst(\outstat(Q), \hatpi(Q))
    \\ \x \qquad \opmu \deltamu(\outstat(Q), \outstat(R)) \opmu \muparent
    \\
    \x \text{if } \canprunepi(\outstat(Q), \outstat(R), \muall)
    \\
    \xx \hatpi(Q) \gets \hatpi(Q) \oppi \deltapi(\outstat(Q), \outstat(R), \muall)
    \\
    \x \text{elif } |Q| \geq |R| \text{ (heuristic)}
    \\
    \xx \text{for } Q' = \kdleft{Q}, \kdright{Q}
    \\
    \xxx \hatpi(Q') \gets \hatpi(Q') \oppi \hatpi(Q)
    \\
    \xxx \text{dfs}(Q', R, \muparent)
    \\
    \xxx \muvisit(Q') \gets \muchild(Q') \opmu \gpiworst(\outstat(Q'), \hatpi(Q'))
    \\
    \xx \text{end for}
    \\
    \xx \muchild(Q) \gets \worst(\muvisit(\kdleft{Q}), \muvisit(\kdright{Q}))
    \\
    \xx \hatpi(Q) \gets 0_{\outpi}
    \\
    \x \mbox{else}
    \\
    \xx (R_a, R_b) \gets \mbox{prioritize}(\outstat(Q), R_L, R_R)
    \\
    \xx \dfs(Q, R_a, \muparent \opmu \deltamu(\outstat(Q), \outstat(R_b))
    \\
    \xx \dfs(Q, R_b, \muparent)
    \\
    \x \text{end if}
    \\
    \text{end dfs}
    \\
    \text{function fixup}(Q, \piparent)
    \\
    \x \hatpi(Q) \gets \hatpi(Q) \oppi \piparent
    \\
    \x \text{if } |Q| = 1 \text{, i.e. } Q = \{q\}
    \\
    \xx \outqr(q) \gets \gpi(q, \hatpi(Q))
    \\
    \x \text{else}
    \\
    \xx \text{fixup}(\kdleft{Q}, \hatpi(Q))
    \\
    \xx \text{fixup}(\kdleft{Q}, \hatpi(Q))
    \\
    \x \mbox{end if}
    \\
    \mbox{end fixup}
  \end{array}
\end{equation*}

\section{Parallel Execution}

\nocite{ex1,ex2}
\bibliographystyle{latex8}
\bibliography{icdm07}

\end{document}

