
INTRODUCTION STUFF
%We here denote a class of algorithms, called \emph{dual-tree} or \emph{multi-tree} algorithms.
%These compute results on two or more hierarchial spatial decomposition trees such as $kd$-trees or ball trees.
%The class of problems these algorithms solve is referred to as the \emph{generalized $N$-body problems} because of their close resemblance to the classic physical $N$-body problem\cite{gray_nbody}.
%Currently, multi-tree algorithms offer the fastest practical serial algorithms for many statistical and physical problems, such as kernel density estimation, $n$-point correlation, nearest neighbors, Gaussian process and local polynomial regression, and several others.\
%% Add citations
%These multi-tree algorithms have so far not been parallelized, although as we shall show, they expose great parallelization potential.

%I will rewrite this intro -- this is the old one.
%
%Parallelism is an approach to speeding up algorithms that will become one of the ultimate sources of speedup.
%Already, this approach has made it in laptop computers and gaming consoles, and work is being done to create chips with as many as 80 cores.
%In this paper we will look at ways to exploit this resource for both a single computer and for clusters of computers.

%{\bf Fast multi-tree algorithms.}
%We here denote a class of algorithms, called \emph{dual-tree} or \emph{multi-tree} algorithms.
%These which compute results on two or more hierarchial spatial decomposition trees such as $kd$-trees or ball trees.
%The class of problems these algorithms solve is referred to as the \emph{generalized $N$-body problems} because of their close resemblance to the classic physical $N$-body problem\cite{gray_nbody}.
%Currently, multi-tree algorithms offer the fastest practical serial algorithms for many statistical and physical problems, such as kernel density estimation, $n$-point correlation, nearest neighbors, Gaussian process and local polynomial regression, and several others.\
% % Add citations
%These multi-tree algorithms have so far not been parallelized, although as we shall show, they expose great parallelization potential.

%{\bf Parallel trees.} There is no shortage of research on the parallelization of spatial tree
%algorithms.
%In fact, the classical physical $N$-body algorithms, the Barnes-Hut\cite{barnes_hut} and Fast Multipole Method\cite{greengard_fmm}, have had massively parallel implementations for well over a decade\cite{singh_loadbal, grama_para_bh, amor_para_bh}.

%In this paper, we present dual-tree algorithms for three different problems, each of which demonstrates variation in mechanical properties of multi-tree methods.
%The analysis of these examples yields strategies that may be reused across the board for multi-tree algorithms.
%Although we briefly mention strategies for massive parallelization, this paper focuses on the core potential for parallelization of these multi-tree algorithms rather than the implementations themselves.

%TO MENTION:
%  Not trivial to parallelize, but their generalization makes parallelization
%     trivial
%What I must show:
% - characterize what dual-tree algorithms are
% - what are query/reference problems, and what is a pure pairwise map-reduce
% - first: characterize problems with global results, and go over intrinsic
% and extrinsic prunes
%P02 What is a generalized N-body problem?
%P00 What is a dual tree algorithm?  Why is it useful?
%P01 What are we showing?
%dep: P00, P02
%We find it is possible to express many dual-tree algorithms in a standard programming model.
%Such a model allows flexible implementations, such that many different expansion patterns, including parallel ones, are allowed.
%Our particular choices are motivated by ongoing yet unpublished work to formalize the characteristics of all generalized $N$-body problems.
%
%----------------------------------------------------------------------------
%
%P03 What is a global reduction and how do we generalize?
%dep: P01


%  \begin{verbatim}
%  - Pangfeng Liu & Sandeep N. Bhatt n-body paper
%    - compute nodes which are required
%      - we can't do this in general
%      - first, we are not a simulation in general; only one time step
%      - to avoid adding programmer complexity we don't require any information about necessary interactions
%        - their algorithms assume only *intrinsic* prunes
%      - we also can't predict load balancing
%      - since I'm actually *generalizing* their work, I have to emphasize and back up this claim
%  - "Oh yeah"
%    - Define "map" operator and the rest of our notation
%  \end{verbatim}

%A {\bf dual-tree algorithm} approaches the task of partitioning
%input sets $X$ and $Y$ throughout computation by precomputing trees
%for $\kdroot{X}$ and $\kdroot{Y}$ and reusing splits from those
%trees to obtain $\kdleft{X} \cup \kdright{X} = X$ and $\kdleft{Y}
%\cup \kdright{Y} = Y$.


% {\bf Nonparametric Bayes Classification.}
% A simple kernel-based classifier labels each point in $Q$ positive or negative by weighing the density generated by a kernel at each training example in $R\spos$ and $R\sneg$, weighed respectively by priors $\pi\spos$ and $\pi\sneg$:
% \begin{equation*}
% \map_{q \in Q} I\Big(\sum_{r \in R\spos} \pi\spos K(q,r) - \sum_{r \in R\sneg} \pi\sneg K(q,r) \geq 0 \Big)
% \end{equation*}
% \noindent for distance-based kernel $K$.
% Each subset of queries maintains an upper and lower bound on the difference of the sums, and is labelled when this range no longer contains zero.
% A simple non-recursive algorithm uses a priority queue, relying on the fact that iterating over all children is quite inexpensive compared to the recursion\footnote{In practice, some of these updates may be performed lazily.}.


%We introduce the concept of a \textit{reduce problem}.
%Reduce problem $\outx$ is defined:
%$$\outx(p, V) \gx \left( \Opx_{v \in V} f(p, v) \right)$$

%\where $\outx$ is the reduction problem, $p$ is an invariant {\it parameter} ....


% WALDO: MIssing background: twopoint












%  \subsection{Global Reductions}
%  
%  A global reduce computes a single scalar or relatively small vector of results, which are the application of an aggregate commutative and associative operator to all pairs of inputs.
%  The simplest form of global reduce can be expressed as a recurrence relation, where all subcomputations are entirely independent.
%  A notable example is the two-point correlation problem:
%  \begin{equation*}
%    \TWOPT(X, r) = \sum_{(x, y) \in X \times X} I(d(x, y) < r)
%  \end{equation*}
%  
%  \noindent where $I$ is the indicator function, $r$ is a constant radius, and $d(x, y)$ is a distance metric.
%  Suppose then we can partition any set $X$ into two subsets $X_L$ and $X_R$ both of cardinality $\Theta(|X|)$, such as in a balanced $kd$-tree.
%  A simple yet over-specified recurrence relation exists for $\TWOPT$, if we allow ourselves to think of $\TWOPT$ as finding the correlation between two different data sets (omitting the radius):
%  \begin{equation}
%   \begin{array}{ll}
%    \lefteqn{\TWOPT(X, Y) =}
%    \\
%    & \left\{
%      \begin{array}{lr}
%        0 & \IF d^{\min}(X, Y) \geq r
%        \\
%        |X||Y| & \IF d^{\max}(X, Y) \leq r
%        \vspace{0.07in}
%        \\
%        \multicolumn{2}{l}{\TWOPT(X_L, Y) + \TWOPT(X_R, Y)}
%        \\
%        \multicolumn{2}{c}{\scriptstyle{\mbox{\small{or}}}}
%        \\
%        \multicolumn{2}{l}{\TWOPT(X, Y_L) + \TWOPT(X, Y_R)}
%      \end{array}
%    \right.
%   \end{array}
%  \end{equation}
%  
%  \noindent where $d^{\max}(X, Y)$ is the maximum distance between any two points in $X$ and $Y$, likewise for $d^{\min}$.
%  Unfortunately, direct computation of actual the maximum distance not practical.
%  Summary statistics to allow the constant-time\footnote{with respect to number of points, not dimensions} computation of an upper bound for the actual maximum distance.
%  In a $kd$-tree, one suitable summary statistic is the bounding hyper-rectangle for each node:
%  \begin{equation}
%  d^{\max}(X, Y) \leq d^{\max}_{\hrect}(\namestat{hrect}(X), \namestat{hrect}(Y))
%  \end{equation}
%  
%  \noindent The meta-function $\outstat$ corresponds to any relevant summary statistic, which can vary from problem to problem.
%  In addition to the commonplace statistic $\namestat{hrect}$, two-point correlation additionally uses the cardinality statistic $\namestat{cardinality}(X) = |X|$.
%  Algorithms that utilize multipole expansions may require the mean, variance, or other moment statistics.
%  
%  We can now generalize the entire recurrence relation for simple global reduce problems.
%  We claim that problems of the form:
%  \begin{equation}
%    \gamma(X, Y) = \bigoplus_{(x, y) \in X \times Y} f(a,b)
%  \end{equation}
%  
%  \noindent where $\bigoplus$ is any commutative and associative operator, can be expressed as:
%  \begin{equation}
%  \label{eqn:global_intrinsic}
%   \begin{array}{ll}
%    \lefteqn{\gamma(X, Y) =}
%    \\
%    & \left\{
%      \begin{array}{lr}
%        \delta \left( \outstat(X), \outstat(Y) \right) & \IF C(\outstat(X), \outstat(Y))
%        \\
%        \gamma(X_L, Y) \oplus \gamma(X_R, Y) & \mbox{if convenient}
%        \\
%        \gamma(X, Y_L) \oplus  \gamma(X, Y_R) & \mbox{if convenient}
%      \end{array}
%    \right.
%   \end{array}
%  \end{equation}
%  
%  \noindent under the following model:
%  \begin{itemize}
%    \item $\gamma(X, Y)$ - The result of the global reduce, on the entire problem or on a subproblem.
%    \item $\outstat(X)$ - Summary statistics over a range of data.
%    \item $\delta(\outstat(X), \outstat(Y))$ - The estimated contribution of a sub-computation using only summary statistics.
%    \item $C(\outstat(X), \outstat(Y))$ - An indicator if a prune is possible, and that the subproblem $\gamma(X, Y)$ can be substituted by $\delta(\outstat(X), \outstat(Y))$.
%    \item $\bigoplus$ - The reduce operator.
%  \end{itemize}
%  
%  \noindent We also note that in most cases $\outstat$ is in the form:
%  \begin{equation}
%    \outstat(X) = \bigoplus^{\outstat}_{x \in X} f^{\outstat}(x)
%  \end{equation}
%  
%  \noindent for some commutative, associative $\bigoplus^{\outstat}$, and can be calculated efficiently in a bottom-up pass while building a $kd$-tree:
%  \begin{equation}
%  \label{eqn:stat}
%   \begin{array}{ll}
%    \lefteqn{\outstat(X) =}
%    \\
%    & \left\{
%      \begin{array}{lr}
%        f^{\outstat}(X) & \IF |X| = 1
%        \\
%        \outstat(X_L) \oplus^{\outstat} \outstat(X_R) & \mbox{otherwise}
%      \end{array}
%    \right.
%   \end{array}
%  \end{equation}
%  
%  \noindent The previous functions define the execution of some generalized $N$-body problems in a form that is easily implemented as a dual-tree algorithm\footnote{In practice, very small subsets may be computed exhaustively to reduce the overhead of treating individual points as entire subsets.}.
%  
%  There is significant freedom in how to compute the thse recurrence relationships.
%  The $\gamma$ recurrence permits many types of expansion, such as depth-first, breadth-first, and importantly, parallel expansions.
%  Thus, by separating the computation of the recurrence relationship from the problem-specific details, parallelization of all these algorithms requires only the up-front cost of a single parallel execution mechanism.
%  
%  The model shown so far is limited to very simple dual-tree algorithms.
%  First, we later discuss additions to allow for efficient computation of query-reference problems such as all-nearest-neighbors and density estimation.
%  Additionally, many algorithms, such as nearest-neighbor, classification problems, and approximate density estimates, require information about previous pairwise computation in order to determine whether pruning is possible.
%  Nonetheless, this simple model leads to effective parallelization of problems such as two-point correlation\footnote{WALDO list more}.
%  
%  \subsection{Query-Reference Problems}
%  
%  % Depends: Global Reductions
%  
%  A query-reference problem considers computes a result independently for each point in a query set.
%  These problems take on the general form:
%   
%    $$\map_{q \in Q} \bigoplus_{r \in R} f(q, r)$$
%  
%  Although each query is independent, speedup can still be achieved by looking at queries {\it en masse}.
%  It may be possible to show that one set of references is irrelevant to an entire set of queries within a computation, and thus prune the subproblem.
%  
%  We define $\outrho(q, R)$ as each element of the map, i.e. $\outrho(q, R) = \bigoplus_{r \in R} f(q, r)$.
%  Next, we decompose $f$ into two separate functions $f^1$ and $f^2$ such that $f^2(q, f^1(q, r)) = f(q, r)$.
%  We make this decomposition to allow us to take advantage of distributivity for problems where $f(\cdot, \cdot)$ distributes over $\bigoplus$.
%  In the dual-tree algorithm, $\outrho$ can be computed as follows:
%  \begin{equation}
%    \label{eqn:rho}
%    \outrho(q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R)) = f^{2}(q, \pi(\{q\}, R))
%    \end{equation}
%    
%  \noindent where $\pi$ is the {\it postponed information} from prunes and/or exhaustive computations:
%  \begin{equation}
%  \label{eqn:pi}
%   \begin{array}{ll}
%    \lefteqn{\pi(Q, R) = f^{2}(q, \bigoplus^{1}_{r \in R} f^{1}(q, R))}
%    \\
%    & \left\{
%      \begin{array}{lr}
%        F^{1}(\outstat(Q), \outstat(R)) & \IF C(\outstat(Q), \outstat(R), \mu(Q))
%        \\
%        \pi(Q^{*}, R) & \mbox{if defined for } Q^{*} \supset Q
%        \\
%        \pi(Q, R_L) \oplus^{1} \pi(Q, R_R) & \mbox{otherwise}
%      \end{array}
%    \right.
%   \end{array}
%  \end{equation}
%  
%  \noindent where the {\it on-line per-query-node pruning information} is defined:
%  \begin{equation}
%  \label{eqn:mu}
%   \begin{array}{ll}
%    \lefteqn{\mu(Q, R) =}
%    \\
%    & \left\{
%      \begin{array}{lr}
%        f(q, r) & \IF Q = \{q\} \mbox{ and } R = \{r\}
%        \\
%        \delta(\outstat(Q), \outstat(R)) & \mbox{if convenient}
%        \\
%        F^2(\outstat(Q), \pi(Q, R)) & \mbox{if defined}
%        \\
%        \mu(Q_L, R) \worst \mu(Q_R, R) & \mbox{if convenient}
%        \\
%        \mu(Q, R_L) \opmu \mu(Q, R_R) & \mbox{if convenient}
%        \\
%        \mu(Q^{*}, R) & \mbox{otherwise, for } Q^{*} \supset Q
%      \end{array}
%    \right.
%   \end{array}
%  \end{equation}
%  
%  under the model\footnote{In practice for Equation \ref{eqn:mu}, rules 1 and 4 are most accurate but only used in depth-first expansions, rule 6 is used only when no other information is available, and rule 5 is used primarily in breadth-first expansions.}:
%  \begin{itemize}
%    \item $F^1(\outstat(Q), \outstat(R))$ - A pairwise function to allow for distributivity only valid when pruning is possible, identical to $f^1(\cdot, \cdot)$ when $|Q| = |R| = 1$.
%    If $f(\cdot, \cdot)$ distributes over $\bigoplus$, then $f^1(\outstat(R))$ is often the same as $\outstat(R)$.
%    Otherwise, it usually corresponds to a value whose effect on the overall problem is indistinguishable from $\forall_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
%    \item $F^2(\outstat(Q), \pi(Q, R))$ - A post-processing function that determines the bounds of the effects of $\pi$ on a region of queries.
%    \item $\bigoplus^{1}$ - A commutative and associative operator that operates on the results of $f^1$ before $f^2$ is applied, simulating the effects of the original $\bigoplus$ as specified in Equation \ref{eqn:rho}.
%    \item $\delta(\outstat(Q), \outstat(R))$ - A function that computes pessimistic bounds on the contribution of all of $R$ to any element of $Q$.
%    Formally, succinct information bounding the set of results $\union_{q \in Q} \bigoplus_{r \in R} f(q, r)$.
%    For kernel density estimation, this is the pair $(\min_{q \in Q} \sum_{r \in R} K(q, r), \max_{q \in Q} \sum_{r \in R} K(q, r))$.
%    \item $\worst$ - An operator to relax the bounds for a query region.
%    Given $\mu(Q_L, R)$ and $\mu(Q_R, R)$, we can create more pessimistic pruning information for the worst-case effect of $R$ on any of the query points $Q$.
%    In many algorithms, this operator takes the minimum of some lower bound and maximum of some upper bound.
%    \item $\Opmu$ - An operator that pessimistically merges the contributions of two disjoint reference subsets on the same query subset.
%    For kernel summation problems, this sums the lower and upper bounds.
%    \item $C$ - A Boolean function indicating if a prune is possible, using information from $\mu$.
%    The operators applied in order to compute $\mu$ depend on the expansion pattern and the sequence of node pairs that have already been visited.
%    It is entirely dependent on the type of problem being solved which expansion pattern, whether it be depth-first or breadth-first, favors the most pruning.
%   For some problems such as exact kernel density estimation, $C$ does not use $\mu$ at all, and any expansion pattern can be used.
%  \end{itemize}
%  
%  It is necessary to clarify in Equation \ref{eqn:pi} the ``if defined'' comment for the second rule.
%  The recurrence relationship only defines a base case if the prune check $C$ is true; thus, $\pi$ is not defined if the second rule is invoked improperly.
%  
%  {\bf Theorem}\footnote{use LaTeX to do this}: If the prune check $C(\outstat(Q), \outstat(R), \cdot)$ is always true for the base case $|Q| = |R| = 1$, then the recurrence of Equation \ref{eqn:pi} invoked via Equation \ref{eqn:rho} must must be defined even if the second rule is not invoked.
%  
%  {\bf Proof}: The initial invokation, per Equation \ref{eqn:rho}, begins with $Q$ being the singleton set $\{q\}$.  Further invokations will retain $|Q| = 1$ since $Q$ is not modified by the third rule.  However, invoking on $R_L$ and $R_R$ always decreases the reference set by at least one.  Therefore, $|R| = 1$ eventually, and since $|Q| = 1$ always, the first rule will eventually define the result.
%  
%  In fact, evaluating $\outrho$ in Equations \ref{eqn:rho} and \ref{eqn:pi} as such is identical to the single-tree algorithm -- recursive subdivision along the reference tree until a prune is possible.
%  However, dual-tree algorithms may prune multiple queries at once, prompting the existence of the second rule in Equation \ref{eqn:pi}.
%  
%  It is also noteworthy that Equation \ref{eqn:mu} may evaluate to very different values during the course of the dual-tree computation.
%  As more of the tree is expanded, it becomes ``convenient'' to expand $\mu$ in a manner more advantageous to pruning.
%  
%  MOTIVATION FOR THIS MODEL: Every type of information transfer is accounted for.

